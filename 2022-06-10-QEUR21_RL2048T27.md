## 「パターンRT法」をテクノメトリックスに使用した強化学習

### 【技術分野】 
【０００１】本発明はタグチメソッドの多変量分析手法の一つであるRTメトリックスを使い、その応用手法である「パターンRT法」を強化学習に適用します。

### 【背景技術】
【０００２】強化学習は1960年代に制御工学としてベルマン方程式が提案されたのが始まりです。2016年にAIが囲碁世界チャンピオンに勝利しましたが、それには強化学習に組み込まれたディープ・ラーニングが大きな役割を果たしています(図1)。そして、強化学習の手法の開発はディープラーニングの普及に伴い急速に進歩しています(図2)。近年では強化学習をロボットに導入して、二足歩行や自動運転などの複雑な動作を実現した事例もあります。

**(図1:ALPHA GOの歴史的勝利)**

![imageRL1-25-1](https://yaber1965.github.io/images/imageRL1-25-1.jpg)

**(図2:強化学習技術の発展)**

![imageRL1-25-2](https://yaber1965.github.io/images/imageRL1-25-2.jpg)

【０００３】ただし、強化学習が普及するにはまだまだ課題があります。強化学習をつかったロボットの事例は、まだ一部しかありません（図３）。現状の強化学習はトレーニング（学習）のためのコストがかかりすぎるためです。


**(図3: 強化学習の現状)**

![imageRL1-25-3](https://yaber1965.github.io/images/imageRL1-25-3.jpg)

【０００４】強化学習は学習主体（Agent）が環境（Environment）とコミュニケーションしながら、エージェントが命令(action)の出し方を学習していきます(図4)。ここで、実環境の代わりにVR(シミュレーションetc)技術を使って学習ができれば学習期間とコストが削減できます(図5)。もし、シミュレーションで実機環境に近い学習ができるのであれば、製品実現の早い段階（R&D段階）で開発を始めることができ、結果として製品のリリースも早くなります（図6）。


**(図4： 強化学習のエコシステム)**

![imageRL1-25-4](https://yaber1965.github.io/images/imageRL1-25-4.jpg)

**(図5：実環境をシミュレーション(VR)で代替する)**

![imageRL1-25-5](https://yaber1965.github.io/images/imageRL1-25-5.jpg)

**(図6: 製品実現プロセス)**

![imageRL1-25-6](https://yaber1965.github.io/images/imageRL1-25-6.jpg)

【０００５】複雑な強化学習の例として、自動運転やAtariに強化学習を導入する場合にはCNN(畳み込みニューラルネットワーク)が使われます。CNN(図7)は2次元画像(パターン)を畳み込み(convolution)で1次元ベクトルに圧縮し、ディープラーニングに情報を引き渡します。この方法は極めて強力ですが、多層の畳み込み関数を使うので必要な計算機のスペックが高くなります。


**(図7: CNNのしくみ)**

![imageRL1-25-7](https://yaber1965.github.io/images/imageRL1-25-7.jpg)

### 【発明の概要】

### 【発明が解決しようとする課題】

【０００６】有名なAtari_BreakoutをCNNで強化学習した場合の計算負荷の事例を紹介します（動画）。このプロジェクトに使用されたPCは、Intel_i7のCPUであり、2020年であってもハイスペックなCPUであり、本文には記載されていませんが、おそらく当該計算機は高性能のGPUも搭載していると思われます。それでも学習には90時間が必要になります（図8）。

**(動画: Atariの強化学習の事例)**

<iframe width="560" height="315" src="https://www.youtube.com/embed/V1eYniJ0Rnk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

**(図8: CNN強化学習の計算負荷)**

![imageRL1-25-8](https://yaber1965.github.io/images/imageRL1-25-8.jpg)

【０００７】強化学習に要求されるPCがそれほどハイスペックになる理由の一つは、ディープラーニングで定義されている関数が複雑であるためです。もちろん、関数の複雑さは解決するテーマ（環境、ゲーム）によって変わりますが、このAtari_Breakoutの事例で学習に使用した関数はCNN強化学習としては簡単な方です。


**(図9: DL関数定義の比較)**

![imageRL1-25-9](https://yaber1965.github.io/images/imageRL1-25-9.jpg)

【０００８】強化学習をよりロボティックスに使用しやすくするには、より簡単なアーキテクチャ（関数）でも同様なパフォーマンスを実現する必要があります。たとえ、その手段に汎用性がなく、一部のテーマだけに適用できるロジックでもかまいません。現状では強化学習が適用可能なテーマはたった30パーセントだけなのですから・・・（図3のコメントを参照）。


### 【課題を解決するための手段】

【０００９】本発明はディープラーニング(DL)強化学習のDLアーキテクチャ（関数定義）を図10のように変更し、ディープラーニングの入力に「パターンRTメトリックス」を使用します。


**(図10: DL関数のアーキテクチャ)**

![imageRL1-25-10](https://yaber1965.github.io/images/imageRL1-25-10.jpg)

【００１０】本発明の構造自体は新しいモノではなく、機械学習分野ではFeature_Engineering（特徴量エンジニアリング）と呼ばれるものです。典型的な手法は主成分分析(PCA)で、画像認識では広く用いられています(図11)。主成分分析では2次元画像を主成分という1次元ベクトルに分割します。そのベクトルのうち主要なもの150件を用いれば、画像をかなり再現できます。


**（図11： PCAを用いたFeature-Engineering）**

![imageRL1-25-11](https://yaber1965.github.io/images/imageRL1-25-11.jpg)

【００１１】しかし、PCAではディープラーニングの入力データを極限まで単純化できません。ここで、上述の「DL入力データを単純にする」には2つの含意があり、ひとつは「入力するデータ次元を最小化すること」、もう一つは「当てはめたDL関数が線形に近くなる」ことです。関数が単純になった事例として、従来RT法を使った外観検査自動機と新RT法を使ったパフォーマンスを比較します(図12と図13)。


**（図12： 従来RT法を用いた外観検査自動機のパフォーマンス）**

![imageRL1-25-12](https://yaber1965.github.io/images/imageRL1-25-12.jpg)

**（図13： 新RT法を用いた外観検査自動機のパフォーマンス）**

![imageRL1-25-13](https://yaber1965.github.io/images/imageRL1-25-13.jpg)

【００１２】図12と図13を比較すると、RTメトリックスを変えることで判別パフォーマンスに20％も変化しました。さらには、SVM（サポートベクトルマシン）に適用した4種類カーネルのうち、最適パフォーマンスをもつカーネルの種類も変わりました。新RTメトリックスではパフォーマンスが上がっただけでなく、カーネルが「Poly(従来RT)」から「Linear(新RT)」に変わりました（図14）。

**(図14: SVMにおけるカーネルの種類)**

![imageRL1-25-14](https://yaber1965.github.io/images/imageRL1-25-14.jpg)

【００１３】このように、本発明の（テクノ）メトリックスを適切に使ってDL関数を最適化したいのです(図15)。さて、どのようにすれば「良いメトリックス」が得られるのか、そのためのアイデアの一つが「一つの命令パターン(action)を一つの状態パターン(state)に対応させる」ということです。強化学習において、一般には状態の次元数はアクションの数以下にはなりません。

**(図15: メトリックスによる関数の単純化)**

![imageRL1-25-15](https://yaber1965.github.io/images/imageRL1-25-15.jpg)

【００１４】このようなメトリックスを生成するにあたって、タグチメソッドのRT法は非常に便利です。RT法は標準状態の多変量データ（ベクトル）と計測対象の多変量データを比較して、その差異を感度とSN比という2次元の値に変換します(図16)。


**（図16： RTメトリックスの原理）**

![imageRL1-25-16](https://yaber1965.github.io/images/imageRL1-25-16.jpg)

【００１５】本発明の「パターンRT法」は代表的なパターンを複数生成し、それを単位空間（標準ベクトル）とします。一方、信号空間（計測ベクトル）は一つであり、両者は比較されてメトリックスを生成します。これらのメトリックスを束ねたものが強化学習の「状態（state）インプット」になります。


**（図17： パターンRTメトリックス）**

![imageRL1-25-17](https://yaber1965.github.io/images/imageRL1-25-17.jpg)


### 【実施例_1】

【００１６】今回の学習事例は図18に示すゲーム2048を強化学習の環境(Environment)にします。このゲームは数字が表示されたパネルが4X4に配置されたボードを上下左右にスライドさせてパネルの数値を大きくさせ、高得点を得ることができます。


**（図18: 環境として採用したゲーム）**

![imageRL1-25-18](https://yaber1965.github.io/images/imageRL1-25-18.jpg)

【００１７】本発明にたいする比較対象の１つ（ベースライン）として「畳み込みRT法」を使って学習しました。畳み込みRT法は、CNN（畳み込みニューラルネットワーク）と同様に畳み込みを使用してパターンをメトリックスに変換します。学習結果を図19と図20に示します。


**（図19: イプシロン値とゲーム盤角部の数値の推移）**

![imageRL1-25-19](https://yaber1965.github.io/images/imageRL1-25-19.jpg)

**（図20: その他のパフォーマンスの推移）**

![imageRL1-25-20](https://yaber1965.github.io/images/imageRL1-25-20.jpg)

【００１８】本実験では強化学習手法はDQN experience replayを使用し、5000回繰り返して学習させています。


### 【実施例_2】

【００１９】次の実験では、これも比較事例として「スライドRT法」を用いて学習しました。スライドRT法は、評価の基準である単位空間のメンバは１つだけで、比較対象となる信号空間のメンバを複数にします(図21)。nターン目のゲームのパターンを単位空間、上下左右（4水準）の命令を与えた後のパターンを信号空間としています。

**（図21: スライドRTの考え方）**

![imageRL1-25-21](https://yaber1965.github.io/images/imageRL1-25-21.jpg)

【００２０】本実験でも学習とともにパフォーマンスが上がっていますが、実施例（1）と同じレベルです(図22と図23)。


**（図22: イプシロン値とゲーム盤角部の数値の推移）**

![imageRL1-25-22](https://yaber1965.github.io/images/imageRL1-25-22.jpg)

**（図23: その他のパフォーマンスの推移）**

![imageRL1-25-23](https://yaber1965.github.io/images/imageRL1-25-23.jpg)

【００２１】本実験でも、強化学習手法は実施例(1)と同じくDQN experience replayを使用し、5000回繰り返しています。


### 【実施例_3】

【００２２】本実験は、本発明の「パターンRT法」を採用して強化学習をした場合です。本手法では、まず最初にパターンを設定しなければなりません。学習が進みスコアが上がってきたときのゲーム命令（action）のパターンを図24に示します。このようにゲーム2048では、特定の命令パターンで高得点が上がることが知られています。

**（図24: スコアが良くなった時の命令パターン）**

![imageRL1-25-24](https://yaber1965.github.io/images/imageRL1-25-24.jpg)

【００２３】ゲーム2048には高得点が得られる「定石」は4種類あり、その内訳を図25に示します。この知見を参考にして、単位空間となるパターンを自動生成します。乱数を使って標準パターンを生成した結果の例を図26に示します。


**（図25: 複数の画像を同時に撮影する）**

- **命令0（左）と命令1（上）　→　パターンが左上に偏る**
- **命令0（左）と命令3（下）　→　パターンが左下に偏る**
- **命令2（右）と命令1（上）　→　パターンが右上に偏る**
- **命令2（右）と命令3（下）　→　パターンが右下に偏る**


**（図26: パターンを自動生成させた事例）**

![imageRL1-25-25](https://yaber1965.github.io/images/imageRL1-25-25.jpg)

【００２４】強化学習を5000回繰り返した実験結果を図27と図28に示します。学習のパフォーマンスは実施例(1)と(2)よりも若干高くなっています。

**（図27: イプシロン値とゲーム盤角部の数値の推移）**

![imageRL1-25-26](https://yaber1965.github.io/images/imageRL1-25-26.jpg)

**（図28: その他のパフォーマンスの推移）**

![imageRL1-25-27](https://yaber1965.github.io/images/imageRL1-25-27.jpg)

【００２５】さらに、学習回数を20000回まで増加した実験結果を図29と図30に示します。学習の後半で角部の値が1024で、スコアが12000になるゲームが現れています。

**（図29 :　イプシロン値とゲーム盤角部の数値の推移）**

![imageRL1-25-28](https://yaber1965.github.io/images/imageRL1-25-28.jpg)

**（図30: その他のパフォーマンスの推移）**

![imageRL1-25-29](https://yaber1965.github.io/images/imageRL1-25-29.jpg)

【００２６】参考として、大学生が卒研で2048ゲームをDQNで強化学習したレポートを紹介します。卒研の実験に使っている関数はノード数が300、5層であり、かなり関数が複雑です。一方、本実験の関数は128ノードで2層になっています(図31)。（注意：レポートにおける関数のアーキテクチャの決定根拠は不明。本実施例では最適アーキテクチャの検証はしておらず、スコア向上のためには検討の余地あり。）


**(図31: DL関数のアーキテクチャ)**

![imageRL1-25-30](https://yaber1965.github.io/images/imageRL1-25-30.jpg)

【００２７】上記レポートの強化学習の実験結果を図32と33に示します。本発明の実施例とレポートの事例は種々の条件が違うので簡単に比較することはできません。しかしながら、本発明は簡単な関数(NODE:128 x LAYER:2)でレポートのいわゆる「実験2」の水準に到達していることがわかります。このように、テクノメトリックスを採用することによって、従来の手法よりも簡単な関数で強化学習できる可能性が見出せました。

**（図32: 実験結果-1）**

![imageRL1-25-31](https://yaber1965.github.io/images/imageRL1-25-31.jpg)

**（図33： 実験結果-2）**

![imageRL1-25-32](https://yaber1965.github.io/images/imageRL1-25-32.jpg)

### 【産業上の利用可能性】

【００２８】本発明は強化学習が使えるテーマのうち、ごく一部のケースでのみ使用できるでしょう(図3)。しかし、それでも有用です。強化学習の「環境」に工夫を加えずに、パターンRT法を使うのは容易ではありません。しかし、加工プロセスのような人工的な環境においては、当初のシステムではパターンを作るのは難しくても、その環境を改善することは難しくはないでしょう。

**(図3（再掲）: 強化学習の現状)**

![imageRL1-25-33](https://yaber1965.github.io/images/imageRL1-25-33.jpg)

【００２９】もし、強化学習を容易にする「パターン」を見出すことができれば、プロセスに適用することは難しくないでしょう。パターンRT法を用いた強化学習はVR（仮想空間）の中での学習が容易なので、通常の強化学習よりも低コストで実現できます。さらには、学習した結果は環境の変化に対してロバストになるでしょう。

【００３０】本発明の適用ができないのは、VR（仮想現実）によって環境を構築できないケースです。「できない」というよりも「適用する価値がない」といったほうが正確でしょう。現在では、種々のVRソフトウェアが提案されており（例として図34）、開発者（VRユーザー）のスキル次第で多くの問題を解決できるでしょう。

**（図34：Unity-VR）**

![imageRL1-25-34](https://yaber1965.github.io/images/imageRL1-25-34.jpg)


## ～　まとめ　～

### ・・・　さらに前回のつづきです。完結なるか・・・　・・・

C部長: “**「EVERYBODY IS CREATIVE CLASS」**は「中二病的」表現であると思っていたが、実はそうでもなかった・・・。”

![imageRL1-25-35](https://yaber1965.github.io/images/imageRL1-25-35.jpg)

QEU:FOUNDER: “そもそも「EVERYBODY IS CREATIVE CLASS」というのは、**「とある府知事をやっているイケメン」**のために小生が考えた概念なんだよ。そもそも・・・。あそこ（O府）に住んでいても給料が全然あがらないものだから・・・。”

![imageRL1-25-36](https://yaber1965.github.io/images/imageRL1-25-36.jpg)

D先生: “へいへい、すんません・・・。しかし、まだQEUシステムとCREATIVE CLASSの関係がまだわかりにくいだろうなァ・・・。”

![imageRL1-25-37](https://yaber1965.github.io/images/imageRL1-25-37.jpg)

QEU:FOUNDER ： “QEU(round2-1)の特徴はディープラーニングを中心としたAIシステムを開発することに重点を置いています。そのためには種々のVR技術を使いこなすことが機能アップとコストダウンに有効です。この一連の作業が「CREATIVE」なんです。パターン化されたAIを使うことよりも、普通の人々が創造力を発揮してVRツールを使いこなすことに重点があるんです。”

![imageRL1-25-38](https://yaber1965.github.io/images/imageRL1-25-38.jpg)

C部長: “それでもEVERYBODY（普通の人々）には難しいんじゃないですか・・・。やっぱり、2000年代の新自由主義のように「選ばれた人たち」じゃないと・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/_QUEXsHfsA0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ： “選ばれてもオッサン・オバはんには難しいよ（笑）。・・・でも、QEUの考え方は、コンピューティングに慣れた若い人が学習するのならば、JHのFastAIよりもはるかにわかりやすいはずです。・・・ただし、まだ（QEUは）成熟はしていないが・・・（笑）。”

![imageRL1-25-39](https://yaber1965.github.io/images/imageRL1-25-39.jpg)

QEU:FOUNDER ： “QEUシステムは「動くモノ（AI）ができること」が各レクチャーの目標になっていますし、高価でハイスペックなPCを所有していることが学習の前提じゃないです。”

![imageRL1-25-40](https://yaber1965.github.io/images/imageRL1-25-40.jpg)

QEU:FOUNDER ： “QEUシステムの根本思想には「メトリックスを活用して関数を簡単にする」という考え方が流れています。これは解析技術編でより明確であり、この考え方はエンジニアリングのあらゆる局面で少なくとも間接的には有効に作用するはずです。

C部長: “なんか、（QEUは）もともとのタグチメソッドからずいぶん外れちゃったなぁ・・・。”

D先生: “タグチ先生はゼロ点一次式(SN比)を思考のベースにしているのだが、QEUシステムでは**「ディープラーニングの関数を単純にすること」**を思考のベースにしているんですよ。我々の狙いとしては、「総合SN比」というメトリックスがプロセスの支配関数の単純化レベルを評価する指標になるはずなのだが・・・。”

![imageRL1-25-41](https://yaber1965.github.io/images/imageRL1-25-41.jpg)

QEU:FOUNDER ： “これはまだ道半ば・・・（笑）。結局、ディープラーニングという「トンでもないもの」が出現し、世の中が複雑になったからゼロ点一次式だけじゃ何ともならなくなったんだ。なにはともあれ、イケメン（右）の現在の経済政策（供給力だより）じゃ全然不足しているから・・・（正直ベース、残念ながら・・・）。なんとか生産力（創造力）の重要性を気づかせてあげないと・・・。”

![imageRL1-25-42](https://yaber1965.github.io/images/imageRL1-25-42.jpg)

C部長: “給料が上がらない・・・。”

QEU:FOUNDER ： “物価高なのにね。”


