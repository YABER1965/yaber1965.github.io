## QEUR21_FATOY13:　カテゴリ変数入りデータの予測 (その8)

## ～　この方法は適材適所で使えば？　～

### ・・・　やっと、プロジェクトがおわります　・・・

D先生 ： “なかなかいいベクトル(CAT2VEC)を作ることができました。これでディープラーニング学習をすればどうなりますかね？”

__(month)__
![image1-57-1](https://yaber1965.github.io/images/image1-57-1.jpg)

__(weekday)__
![image1-57-2](https://yaber1965.github.io/images/image1-57-2.jpg)

QEU:FOUNDER ： “さあて、ついにフィニッシュだ！！DL学習プログラムをドン・・・。”

```python
# -------------------------
# calc_dl_t2m_cat2vec_double.py
# エンベッティングによる予測事例(PyTorch - t2mダブル)
# -------------------------
import pandas as pd
import torch
import numpy as np
import matplotlib.pyplot as plt
# One-hot Encoding the Island Feature
from sklearn.preprocessing import OneHotEncoder
%matplotlib inline 

# -------------------------
# TRAIN側の変換作業
# -------------------------
path = './'
# 基本データ
data_df = pd.read_csv(path+'bike_sharing_daily_t2m.csv')
del data_df['dteday']
del data_df['yr']
del data_df['casual']
del data_df['registered']
num_data = len(data_df)
print(num_data)
data_df.head()

# mnthのエンベッディング
# CSVファイルを読み込む
mt_df = pd.read_csv(path+'t2m_table_double_month.csv')

# month : エンベッディングに変換する
arr_mnth    = mt_df['ITEM'].values
arr_embed_1 = mt_df['COEFF_C'].values
arr_embed_2 = mt_df['COEFF_R'].values
# -----
add_embeddA = []
add_embeddB = []
for iRow in range(num_data):
    val_mnth = data_df.loc[iRow, 'mnth']
    jCol = val_mnth -1
    add_embeddA.append(arr_embed_1[jCol])
    add_embeddB.append(arr_embed_2[jCol])
# -----
data_df['mnth_A'] = add_embeddA
data_df['mnth_B'] = add_embeddB
del data_df['mnth']
data_df.head()

# weekdayのエンベッディング
# CSVファイルを読み込む
wd_df = pd.read_csv(path+'t2m_table_double_weekday.csv')
wd_df

# weekday : エンベッディングに変換する
arr_wkday   = wd_df['ITEM'].values
arr_embed_1 = wd_df['COEFF_C'].values
arr_embed_2 = wd_df['COEFF_R'].values
#print(arr_wkday)
# -----
add_embeddA = []
add_embeddB = []
for iRow in range(num_data):
    val_wkday = data_df.loc[iRow, 'weekday']
    jCol = val_wkday -1
    add_embeddA.append(arr_embed_1[jCol])
    add_embeddB.append(arr_embed_2[jCol])
#print(add_embeddA)
#print(add_embeddB)
# -----
data_df['wkday_A'] = add_embeddA
data_df['wkday_B'] = add_embeddB
del data_df['weekday']
data_df.head()

# ------
# One-hot Encoding the weathersit Feature
one_hot = OneHotEncoder()
data_df2 = data_df.copy()
encoded = one_hot.fit_transform(data_df2[['weathersit']])
data_df2[one_hot.categories_[0]] = encoded.toarray()
del data_df2['weathersit']
data_df2.head()

# ------
# train用データセット
train_df = data_df2.copy()
train_df = train_df[train_df["use"]=="train"]
del train_df['use']
arr_train_Y = train_df.loc[:,"cnt"].values
print(arr_train_Y)
del train_df['cnt']
mx_train_Xs = train_df.values
print(mx_train_Xs)

# ------
# test用データセット
test_df = data_df2.copy()
test_df = test_df[test_df["use"]=="test"]
del test_df['use']
arr_test_Y = test_df.loc[:,"cnt"].values
del test_df['cnt']
mx_test_Xs = test_df.values

# ------
# テンソル化
X_train = torch.from_numpy(mx_train_Xs).float()
new_shape = (len(arr_train_Y), 1)
y_train = torch.from_numpy(arr_train_Y).float()
y_train = y_train.view(new_shape)
X_test = torch.from_numpy(mx_test_Xs).float()
new_shape = (len(arr_test_Y), 1)
y_test = torch.from_numpy(arr_test_Y).float()
y_test = y_test.view(new_shape)
#print(y_train.shape)

# インプットデータの確認
print("X_train.shape：",X_train.shape)
print("y_train.shape：",y_train.shape)
#X_train.shape： torch.Size([610, 26])
#y_train.shape： torch.Size([610, 1])

# ------
# D_in is input dimension;H is hidden dimension; D_out is output dimension.
D_in, H, D_out = 11, 20, 1
# 学習モデルの定義
# model
class Net(torch.nn.Module):
    def __init__(self, D_in, H, D_out):
        super(Net, self).__init__()
        self.L0 = torch.nn.Linear(D_in, H)
        self.N0 = torch.nn.ReLU()
        self.L1 = torch.nn.Linear(H, D_out)

    def forward(self, x):
        x = self.L0(x)
        x = self.N0(x)
        x = self.L1(x)
        return x

model = Net(D_in, H, D_out)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

# NN繰り返し学習する
losses = []
testlosses = []
for t in range(5000):
    y_pred = model(X_train)

    loss = criterion(y_pred, y_train)
    losses.append(loss.item())
    if t % 500 == 0:
        print(t, loss.item())

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    with torch.no_grad():
        y_test_pred = model(X_test)
        testloss = criterion(y_test_pred, y_test)
        testlosses.append(testloss.item())

# ------
# drawing learning curve
def learn_curve(losses, testlosses):
    fig = plt.figure(figsize=(10,6))
    ax = fig.add_subplot(1,1,1)
    plt.plot(list(range(len(losses))), losses, label = "losses" , color = "r")
    plt.plot(list(range(len(losses))), testlosses, label = "testlosses" , color = "g")
    plt.xlabel('#epoch')
    plt.ylabel('loss')
    plt.legend(loc='upper right')
    plt.show()

# 学習結果（loss:trainloss, testloss:いわゆるvalidloss）
learn_curve(losses, testlosses)

# ------
# make a class prediction for one row of data
def predict(row, model):
    # make prediction
    yhat = model(row)
    # retrieve numpy array
    yhat = yhat.detach().numpy()
    return yhat.flatten()

# 学習結果（精度検証）
ycomp = predict(X_test, model)
#print(ycomp)
fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(1,1,1)
plt.scatter(ycomp, arr_test_Y, label = "accuracy" , color = "r")
plt.xlabel('prediction')
plt.ylabel('actual')
plt.legend(loc='best')
plt.show()

```

QEU:FOUNDER ： “このコードをそのまま実行すると、以下の結果がでてきます。”

![image1-57-3](https://yaber1965.github.io/images/image1-57-3.jpg)

D先生 ： “予測精度についてはかなり改善されています。でも、検証(test)データの学習曲線の変動はなぜ大きくなるんでしょうか？”

QEU:FOUNDER ： “ちょっとディープラーニングのハイパーパラメータを変えてやってみました。おそらく、学習率(lr)を小さくすることがかなり効くんだと思いますよ。”

![image1-57-4](https://yaber1965.github.io/images/image1-57-4.jpg)

D先生 ： “すくなくとも傾きに関しては、この方がよさげです。そういえば、ベクトルを作るときにT法の単位空間として何を取ったんでしたっけ？”

QEU:FOUNDER ： “再び検証(test)データの全部をつかいました（2-5-8-11月）。でも単位空間を変更する件、これ以上はやらないよ。単位空間の選択で予測が変わることはすでに確認済みだから・・・。

D先生 ： “今回のプロジェクトには**「予測の目的がない」**ですから、単位空間の選択基準がありません。”

QEU:FOUNDER ： “D先生・・・、今回のプロジェクトの結論として、Keras:Embeddingによるベクトル生成（CAT2VEC）とT法(2)によるベクトル生成のどちらが好き？感覚でいいから・・・。”

D先生 ： “おそらく「ケースバイケース」になるのではないかと思います。ただし、全体の印象的にはT法の方が好きです。”

QEU:FOUNDER ： “おっ・・・、D先生のその御意見はめずらしい。なぜですか？”

D先生 ： “多くの人は単位空間の選択を面倒だと思うだろうと思います。しかし、企業が使う場合には予測には必ず「目的」があります。**予測の目的を適切に選択できる意味で単位空間って役に立つ**んじゃないかと・・・。”

QEU:FOUNDER ： “そうなもんかねぇ・・・、小生は意見を控えるよ。これは「ついでにやった」プロジェクトだし・・・（笑）。”

D先生 ： “その割には気合をいれましたね・・・。”

QEU:FOUNDER ： “予測にとって重要な技術であるCAT2VECに関する記事がJ国語のWeb界隈にほとんどないことに危機感を覚えたわけ・・・。それで思わず・・・。”

D先生 ： “そう・・・、ひどいもんですね、おそらく知っている人はいるのだが、その貴重な知識を「シェア」しないのでしょう。これでは国際的な技術競争に負けますよ。”

QEU:FOUNDER ： “頭の古いオッサンに残念なことをお知らせすると、**最先端技術はシェアすることで生まれるんですよ・・・。最先端技術には「価値はあるが、値段はない（創発価値）」んです。**”


## ～　まとめ　～

### ・・・　MMT、いきなり復活！！　・・・

QEU:FOUNDER ： “全国民ベーシック・インカム、一律、20万円。Y先生もお墨付きじゃあ～！”

<iframe width="560" height="315" src="https://www.youtube.com/embed/jaDFD-P2PDo" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

C部長 : “FOUNDER・・・。いきなり、なにをまた・・・？”

QEU:FOUNDER ： “J国復活の処方箋としては、小生も以前から「これしかない」と思っていたからね。安い給料で働く必要などない、気軽に生活保護で暮らせば十分・・・。小生、この前この絵（↓）を見たとき、「猫の国はもうだめだァ～・・・」と絶望したからね・・・。”

![image1-57-5](https://yaber1965.github.io/images/image1-57-5.jpg)

QEU:FOUNDER ： “あのね・・・。マンガって、この国が常に軽蔑し虐げてきたモノじゃないの？働く人のキツさ、給料の低さはトンでもないレベルになってる。・・・で・・・、この国に、ついにな～んにもなくなって、あとで残ったものにすがろうという・・・。まぁ浅ましいこと・・・。”

C部長 : “まあ、若者に言わせると「国ガチャ」だから・・・。”

QEU:FOUNDER ： “でも、これは象徴的なんですよ。マンガはもっともパクリの多い分野です。オマージュといったほうが聞こえがいいかな。J国にとって、もっとも必要なモノはなにか？それは「パクリ」なんです・・・。他の分野も見習ってほしい・・・。”

![image1-57-6](https://yaber1965.github.io/images/image1-57-6.jpg)

C部長 : “それはちょっと失礼じゃ・・・。“

QEU:FOUNDER ： “最後まで聞いてくれ、これはC国に対する最大のリスペクトだ・・・。C国がなぜ技術的に発展したのか？その最大の貢献は**金盾（Great Fire Wall）**にあると思っています。”

D先生 ： “えっ？国内と海外のインターネットアクセスを遮断するアレ？う～ん、普通はそうは考えないでしょう。“

QEU:FOUNDER ： “あれ(G-Wall)があるのでC国のネット民は海外のネット情報のうち重要なものを拾い出し、それらを翻訳して、ブログとして国内に大量に保管しています。アクセス遮断により大騒ぎになったC国ネット民も、そのような努力の後でそれほど不便を感じていません。”

D先生 ： “それとどういう関係が？ “

QEU:FOUNDER ： “人間は深くモノを考えるときには母国語で考えます。たとえその人がバイリンガル・鳥リンガルであってもね・・・。母国語の情報が**「検索可能な状態」で、どれだけ大量に手に入るのか**がイノベーションを左右するんです。”

D先生 ： “そうすると、J（国）の現状は？“

QEU:FOUNDER ： “コンテンツの量はTW（国）にははるかに及ばない、K国よりもちょっと下のレベルでお話になりません。質はもっとひどいでしょう。特にJ国ブログの内容がね、その99％が「第一本書」のレベル・・・。”

![image1-57-7](https://yaber1965.github.io/images/image1-57-7.jpg)

D先生 ： “第一書・・・、なにそれ？“

QEU:FOUNDER ： “これはTWの俗語です。「全く知らない人が一番最初に読むべき本」って意味・・・。コンピュータ技術の場合、TWには「鉄人レース」、A国には「Kaggle」があるので高レベルのWeb情報があるんですが・・・。そして、C国は目利きが有用な情報を翻訳して国内に取り込むと・・・。”

D先生 ： “なるほど、J国の負け確定だ・・・。“

QEU:FOUNDER ： “実はこうなるのにはG社の技術的な背景もあります。G社の収益還元システムであるAdsenseはYouTube動画を強化し、ブログ記事を軽視する方向で進んでいます。「民間だけ」に頼っていては、これからは良質な文字情報がWebに蓄積するとは思えません。さて、ちょっと話題を変えるよ。我々のQEUシステムってオリジナリティがあると思う？”

D先生 ： “まあ、ある方じゃないでしょうか・・・。“

QEU:FOUNDER ： “あれ(QEUシステム)はパクリの塊・・・（笑）。十数年にわたって何重にもパクリを積み上げているので、一般人にはわかんないだけ。”

D先生 ： “そういえば、そうかも・・・。“

QEU:FOUNDER ： “すべからず、なんでもパクリから始まるの・・・（笑）。パクリだけで終わらせないためにはどうするのか？もう一度、パクること・・・。何回も何回もパクって、それらを互いに比較する。自然と自分に合わないものは淘汰されていくから・・・。”

D先生 ： “それが結果としてオリジナリティに見えているだけ・・・。“

QEU:FOUNDER ： “**J国には「オリジナリティ」を生成するのに十分なブログ（検索可能な文字情報）がありません**。これがイノベーション（ってあるのか？）形成における最も大きな問題です。じゃあ、必要な情報を生成するためにはどうするのか？一番いいのは、「J国が誇る100万人のニート軍団」に海外の情報を翻訳し、ブログ化してもらう。これが経済成長のもっとも近道です。それでも5年はかかるがね・・・。”

D先生 ： “そのために全国民に月20万円・・・。“

QEU:FOUNDER ： “はっきりいうと、安いモンだね・・・。ちなみに翻訳するものは何でもいいの！！「最先端のもの」、「価値のあるもの」なんか現時点ではわからないから・・・。翻訳者が自分の好きなモノを選んで訳せばいい。DEEP-Lの機械翻訳で十分です。内容はパキスタン料理の作り方でもいいし、アルパカの飼い方でもいい。さらにいえば、少女が使い古したテディベアを収集しているオヤジのブログでもいい・・・。”

C部長 : “それは変態・・・。 “

QEU:FOUNDER ： “いやいや・・・、少女が長年使ったテディベアはその道では高値で取引されているんですよ。これホント（笑）。”

