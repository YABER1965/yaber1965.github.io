## QEUR21_TREYE26:　本実験～両目法（その5）: マハラノビス学習の見直し

## ～　新たな課題が・・・。進歩なのか？　～

QEU:FOUNDER ： “これから、マハラノビス距離のパラメータを見直しましょう。多量の画像を使って・・・。”

![image3-46-1](https://yaber1965.github.io/images/image3-46-1.jpg)

D先生 ： “多量って・・・。ワーク（被検査物）ベースで、何件学習したんですか？”

QEU:FOUNDER ： “60件です。**1つのワークに3枚の画像が必要なので、データ量がすごい**んですよ。現実問題として単位空間として100件のデータが上限になると思いますよ。それでは、プログラムをドン！！”


```python
# -------------------- プログラムの始まり -------------------
# -*- coding: utf-8 -*-
# filename: testDE6_maharanobis_learning.py
# 複数画像を読み込み、マハラノビス距離を出力するための分散共分散行列を生成する 
# ---------------------------------------------------
# モジュールのインポート
import cv2
from fastai.vision.all import *
import pandas as pd
import numpy as np

#=================================================
# READ CSV_FILE FUNCTION
#=================================================
# 畳み込みファイルを読み込み表示する
def read_csvfile(file_readcsv): 
 
    # 畳み込みファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    # ------------------
    # 選択項目の読み込み
    # Xsマトリックス
    mx_Xs = df.loc[:,"col1":"col5"].values
    #print("----- mx_Xs -----")
    #print(mx_Xs)
  
    return mx_Xs

# ---------------------------
# LCR画像ファイルを読んで、テンソルを出力する
def read_LCRpics(pic_nameL, pic_nameC, pic_nameR): 

    # ------------------
    # 左(L)画像
    filename = foldername + pic_nameL
    imgL = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropL = imgL[212:856,47:1867]
    # リサイズ
    img_resizeL = cv2.resize(img_cropL , newsize)

    # ------------------
    # 中央(C)画像
    filename = foldername + pic_nameC
    imgC = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropC = imgC[212:856,47:1867]
    # リサイズ
    img_resizeC = cv2.resize(img_cropC , newsize)

    # ------------------
    # 右(R)画像
    filename = foldername + pic_nameR
    imgR = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
    img_cropR = imgR[212:856,47:1867]
    # リサイズ
    img_resizeR = cv2.resize(img_cropR , newsize)
    # 画像出力
    #cv2.imshow("image(Right)", img_resizeR)
    #cv2.waitKey(0)
    #cv2.destroyAllWindows()

    # ------------------
    # 画像データのテンソル化(L,C,R)
    img_tensorL = tensor(img_resizeL/255)  # 信号空間
    img_tensorC = tensor(img_resizeC/255)  # 単位空間（標準ベクトル）
    img_tensorR = tensor(img_resizeR/255)  # 信号空間
    
    return img_tensorL, img_tensorC, img_tensorR

# ---------------------------
# 両目法RTメトリックスを計算する(テンソル活用版)
def calc_deyeRTmet(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    lnr_yarray, btY1_yarray = [], []
    st_yarray, sb_yarray, se_yarray, ve_yarray = [], [], [], []
    yita_yarray, Y2_yarray = [], []

    # 有効除数の計算
    yuko = torch.dot(tsr_tani_array, tsr_tani_array)

    # 線形式の計算
    for i in range(len_temp):
        lnr_yitem  = torch.dot(tsr_sig_matrix[i], tsr_tani_array)
        btY1_yitem = lnr_yitem / yuko
        lnr_yarray.append(lnr_yitem.item())
        btY1_yarray.append(btY1_yitem.item())

    # 全変動ST及び各種中間指標SB,SE,VE,η
    for i in range(len_temp):
        sum_item  = torch.dot(tsr_sig_matrix[i], tsr_sig_matrix[i])
        st_yarray.append(sum_item.item())
        sb_yarray.append(lnr_yarray[i] ** 2 / yuko.item())
        se_yarray.append(st_yarray[i] - sb_yarray[i])

        # 異常処理
        temp_ve = se_yarray[i] / float(max_jy_index - 1.0)
        if temp_ve < 0.0001:
            ve_yarray.append(0.0)   
            yita_yarray.append(0.0)  
            Y2_yarray.append(0.0) 
        else:
            temp_Y2 = math.sqrt(temp_ve)
            ve_yarray.append(temp_ve)   
            yita_yarray.append(1.0 / temp_ve)  
            Y2_yarray.append(temp_Y2) 

    return round(Y2_yarray[0] - Y2_yarray[1], 5)

# ---------------------------
# 畳み込み処理を実施する
def apply_kernel(row, col, kernel, img_tensor):
    return (img_tensor[row:row+max_cnv_row,col:col+max_cnv_col] * kernel).sum()

# ---------------------------
# RTメトリックスを計算する(テンソル活用版)
def calc_RTmetrics(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    lnr_yarray, btY1_yarray = [], []
    st_yarray, sb_yarray, se_yarray, ve_yarray = [], [], [], []
    yita_yarray, Y2_yarray = [], []

    # 有効除数の計算
    yuko = torch.dot(tsr_tani_array, tsr_tani_array)

    # 線形式の計算
    for i in range(len_temp):
        lnr_yitem  = torch.dot(tsr_sig_matrix[i], tsr_tani_array)
        btY1_yitem = lnr_yitem / yuko
        lnr_yarray.append(lnr_yitem.item())
        btY1_yarray.append(btY1_yitem.item())

    # 全変動ST及び各種中間指標SB,SE,VE,η
    for i in range(len_temp):
        sum_item  = torch.dot(tsr_sig_matrix[i], tsr_sig_matrix[i])
        st_yarray.append(sum_item.item())
        sb_yarray.append(lnr_yarray[i] ** 2 / yuko.item())
        se_yarray.append(st_yarray[i] - sb_yarray[i])

        # 異常処理
        temp_ve = se_yarray[i] / float(max_jy_index - 1.0)
        if temp_ve < 0.0001:
            comment = 'ゼロ異常発生！！'
            print(comment)
            ve_yarray.append(-1.0)   
            yita_yarray.append(-1.0)  
            Y2_yarray.append(-1.0) 
        else:
            temp_Y2 = math.sqrt(temp_ve)
            ve_yarray.append(temp_ve)   
            yita_yarray.append(1.0 / temp_ve)  
            Y2_yarray.append(temp_Y2) 

    return btY1_yarray, Y2_yarray

#=================================================
# SUB PROGRAM(1) : 両目RTメトリックスの準備
#=================================================
# 1.両目RTメトリックス
# 画像リサイズのパラメタ
newsize = (900, 300)
# テンソルイメージ処理のパラメタ
dmax_tsr_col, dmax_tsr_row   = newsize[0], newsize[1]  #(900, 300)
# ストライド量
stride_tsr_row, stride_tsr_col  = 5, 5
# RT法処理のサイズ
drtm_tsr_row, drtm_tsr_col   = 5, 5
# RTベクトル長と信号空間長
dmax_jy_idx  = 5*5
len_dmx      = 2  # 右と左

# ---------------------------
# 1.両目RTメトリックス: テンソル行列の処理(for)開始点のベクトルを生成
row_range1, col_range1 = [], []
sum_row   = 0
while sum_row <= dmax_tsr_row-drtm_tsr_row:
    row_range1.append(sum_row)
    sum_row  += stride_tsr_row
#print(row_range1)    # len 60
# -----
sum_col   = 0
while sum_col <= dmax_tsr_col-drtm_tsr_col:
    col_range1.append(sum_col)
    sum_col  += stride_tsr_col
#print(col_range1)    # len 180

#=================================================
# SUB PROGRAM(2) : 畳み込みメトリックスの準備
#=================================================
# 2.畳み込み：画像の処理(for)開始点のベクトルを生成する
# パラメタの設定
max_cnv_parts  = 8
# 画像サンプルのサイズ
max_sp_row, max_sp_col  = 60, 180
# 畳み込み部品のサイズ
max_cnv_row, max_cnv_col  = 5, 5
# ストライド量
stride_cnv_row, stride_cnv_col  = 5, 5
# ---------------------------
# 2.畳み込み：画像の処理(for)開始点のベクトルを生成する
row_range2, col_range2 = [], []
sum_row   = 0
while sum_row <= max_sp_row-max_cnv_row:
    row_range2.append(sum_row)
    sum_row  += stride_cnv_row
# -----
sum_col   = 0
while sum_col <= max_sp_col-max_cnv_col:
    col_range2.append(sum_col)
    sum_col  += stride_cnv_col
# ------------------
nam_cnv_input = "基準用CSVデータ" 
code_cnv_input = ["NA"] * 8 # CSVコードの指定
code_cnv_input[0] = "bend1_cnv" # CSVコードの指定
code_cnv_input[1] = "bend2_cnv" # CSVコードの指定
code_cnv_input[2] = "bend3_cnv" # CSVコードの指定
code_cnv_input[3] = "bend4_cnv" # CSVコードの指定
code_cnv_input[4] = "line1_cnv" # CSVコードの指定
code_cnv_input[5] = "line2_cnv" # CSVコードの指定
code_cnv_input[6] = "datum1_cnv" # CSVコードの指定
code_cnv_input[7] = "datum2_cnv" # CSVコードの指定
# ------------------
# 畳み込み処理を実施する
for i in range(max_cnv_parts):
    # ---------------------------
    # CSVファイル(実験)情報を読み込み表示する
    # 畳み込みファイル
    file_cnv_input = foldername + code_cnv_input[i] + ".csv"  # ファイルパス名の生成 
    mx_conv = read_csvfile(file_cnv_input)
    if i == 0:    
        tensor_bend1 = tensor(mx_conv).float()
    elif i == 1:
        tensor_bend2 = tensor(mx_conv).float()
    elif i == 2:
        tensor_bend3 = tensor(mx_conv).float()
    elif i == 3:
        tensor_bend4 = tensor(mx_conv).float()
    elif i == 4:
        tensor_line1 = tensor(mx_conv).float()
    elif i == 5:
        tensor_line2 = tensor(mx_conv).float()
    elif i == 6:
        tensor_datum1 = tensor(mx_conv).float()
    elif i == 7:
        tensor_datum2 = tensor(mx_conv).float()
# ---------------------------
# 畳み込みカーネルを生成する
signal_kernels  = torch.stack([tensor_bend1, tensor_bend2, tensor_bend3, tensor_bend4, tensor_line1, tensor_line2])
tani_kernel     = tensor_datum1 + tensor_datum2

#=================================================
# SUB PROGRAM(3) : 畳み込みRTメトリックスの準備
#=================================================
# 3. 畳み込みRTメトリックス
# テンソルイメージ処理のパラメタ
cmax_tsr_row, cmax_tsr_col   = 12, 36
# ストライド量
stride_tsr_row, stride_tsr_col  = 4, 4
# RT法処理のサイズ
crtm_tsr_row, crtm_tsr_col   = 4, 4
# RTベクトル長と信号空間長
cmax_jy_idx, len_cmx  = 4*4, 6
# ---------------------------
# テンソル行列の処理(for)開始点のベクトルを生成する
row_range3, col_range3 = [], []
sum_row   = 0
while sum_row <= cmax_tsr_row-crtm_tsr_row:
    row_range3.append(sum_row)
    sum_row  += stride_tsr_row
#print(row_range3)
# -----
sum_col   = 0
while sum_col <= cmax_tsr_col-crtm_tsr_col:
    col_range3.append(sum_col)
    sum_col  += stride_tsr_col
#print(col_range3)

#=================================================
# FUNCTION : 両目畳み込みRTメトリックスを連続して実行する
#=================================================
def chained_process(img_tensorL, img_tensorC, img_tensorR):

    #=================================================
    # MAIN PROGRAM(1) : 両目RTメトリックスを実行する
    #=================================================
    # 両目RT法によるマトリックスの生成
    num_rows, num_cols = len(row_range1), len(col_range1)
    deyeY2_pool = []
    # ------
    cnt_row = 0
    for row in row_range1:
        cnt_col = 0
        for col in col_range1:
            # ------
            # 単位空間の空間ベクトルを生成する
            tsr_tani_array = img_tensorC[row:row+drtm_tsr_row,col:col+drtm_tsr_col].flatten()
            #print(tsr_tani_array)
            # ------
            # 信号空間の空間マトリックスを生成する
            tsr_sigL = img_tensorL[row:row+drtm_tsr_row,col:col+drtm_tsr_col].flatten()
            tsr_sigR = img_tensorR[row:row+drtm_tsr_row,col:col+drtm_tsr_col].flatten()
            # -----
            tsr_sig_matrix = torch.stack([tsr_sigL, tsr_sigR])
            #print("----- tsr_sig_matrix -----")
            #print(tsr_sig_matrix)
            # ------
            # 両目法RTメトリックスを計算する
            deyeY2 = calc_deyeRTmet(len_dmx, dmax_jy_idx, tsr_sig_matrix, tsr_tani_array)
            deyeY2_pool.append(deyeY2)
            # ------
            # COUNT UP
            cnt_col = cnt_col + 1
        # ------
        # COUNT UP.append
        cnt_row = cnt_row + 1

    # ---------------------------
    # 結果をテキスト出力する
    mx_deyeY2 = np.array(deyeY2_pool).reshape([num_rows, num_cols])
    tensor_deyeY2 = tensor(mx_deyeY2)
    #print(tensor_deyeY2)

    #=================================================
    # MAIN PROGRAM(2) : 畳み込みメトリックスを実行する
    #=================================================
    # 単位空間用の畳み込み処理を実施する
    tani_timage = tensor([[apply_kernel(i,j, tani_kernel, tensor_deyeY2) for j in col_range2] for i in row_range2])
    #print("----- tani_timage -----")
    #print(tani_timage)
    # ------------------
    # 信号空間用の畳み込み処理を実施する
    for i in range(max_cnv_parts-2):
        # ---------------------------
        # CSVファイル(実験)情報を読み込み表示する
        kernel = signal_kernels[i]
        calc_conv = tensor([[apply_kernel(i,j, kernel, tensor_deyeY2) for j in col_range2] for i in row_range2])
        # -----
        if i == 0:    
            tsr_cvbend1 = tensor(calc_conv).float()
        elif i == 1:
            tsr_cvbend2 = tensor(calc_conv).float()
        elif i == 2:
            tsr_cvbend3 = tensor(calc_conv).float()
        elif i == 3:
            tsr_cvbend4 = tensor(calc_conv).float()
        elif i == 4:
            tsr_cvline1 = tensor(calc_conv).float()
        elif i == 5:
            tsr_cvline2 = tensor(calc_conv).float()
    # ---------------------------
    # 畳み込みテンソルイメージの生成  
    signal_timages = torch.stack([tsr_cvbend1, tsr_cvbend2, tsr_cvbend3, tsr_cvbend4, tsr_cvline1, tsr_cvline2])
    # 結果の出力
    #print(signal_timages.shape)     # torch.Size([6, 12, 36])
    #print("----- signal_timages[0] -----")
    #print(signal_timages[0])

    #=================================================
    # MAIN PROGRAM(3) : 畳み込みRTメトリックスを実行する
    #=================================================
    # 3. 畳み込みRTメトリックス
    # チェーンリストの初期化
    mx_btY1_chain, mx_Y2_chain = [], []
    # ---------------------------
    # RTメトリックスをCSVファイルに出力するための行列
    mx_p0Y1_out = np.zeros([3,9])
    mx_p1Y1_out = np.zeros([3,9])
    mx_p2Y1_out = np.zeros([3,9])
    mx_p3Y1_out = np.zeros([3,9])
    mx_p4Y1_out = np.zeros([3,9])
    mx_p5Y1_out = np.zeros([3,9])
    # ---
    mx_p0Y2_out = np.zeros([3,9])
    mx_p1Y2_out = np.zeros([3,9])
    mx_p2Y2_out = np.zeros([3,9])
    mx_p3Y2_out = np.zeros([3,9])
    mx_p4Y2_out = np.zeros([3,9])
    mx_p5Y2_out = np.zeros([3,9])
    # ---------------------------
    cnt_row = 0
    for row in row_range3:
        cnt_col = 0
        for col in col_range3:
            # ------
            # 単位空間の空間ベクトルを生成する
            tsr_tani_array = tani_timage[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
            #print(tsr_tani_array)
            # ------
            # 信号空間の空間マトリックスを生成する
            temp_matrix = signal_timages[0]
            tsr_sig_p0 = temp_matrix[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
            temp_matrix = signal_timages[1]
            tsr_sig_p1 = temp_matrix[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
            temp_matrix = signal_timages[2]
            tsr_sig_p2 = temp_matrix[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
            temp_matrix = signal_timages[3]
            tsr_sig_p3 = temp_matrix[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
            temp_matrix = signal_timages[4]
            tsr_sig_p4 = temp_matrix[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
            temp_matrix = signal_timages[5]
            tsr_sig_p5 = temp_matrix[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
            # -----
            tsr_sig_matrix = torch.stack([tsr_sig_p0, tsr_sig_p1, tsr_sig_p2, tsr_sig_p3, tsr_sig_p4, tsr_sig_p5])
            #print("----- tsr_sig_matrix -----")
            #print(tsr_sig_matrix)
            # ------
            # RTメトリックスを計算する
            btY1_yarray, Y2_yarray = calc_RTmetrics(len_cmx, cmax_jy_idx, tsr_sig_matrix, tsr_tani_array)
            # ------
            # CSV出力用のマトリックスを生成する
            mx_p0Y1_out[cnt_row, cnt_col] = round(btY1_yarray[0],3)
            mx_p1Y1_out[cnt_row, cnt_col] = round(btY1_yarray[1],3)
            mx_p2Y1_out[cnt_row, cnt_col] = round(btY1_yarray[2],3)
            mx_p3Y1_out[cnt_row, cnt_col] = round(btY1_yarray[3],3)
            mx_p4Y1_out[cnt_row, cnt_col] = round(btY1_yarray[4],3)
            mx_p5Y1_out[cnt_row, cnt_col] = round(btY1_yarray[5],3)
            # ---
            mx_p0Y2_out[cnt_row, cnt_col] = round(Y2_yarray[0],3)
            mx_p1Y2_out[cnt_row, cnt_col] = round(Y2_yarray[1],3)
            mx_p2Y2_out[cnt_row, cnt_col] = round(Y2_yarray[2],3)
            mx_p3Y2_out[cnt_row, cnt_col] = round(Y2_yarray[3],3)
            mx_p4Y2_out[cnt_row, cnt_col] = round(Y2_yarray[4],3)
            mx_p5Y2_out[cnt_row, cnt_col] = round(Y2_yarray[5],3)
            # ------
            # COUNT UP
            #print(cnt_row, cnt_col)
            cnt_col = cnt_col + 1
        # ------
        # COUNT UP
        cnt_row = cnt_row + 1

    # ---------------------------
    # Chain化する
    mx_btY1_chain = [mx_p0Y1_out, mx_p1Y1_out, mx_p2Y1_out, mx_p3Y1_out, mx_p4Y1_out, mx_p5Y1_out]
    mx_Y2_chain   = [mx_p0Y2_out, mx_p1Y2_out, mx_p2Y2_out, mx_p3Y2_out, mx_p4Y2_out, mx_p5Y2_out]
    # ---------------------------
    # 結果のテキスト出力する
    #print(mx_btY1_chain)
    #print(mx_Y2_chain)

    return mx_btY1_chain, mx_Y2_chain

#=================================================
# MAIN PROGRAM(10) : csvファイルを読み込む
#=================================================
# VR画像ファイルを読み込み表示する
def read_vrImgfile(file_readcsv): 
 
    # 畳み込みファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    dfL = df.query("camNO == 0")
    dfC = df.query("camNO == 1")
    dfR = df.query("camNO == 2")
    # ------------------
    # 選択項目の読み込み
    # Xsマトリックス
    mx_Xs = df.loc[:,"file_name":"label"].values
    arr_VRfL = dfL.loc[:,"file_name"].values
    arr_VRfC = dfC.loc[:,"file_name"].values
    arr_VRfR = dfR.loc[:,"file_name"].values
    #print("----- mx_Xs -----")
    #print(mx_Xs)
  
    return mx_Xs, arr_VRfL, arr_VRfC, arr_VRfR

# ---------------------------
# CSVファイル(実験)情報を読み込み表示する
# カメラは3種類(左 - 中央 - 右)
foldername = "./ARRAY_RTCNN/"
file_VRinput = foldername + "labels0_OK.csv"  # ファイルパス名の生成 
mx_Xs, arr_VRfL, arr_VRfC, arr_VRfR = read_vrImgfile(file_VRinput)
num_VRfs = len(arr_VRfC)

# ---------------------------
# リストを生成する
acc_p0Y1, acc_p1Y1, acc_p2Y1, acc_p3Y1, acc_p4Y1, acc_p5Y1,  = [], [], [], [], [], []
acc_p0Y2, acc_p1Y2, acc_p2Y2, acc_p3Y2, acc_p4Y2, acc_p5Y2,  = [], [], [], [], [], []
# -----
for i in range(num_VRfs):   # num_VRfs
    # ---------------------------
    # 読み込み先の指定
    pic_nameL = arr_VRfL[i] + ".png"   # 左カメラ画像(普通色)
    pic_nameC = arr_VRfC[i] + ".png"   # 中央カメラ画像(普通色)
    pic_nameR = arr_VRfR[i] + ".png"   # 右カメラ画像(普通色)
    # ---------------------------
    # LCR画像ファイルを読んで、テンソルを出力する
    img_tensorL, img_tensorC, img_tensorR = read_LCRpics(pic_nameL, pic_nameC, pic_nameR)
    #print(img_tensorC)
    # ---------------------------
    # チェーン化された感度とSN比を出力
    mx_btY1_chain, mx_Y2_chain = chained_process(img_tensorL, img_tensorC, img_tensorR)
    # 結果のテキスト出力する
    #print("---- NO:{} ----".format(i))
    #print(mx_btY1_chain[0])
    #print(mx_Y2_chain[0])
    # ---------------------------
    # チェーン化された感度とSN比を分解して再構成する
    if i == 0:
        acc_p0Y1 = np.array(mx_btY1_chain[0]).flatten()
        acc_p1Y1 = np.array(mx_btY1_chain[1]).flatten()
        acc_p2Y1 = np.array(mx_btY1_chain[2]).flatten()
        acc_p3Y1 = np.array(mx_btY1_chain[3]).flatten()
        acc_p4Y1 = np.array(mx_btY1_chain[4]).flatten()
        acc_p5Y1 = np.array(mx_btY1_chain[5]).flatten()
        # -----
        acc_p0Y2 = np.array(mx_Y2_chain[0]).flatten()
        acc_p1Y2 = np.array(mx_Y2_chain[1]).flatten()
        acc_p2Y2 = np.array(mx_Y2_chain[2]).flatten()
        acc_p3Y2 = np.array(mx_Y2_chain[3]).flatten()
        acc_p4Y2 = np.array(mx_Y2_chain[4]).flatten()
        acc_p5Y2 = np.array(mx_Y2_chain[5]).flatten()
    else:
        acc_p0Y1 = np.hstack([acc_p0Y1, np.array(mx_btY1_chain[0]).flatten()]) 
        acc_p1Y1 = np.hstack([acc_p1Y1, np.array(mx_btY1_chain[1]).flatten()]) 
        acc_p2Y1 = np.hstack([acc_p2Y1, np.array(mx_btY1_chain[2]).flatten()]) 
        acc_p3Y1 = np.hstack([acc_p3Y1, np.array(mx_btY1_chain[3]).flatten()]) 
        acc_p4Y1 = np.hstack([acc_p4Y1, np.array(mx_btY1_chain[4]).flatten()]) 
        acc_p5Y1 = np.hstack([acc_p5Y1, np.array(mx_btY1_chain[5]).flatten()]) 
        # -----
        acc_p0Y2 = np.hstack([acc_p0Y2, np.array(mx_Y2_chain[0]).flatten()])
        acc_p1Y2 = np.hstack([acc_p1Y2, np.array(mx_Y2_chain[1]).flatten()])
        acc_p2Y2 = np.hstack([acc_p2Y2, np.array(mx_Y2_chain[2]).flatten()])
        acc_p3Y2 = np.hstack([acc_p3Y2, np.array(mx_Y2_chain[3]).flatten()])
        acc_p4Y2 = np.hstack([acc_p4Y2, np.array(mx_Y2_chain[4]).flatten()])
        acc_p5Y2 = np.hstack([acc_p5Y2, np.array(mx_Y2_chain[5]).flatten()])
# ---------------------------
# データを結合して単位空間を生成する
acmx_Y1  = np.vstack([acc_p0Y1, acc_p1Y1, acc_p2Y1, acc_p3Y1, acc_p4Y1, acc_p5Y1])
acmx_Y2  = np.vstack([acc_p0Y2, acc_p1Y2, acc_p2Y2, acc_p3Y2, acc_p4Y2, acc_p5Y2])
mx_Xs_tani  = np.concatenate([acmx_Y1, acmx_Y2], axis=0)
# 結果のテキスト出力する
#print(mx_Xs_tani.shape)

#=================================================
# MAIN PROGRAM(11) : 逆行列の計算
#=================================================
# 項目数（感度6件、SN比6件）
max_iRow_items = 12
# -----
arr_Xs_tani = []
for kItem in range(max_iRow_items):
    # 中央値を計算する
    mean_X_tani = np.mean(mx_Xs_tani[kItem, :])
    arr_Xs_tani.append(mean_X_tani)
#print("---- arr_Xs_tani ----")
#print(arr_Xs_tani)

# ------------------
# 単位空間を用いた学習
# 分散共分散行列を計算する
cov = np.cov(mx_Xs_tani)
# 分散共分散行列の逆行列を計算する
cov_i = np.linalg.pinv(cov)
#print("---- cov_i ----")
#print(cov_i)

#=================================================
# MAIN PROGRAM(12) : CSVファイルに学習結果を出力する
#=================================================
# OUT CSV FUNCTION(12-1)
#=================================================
# CSVファイルに出力する(分散共分散逆行列)
def save_invcorr(mx_result): 

    # ---------------------------
    # データフレームを作成
    arr_index = list(range(max_iRow_items))
    arr_columns = list(range(max_iRow_items))
    df_metrics = pd.DataFrame(mx_result, index=arr_index, columns=arr_columns)
    #print("------------ 分散共分散逆行列のデータフレーム -------------")  
    #print(df_metrics) 
    # CSV ファイル (RTm_invcorr.csv) として出力
    file_csvout = foldername + "RTm_invcorr.csv" # ファイルパス名の生成
    df_metrics.to_csv(file_csvout)
    print("CSVファイルの出力完了 : {}".format(file_csvout)) 
    
    return df_metrics

# ---------------------------
# CSVファイルに出力する(相関逆行列)
# ---------------------------
save_invcorr(cov_i)

#=================================================
# OUT CSV FUNCTION(12-2)
#=================================================
# CSVファイルに出力する(単位空間の中心)
def save_taniXs(mx_result): 

    # ---------------------------
    # データフレームを作成
    arr_columns = list(range(max_iRow_items))
    df_metrics = pd.DataFrame(mx_result, columns=arr_columns)
    #print("------------ 単位空間の中心のデータフレーム -------------")  
    #print(df_metrics) 
    # CSV ファイル (RTm_taniXs.csv) として出力
    file_csvout = foldername + "RTm_taniXs.csv" # ファイルパス名の生成
    df_metrics.to_csv(file_csvout)
    print("CSVファイルの出力完了 : {}".format(file_csvout)) 
    
    return df_metrics

# ---------------------------
# CSVファイルに出力する(単位空間の中心)
# ---------------------------
save_taniXs([arr_Xs_tani])

```

QEU:FOUNDER ： “例によって、以下のようなデータが出力されます。”

![image3-46-2](https://yaber1965.github.io/images/image3-46-2.jpg)

D先生 ： “もうちょっと違うアプトプットがないんですか？”

QEU:FOUNDER ： “じゃあ、これら（学習データ）を使って検査をしてみましょう。ワークは前回と同じ条件です。まずは端子の傾き系列からドン・・・。”

![image3-46-3](https://yaber1965.github.io/images/image3-46-3.jpg)

D先生 ： “学習量を増やした効果もあり、全体的に分布のクセが少なくなっています。あれれ？41ってPINNOでコネクタの中心でしょ？最大距離が発生する位置がずれやすいですよね。傾き量が少ないためなのでしょうか・・・。”

QEU:FOUNDER ： “う～ん・・・、次は端子が垂直に抜けた場合のヒートマップね。”

![image3-46-4](https://yaber1965.github.io/images/image3-46-4.jpg)

D先生 ： “これもちょっとねぇ・・・。”

QEU:FOUNDER ： “最後に良品（端子抜けなし）の結果をドン！！”

![image3-46-5](https://yaber1965.github.io/images/image3-46-5.jpg)

D先生 ： “これでは誤作動が多すぎでしょう・・・。あっ、前回の予測精度の結果を思い出した。”

（注：これは前回の条件の結果です）

![image3-46-6](https://yaber1965.github.io/images/image3-46-6.jpg)

QEU:FOUNDER ： “単位空間がわるいんでしょう。・・・というか、**検査ジグ**かな？”

D先生 ： “治具の光源とワーク回転の変動レベルはどれぐらいですか・・・。”

QEU:FOUNDER ： “ワークの回転量は±2度、光源明るさは各ランプが500-300の間を独立して動きます。”

D先生 ： “なんと、**VRで検査ジグの評価もできるようになる**とは・・・。”

QEU:FOUNDER ： “しょうがない。もう一度、検査ジグをチューニングして、満足のいく精度がでるようになればSVMの学習段階に進むようにしましょう。”

## ～　まとめ　～

C部長 : “イケメンバトルに行きましょうか・・・？”

![image3-46-7](https://yaber1965.github.io/images/image3-46-7.jpg)

QEU:FOUNDER ： “じゃ、コレ・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/yTxdddkhTcU" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 ： “前半が「なんじゃこりゃ」状態・・・。”

QEU:FOUNDER ： “あれ？おもしろくないの・・・？”

D先生 ： “フン、まだまだマーケティングが足らんね・・・。私のイケメンは、オリジナル飲料水の販売開始だ！これでどうだ！！”

<iframe width="560" height="315" src="https://www.youtube.com/embed/16Zs-Kpv6OY?controls=0&amp;start=959" title="YouTube video player" frameborder="0" al-low="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ： “はぁ？”

