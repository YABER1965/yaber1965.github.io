## QEUR21_RL2048T11:　Feature Engineering(その1) ～ 新RTメトリックスで学習する

## ～　DQN with Experience Replayでやってみたが・・・　～

**（今回のパフォーマンス：新RT法）**

![imageRL1-8-1](https://yaber1965.github.io/images/imageRL1-8-1.jpg)

QEU:FOUNDER ： “やってみたよ・・・。ついでに、念のためにActor-Criticでもやってみた。実は前回の2048プロジェクトの最後の部分でやったからね。今回は、メトリックスは「畳み込みRTメトリックス」を使いました。”

![imageRL1-8-2](https://yaber1965.github.io/images/imageRL1-8-2.jpg)

D先生 ： “10000回も学習したので、すこしは効果がありそうなものですが・・・（ダメダメ）。前回はある程度のスコアアップをしたんでしょ？”

QEU:FOUNDER ： “以前は、ボードの角部に数字が高いと報酬を高くするとかのトリックをつけました。今回は、それらを全部消しました。「素うどん」の強化学習で、どこまでできるのか・・・。”

![imageRL1-8-2A](https://yaber1965.github.io/images/imageRL1-8-2A.jpg)

QEU:FOUNDER ： “Actor-Critics(AC)はメイズ（↑、迷路）のように、損失が小さい合理的な解を見出す場合に使いやすいんでしょうね。ACはイプシロン-Greedyによる探索ができないしくみですから、システムが冒険をしない・・・。あとはプログラムにいきましょう。でも、強化学習のエコシステムのうち、環境(environment)のプログラムについては載せていません。”

![imageRL1-8-3](https://yaber1965.github.io/images/imageRL1-8-3.jpg)

D先生 ： “他の人のシステムを使ったんでしょ？”

![imageRL1-8-4](https://yaber1965.github.io/images/imageRL1-8-4.jpg)

QEU:FOUNDER ： “AGENT側のプログラムがENVIRONMENT側のクラスを読み込んで、直接操作すればいいですからね。あとは、AGENT側のプログラムをドン・・・。”

```python
# ----------------
# 2048ゲームの強化学習システム
# step1 : 畳み込み(RT)を動的メトリックスとして活用する
# step1 : DQN_newRT_game2048_agent.py
# step1 : AACと畳み込みRT法でデータを生成します
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import copy, random, time
import pandas as pd
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
#%matplotlib inline
# ---------------- 
# environment
import RT_auto_game_2048_2

# =================================================
# difinition of function(1) - ONEHOT関連
# =================================================
# [array]-action をonehot変換します。
def create_arronehot(action):
    # ----------------
    # action number / onehot
    # 0 / 1000
    # 1 / 0100
    # 2 / 0010
    # 3 / 0001
    # ----------------
    # ONEHOTに変換
    if action == 0:
        arr_onehot = [1.0, 0.0, 0.0, 0.0]
    elif action == 1:
        arr_onehot = [0.0, 1.0, 0.0, 0.0]
    elif action == 2:
        arr_onehot = [0.0, 0.0, 1.0, 0.0]
    elif action == 3:
        arr_onehot = [0.0, 0.0, 0.0, 1.0]

    return np.array(arr_onehot)


# =================================================
# difinition of function(2)
# =================================================
# イレギュラー検出関数(タイル移動後の状態とスコア)
def is_invalid_action(state, a_order):
    # ----------------
    spare = copy.deepcopy(state)  # 状態
    #print("spare",type(spare))
    reward = 0  # 当該行動による報酬
    if a_order == 0:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y, x - z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, x - z_cnt] *= 2
                    reward += spare[y, x - z_cnt]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 1:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y - z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y - z_cnt, x] *= 2
                    reward += spare[y - z_cnt, x]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 2:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, 3 - x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, 3 - x] != prev:
                    tmp = spare[y, 3 - x]
                    spare[y, 3 - x] = 0
                    spare[y, 3 - x + z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, 3 - x + z_cnt] *= 2
                    reward += spare[y, 3 - x + z_cnt]
                    spare[y, 3 - x] = 0
                    prev = -1
    elif a_order == 3:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[3 - y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[3 - y, x] != prev:
                    tmp = state[3 - y, x]
                    spare[3 - y, x] = 0
                    spare[3 - y + z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[3 - y + z_cnt, x] *= 2
                    reward += spare[3 - y + z_cnt, x]
                    spare[3 - y, x] = 0
                    prev = -1

    if (state - spare).any() == False:
        return True, reward, spare
    else:
        return False, reward, spare


# ---------------------------
# 畳み込みパターンファイルを読み込み表示する
def read_csvfile(file_readcsv, max_idx, max_col): 
 
    # 画像異常検出ファイルの読み込み
    df = pd.read_csv(file_readcsv, header=None) 
    #print(df)
    # --------------------------------------------------
    # 選択項目の読み込み
    # 原因系Xs
    mx_Xs = df.iloc[0:max_idx,0:max_col].values
  
    return mx_Xs

    
# ---------------------------
# 畳み込み処理を実施する
def apply_kernel(row, col, kernel, img_tensor):
    return (img_tensor[row:row+max_cnv_idx,col:col+max_cnv_col] * kernel).sum()


# ---------------------------
# newRTメトリックスを計算する(テンソル活用版)
def calc_newRT(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    L1_loss = torch.nn.L1Loss()
    btY1_yarray, Y2_yarray = [], []

    # 繰り返し
    for i in range(len_temp):

        y = tsr_sig_matrix[i]
        x = tsr_tani_array

        xx = torch.dot(x,x)
        xy = torch.dot(x,y)
        beta = xy/xx
        mDistance   = L1_loss(y, beta*x)
        
        btY1_yarray.append(beta.item())
        Y2_yarray.append(mDistance.item())

    return torch.tensor(btY1_yarray).float(), torch.tensor(Y2_yarray).float()

# ----------------
# 移動可能な方向をまとめる
def calc_movables(state):        # 定义动作选择函数 (x为状态)

    # --------------------------------------------------
    # 配列の初期化
    spare = np.array(state)
    # --------------------------------------------------
    # オリジナル操作方法 -> 数字に変更します(ENVIRONMENT)
    # 数字を上へ -> 1
    # 数字を下へ -> 3
    # 数字を左へ -> 0
    # 数字を右へ -> 2
    # --------------------------------------------------
    a_map, movables, arr_rewards = [0, 1, 2, 3], [], []
    # --------------------------------------------------
    iCnt = 0
    for i_map in a_map:
        flg_invalid, val_reward, mx_spare_next = is_invalid_action(spare, i_map)
        if flg_invalid == False:
            arr_rewards.append(val_reward + 4)
            movables.append(i_map)
            iCnt = iCnt + 1
            
    num_avail = iCnt

    return num_avail, movables, arr_rewards

# ----------------
# 使用可能な配列を提示する
def make_metrics(state, tani_kernel, signal_kernels):
    # ---------------------------
    # テンソルの初期化
    spare = copy.deepcopy(state)
    tsr_spare = torch.tensor(spare).float()
    #print(tsr_spare)
    # ---------------------------
    # tsr_spareを対数化する
    for iRow in range(4):
        for jCol in range(4):
            raw_value = tsr_spare[iRow, jCol]
            # print("make_metrics - raw_value",raw_value)
            if raw_value > 0:
                tsr_spare[iRow, jCol] = math.log2(raw_value)
            else:
                tsr_spare[iRow, jCol] = 0

    # ---------------------------
    # feature-engineeringの計算(新RT法)
    # 単位空間用の畳み込みテンソルを生成する  
    tsr_tani_array = torch.tensor([[apply_kernel(i,j, tani_kernel, tsr_spare) for j in [0,1]] for i in [0,1]]).flatten()
    
    # ------------------
    # 信号空間用の畳み込みテンソルを生成する 
    for i in range(len_cmx):
        # ---------------------------
        # CSVファイル(実験)情報を読み込み表示する
        kernel = signal_kernels[i]
        calc_conv = torch.tensor([[apply_kernel(i,j, kernel, tsr_spare) for j in [0,1]] for i in [0,1]]).flatten()
        # -----
        if i == 0:    
            tsr_cvbend1 = calc_conv
        elif i == 1:
            tsr_cvbend2 = calc_conv
        elif i == 2:
            tsr_cvbend3 = calc_conv
        elif i == 3:
            tsr_cvbend4 = calc_conv
        elif i == 4:
            tsr_cvline1 = calc_conv
        elif i == 5:
            tsr_cvline2 = calc_conv
    tsr_sig_matrix = torch.stack([tsr_cvbend1, tsr_cvbend2, tsr_cvbend3, tsr_cvbend4, tsr_cvline1, tsr_cvline2])
    
    # ---------------------------
    # FEATURE-ENGINEERING(新RTメトリックスを計算する)
    btY1_yarray, Y2_yarray = calc_newRT(len_cmx, cmax_jy_idx, tsr_sig_matrix, tsr_tani_array)

    # arr_feature結果の表示
    arr_feature = np.hstack((btY1_yarray.numpy(), Y2_yarray.numpy()))

    return arr_feature


#=================================================
# agent_main class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 64)
        self.fc2 = torch.nn.Linear(64, 64)
        self.fc3 = torch.nn.Linear(64, dim_output)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Solving the maze in Deep Q-learning
class DQN_Solver:
    def __init__(self, state_size, action_size):
        self.state_size  = state_size
        self.action_size = action_size
        self.memory  = deque(maxlen=MEMORY_CAPACITY)
        self.gamma   = GAMMA
        self.epsilon = EPSILON
        self.e_decay = EPDECAY
        self.e_min   = EPMIN
        self.learning_rate = LRATE

        # --------------------------------------------------
        # crate instance for input
        self.model = Net()
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.model.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.model.eval()


    def remember_memory(self, state, action, reward, next_state, next_movables, done):
        self.memory.append((state, action, reward, next_state, next_movables, done))


    def choose_action(self, x_input_array, num_avail, movables, arr_rewards, iCnt_turn, iCnt_play):        

        # --------------------------------------------------
        # オリジナル操作方法 -> 数字に変更します(ENVIRONMENT)
        # 数字を上へ -> 1
        # 数字を下へ -> 3
        # 数字を左へ -> 0
        # 数字を右へ -> 2
        # --------------------------------------------------
        # 命令の初期選択
        a_order, acType, rl_Qvalue, rl_done = -1, "NA", -1, False
        mx_input    = []  # 状態(state)と行動(action)マトリックス
        val_boltz   = 10
        if (num_avail == 0):
            rl_done = True
            acType  = "gameover"
            print("ゲームオーバー -- iCnt_play: {0}, iCnt_turn: {1}".format(iCnt_play, iCnt_turn))
        else:
            if self.epsilon <= random.random():
                # DQNによる選択
                acType  = "machine"
                iCnt = 0
                for a in movables:
                    # --- model ---
                    # [array]-action をonehot変換します。
                    arr_onehot = create_arronehot(a)
                    temp_s_a = np.append(x_input_array, arr_onehot)
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.append(mx_input, [temp_s_a], axis=0)  # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                mx_input = np.array(mx_input)
                # print("----- mx_input -----")
                # print(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.model(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                rl_Qvalue   = np.max(y_pred)
                temp_action = np.argmax(y_pred)
                a_order     = movables[temp_action]
            else:            
                # 乱数による選択
                acType  = "random"
                qs_prob = softmax(np.array(arr_rewards) / val_boltz)
                a_order = np.random.choice(movables, p=qs_prob)
                
        # --------------------------------------------------
        if iCnt_turn%10 == 0:
            print("行動タイプ:{0}, プレイ:{1}, ターン:{2}, 行動選択結果:{3}".format(acType, iCnt_play, iCnt_turn, a_order))

        return a_order, acType, rl_Qvalue, rl_done


    def replay_experience(self, batch_size):

        # set training parameters
        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate)
        criterion = torch.nn.MSELoss()
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = np.array([]), np.array([])
        for ibat in range(batch_size):
            state, action, reward, next_state, next_movables, done = minibatch[ibat]
            # [array]-action をonehot変換します。
            arr_onehot = create_arronehot(action)
            state_action_curr = np.append(state, arr_onehot)
            if done:
                target_f = reward
            else:
                mx_input = []  # 状態(state)と行動(action)マトリックス
                iCnt = 0
                for imov in next_movables:
                    # [array]-action をonehot変換します。
                    arr_onehot = create_arronehot(imov)
                    temp_s_a = np.append(next_state, arr_onehot)
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.append(mx_input, [temp_s_a], axis=0)  # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                    # ----
                mx_input = np.array(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.model(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # -----
            if ibat == 0:
                X = [state_action_curr]  # 状態(S)行動(A)マトリックス
            else:
                X = np.append(X, [state_action_curr], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_next = torch.from_numpy(X).float()
        expected_q_value = torch.from_numpy(Y.reshape(-1, 1)).float()
        
        # --- building model ---
        q_value = self.model(state_action_next)

        # calculate loss
        loss = criterion(q_value, expected_q_value)

        # update weights
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Show progress
        print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return self.epsilon, loss.data.numpy()


#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # 記録用パラメタ類(プレイベース)の初期化
        self.arr_iCnt_play  = []  # count game play    プレイ番号
        self.arr_maxturn    = []  # max_turn   ゲームのターン数
        self.arr_csv_play   = []  # name game play    プレイファイル名のリスト
        self.arr_maxscore   = []  # rl_score game play    最終プレイスコア
        # ----------
        self.arr_epsilon    = []  # イプシロン
        self.arr_upright    = []  # 角のスコア（右上）
        self.arr_upleft     = []  # 角のスコア（左上）
        self.arr_downright  = []  # 角のスコア（右下）
        self.arr_downleft   = []  # 角のスコア（左下）
        # ----------
        # 統計データの初期化(プレイベース)
        self.arr_loss       = []  # 学習損失
        # ----------
        # AV値の分析用(プレイベース)
        self.arr_num_AV     = []  # AV値のN数
        self.arr_max_AV     = []  # AV値の最大値
        self.arr_q25_AV     = []  # AV値の4分の1値
        self.arr_q75_AV     = []  # AV値の4分の3値

        # --------------------------------------------------
        # CSV-playlist用のリスト
        self.arr_usefile    = np.zeros(num_episodes)        # usefile    CSVファイルを出力するか use 1, un-use 0
        self.arr_perform    = np.zeros(num_episodes)        # performance   命令の一致率
        
        # --------------------------------------------------
        gamepanel       = RT_auto_game_2048_2.Board()
        self.game2048   = RT_auto_game_2048_2.Game(gamepanel)
        self.game2048.gamepanel.gridCell = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])

        # --------------------------------------------------
        # 畳み込み用部品(8種類)を読み込む
        for i_cnv in range(max_cnv_parts):    # max_cnv_parts
            # -----
            # 畳み込みファイル
            file_cnv_input = foldername + code_cnv_input[i_cnv] + ".csv"  # ファイルパス名の生成 
            mx_conv = read_csvfile(file_cnv_input, max_cnv_idx, max_cnv_col)
            if i_cnv == 0:    
                tsr_bend1 = torch.tensor(mx_conv).float()
            elif i_cnv == 1:
                tsr_bend2 = torch.tensor(mx_conv).float()
            elif i_cnv == 2:
                tsr_bend3 = torch.tensor(mx_conv).float()
            elif i_cnv == 3:
                tsr_bend4 = torch.tensor(mx_conv).float()
            elif i_cnv == 4:
                tsr_line1 = torch.tensor(mx_conv).float()
            elif i_cnv == 5:
                tsr_line2 = torch.tensor(mx_conv).float()
            elif i_cnv == 6:
                tsr_datum1 = torch.tensor(mx_conv).float()
            elif i_cnv == 7:
                tsr_datum2 = torch.tensor(mx_conv).float()

        # --------------------------------------------------
        # 畳み込みカーネルを生成する
        signal_kernels  = torch.stack([tsr_bend1, tsr_bend2, tsr_bend3, tsr_bend4, tsr_line1, tsr_line2])
        tani_kernel     = tsr_datum1 + tsr_datum2

        # --------------------------------------------------
        # ゲーム2048をプレイする
        for iCnt_play in range(num_episodes):       # num_episodes
        
            num_maxturn, num_maxscore, arr_corner = self.get_episode(tani_kernel, signal_kernels, iCnt_play)

            # ----------
            # DQN-Experience Replay学習
            val_epsilon, loss = dql_solver.replay_experience(BATCH_SIZE)

            # ----------
            code_csvout = "NA"
            if iCnt_play%40 == 0:
                # CSV ファイル (file_csvout) として出力する
                code_csvout = "game2048_play{0}_score{1}.csv".format(iCnt_play, num_maxscore)       # file name  
                print("メトリックス保管用CSVファイル名 ：{0}".format(code_csvout))
                # ----------
                # ゲームデータの出力
                #code_csvout = self.save_csvGAME(iCnt_play, code_csvout)        # CSVファイルに保存する

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iCnt_play.append(iCnt_play)        # count game play    プレイ番号
            self.arr_maxturn.append(num_maxturn)        # max_turn   ゲームのターン数
            self.arr_csv_play.append(code_csvout)       # name game play    プレイファイル名のリスト
            self.arr_maxscore.append(num_maxscore)      # rl_score game play    最終プレイスコア
            self.arr_loss.append(loss)                  # DQN-Experience Replay学習
            # ----------
            self.arr_epsilon.append(val_epsilon)        # イプシロン
            self.arr_upright.append(arr_corner[0])      # 角のスコア（右上）
            self.arr_upleft.append(arr_corner[1])       # 角のスコア（左上）
            self.arr_downright.append(arr_corner[2])    # 角のスコア（右下）
            self.arr_downleft.append(arr_corner[3])     # 角のスコア（左下）
            # ----------
            #print(f"Learn End(main) -- episode:{iCnt_play}, DL_epoch:{epoch}, DL_loss:{loss}")            

            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%20 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する(1)
        
        x = list(range(num_episodes))
        # print(x) 
        
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(x, self.arr_epsilon)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Corner score')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('number')
        ax2.plot(x, self.arr_upright, label="upright", color="red")
        ax2.plot(x, self.arr_upleft, label="upleft", color="blue")
        ax2.plot(x, self.arr_downright, label="downright", color="orange")
        ax2.plot(x, self.arr_downleft, label="downleft", color="green")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph(num_episodes)

        # --------------------------------------------------
        # PyTorchモデルの保存
        #torch.save(ActorCritic.state_dict(), file_output_model)


    # --------------------------------------------------
    def get_episode(self, tani_kernel, signal_kernels, iCnt_play):

        # --------------------------------------------------
        acType, val_reward, rl_done, a_order  = "random", 0, False, 0
        state    = np.array([[0]*4 for i in range(4)])
        a_order_prev, ttlscore_prev = 0, 0
        iCnt_turn , num_maxturn, num_maxscore = 0, 0, 0
        
        # --------------------------------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_icount = []        # カウンタリスト
        self.arr_order = []         # 指示リスト
        self.arr_avail = []         # 使用可能な命令数リスト
        self.arr_ttlscore = []      # ゲームスコアリスト(真数)  
        self.arr_reward = []        # 報酬リスト(真数)  
        self.arr_yzero = []         # ゼロ・スコアの数
        self.arr_acType = []        # 指示タイプ(random,machine)
        self.arr_predAV = []        # DL予測されたAV値

        # --------------------------------------------------
        # ゲーム・エピソードを行う
        self.reset()  # 環境をﾘｾｯﾄする
        state = self.game2048.gamepanel.gridCell
        s     = make_metrics(state, tani_kernel, signal_kernels)
        num_avail, movables, arr_rewards = 4, [0,1,2,3], [0,0,0,0]
        iCnt_turn, ttlscore, ttlscore_prev= 0, 0, 0
        while True: 
        
            # --------------------------------------------------
            # 命令を選択する
            a_order, acType, rl_Qvalue, rl_done = dql_solver.choose_action(s, num_avail, movables, arr_rewards, iCnt_turn, iCnt_play) 
            # ------
            arr_corner = np.zeros(4)
            arr_corner[0] = state[0][3]   # arr_upright , 角のスコア（右上）
            arr_corner[1] = state[0][0]   # self.arr_upleft , 角のスコア（左上）
            arr_corner[2] = state[3][3]   # self.arr_downright , 角のスコア（右下）
            arr_corner[3] = state[3][0]   # self.arr_downleft , 角のスコア（左下）

            # --------------------------------------------------
            # 環境(ENVIRONMENT)を実行する
            state_next, ttlscore, done = self.game2048.link_keys(a_order)
            s_temp = make_metrics(state_next, tani_kernel, signal_kernels)
            num_avail, next_movables, arr_rewards = calc_movables(state_next)

            # 報酬を計算する
            val_reward = ttlscore - ttlscore_prev

            # --------------------------------------------------
            # 学習データの積み上げ
            # 変数の変換(machine, random)
            s       = s
            a       = a_order
            r       = val_reward 
            s_      = s_temp

            # Experience Replay配列に保管する
            if num_avail > 0:
                dql_solver.remember_memory(s, a, r, s_, next_movables, done)

            # --------------------------------------------------  
            self.arr_icount.append(iCnt_turn)       # カウンタリスト
            self.arr_order.append(a_order)        # 指示リスト
            self.arr_avail.append(num_avail)     # 使用可能な命令数リスト
            self.arr_ttlscore.append(ttlscore)      # ゲームスコアリスト(真数)  
            self.arr_reward.append(val_reward)       # 報酬リスト(真数)  
            self.arr_acType.append(acType)       # 指示タイプ(random,machine)     
            self.arr_predAV.append(rl_Qvalue)      # DL予測されたAV値
            # ----------   
            board_flatten = np.array(state).flatten()
            cc = Counter(board_flatten)
            self.arr_yzero.append(cc[0]) 

            # --------------------------------------------------
            # 次回への引き渡し
            s               = s_temp
            state           = state_next
            movables        = next_movables
            ttlscore_prev   = ttlscore
            iCnt_turn       = iCnt_turn + 1

            if done:       # 如果done为True
                print("# of episode :{}, turn : {}, ttlscore : {:.1f}".format(iCnt_play, iCnt_turn, ttlscore))
                break      

        # --------------------------------------------------  
        # AV値の分析用配列群の計算(プレイベース)
        arr_temp = []
        for i in range(len(self.arr_predAV)):
            if self.arr_predAV[i] > 0:
                arr_temp.append(self.arr_predAV[i])
        # ----------------
        if len(arr_temp) < 5:
            self.arr_num_AV.append(iCnt_turn)                    # AV値のN数
            self.arr_max_AV.append(-1)                                  # AV値の最大値
            self.arr_q25_AV.append(-1)                                      # AV値の4分の1値
            self.arr_q75_AV.append(-1)                                      # AV値の4分の3値
        else:
            self.arr_num_AV.append(iCnt_turn)                    # AV値のN数
            self.arr_max_AV.append(np.max(self.arr_predAV))                 # AV値の最大値
            self.arr_q25_AV.append(np.percentile(self.arr_predAV, q=25))    # AV値の4分の1値
            self.arr_q75_AV.append(np.percentile(self.arr_predAV, q=75))    # AV値の4分の3値

        # -------------------------------------------------- 
        # プレイリストに引き渡す変数
        num_maxturn     = iCnt_turn
        num_maxscore    = ttlscore

        return num_maxturn, num_maxscore, arr_corner


    # ----------------
    # ゲームをﾘｾｯﾄする
    def reset(self):   
    
        state_init = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])
    
        # --------------------------------------------------
        # パラメタの初期化(Panel)
        self.game2048.gamepanel.gridCell=state_init
        self.game2048.gamepanel.compress=False
        self.game2048.gamepanel.merge=False
        self.game2048.gamepanel.moved=False
        self.game2048.gamepanel.score=0
        
        # --------------------------------------------------
        # パラメタの初期化(Game)
        self.game2048.end=False
        self.game2048.won=False

        # --------------------------------------------------
        # 記録用リストの初期化(ターンベース)
        self.arr_icount = []        # カウンタリスト
        self.arr_order = []         # 指示リスト
        self.arr_avail = []         # 使用可能な命令数リスト
        self.arr_ttlscore = []      # ゲームスコアリスト(真数)  
        self.arr_reward = []        # 報酬リスト(真数)  
        self.arr_yzero = []         # ゼロ・スコアの数
        self.arr_acType = []        # 指示タイプ(random,machine)
        self.arr_predAV = []        # DL予測されたAV値


    # ----------------
    # 学習結果のグラフ化
    def show_graph(self, num_episodes):

        x = list(range(num_episodes))
        # print(x)

        fig = plt.figure(figsize=(12, 10))
        # -----
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : loss')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('Learn-loss')
        ax1.grid(True)
        ax1.plot(x, self.arr_loss)
        # -----
        ax2 = fig.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : QValues')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('QValue')
        ax2.grid(True)
        ax2.plot(x, self.arr_q25_AV, label="q25_AV", color="blue")
        ax2.plot(x, self.arr_q75_AV, label="q75_AV", color="red")
        ax2.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()

        ax3 = fig.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(x, self.arr_maxturn, label="original", color="blue")
        ax3.plot(x, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        
        ax4 = fig.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(x, self.arr_maxscore, label="original", color="blue")
        ax4.plot(x, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # DQN Parameter
    state_size  = 12
    action_size = 4
    # -----
    dim_input   = state_size + action_size
    dim_output  = 1

    # ---------------------------
    # ハイパーパラメタ
    BATCH_SIZE = 128                          # サンプルサイズ
    LRATE = 0.005                            # 学習率
    EPSILON = 0.99                            # greedy policy
    EPDECAY = 0.998
    EPMIN = 0.01
    GAMMA = 0.95                             # reward discount
    MEMORY_CAPACITY = 3000                   # メモリ容量
    # ---------------------------
    SLEEP_TIME      = 0.01
    print_interval  = 50                     # 出力頻度
    num_episodes    = 5000                   # 繰り返し回数

    # ---------------------------
    # パラメタの設定
    max_cnv_parts = 8       # 畳み込み部品数
    len_cmx       = max_cnv_parts - 2  # 信号空間の数
    cmax_jy_idx   = 4       # RT法のマトリックス
    # ---------------------------
    max_sp_idx  = 4          # 盤面のidx数
    max_cnv_idx = 3          # 畳み込み部品のidx数
    max_sp_col  = 4          # 盤面のidx数
    max_cnv_col = 3          # 畳み込み部品のidx数

    # ---------------------------
    # フォルダ名の指定
    foldername = "./ARRAY_RTCNN/"  # My project folder

    # ---------------------------
    # solver
    dql_solver  = DQN_Solver(state_size, action_size)

    # ---------------------------
    # 入力用：畳み込み部品のCSVファイルを定義する
    nam_cnv_input = "畳み込み部品用CSVデータ" 
    code_cnv_input = ["NA"] * 8 # CSVコードの指定
    code_cnv_input[0] = "bend1_cnv" # CSVコードの指定
    code_cnv_input[1] = "bend2_cnv" # CSVコードの指定
    code_cnv_input[2] = "bend3_cnv" # CSVコードの指定
    code_cnv_input[3] = "bend4_cnv" # CSVコードの指定
    code_cnv_input[4] = "line1_cnv" # CSVコードの指定
    code_cnv_input[5] = "line2_cnv" # CSVコードの指定
    code_cnv_input[6] = "datum1_cnv" # CSVコードの指定
    code_cnv_input[7] = "datum2_cnv" # CSVコードの指定
    print("---- 入力CSVファイル名 ----")
    print(code_cnv_input)
    print("----------------------")

    # ---------------------------
    # 出力用Pytorchモデルのファイル名
    comment_output_model = "game2048_DQN"
    code_output_model = "model_game2048_DQN.pt"
    file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # DQN-ERで学習する
    Agent()

```

QEU:FOUNDER ： “これが今回の結論です。・・・もっとも、5000回程度の学習では全然足らないのだが・・・。本当に効果を確認したいのであれば50000回はプログラムを回さないと・・・。”

**(イプシロン値とゲーム盤角部の数値の推移)**

![imageRL1-8-5](https://yaber1965.github.io/images/imageRL1-8-5.jpg)

**(その他のパフォーマンスの推移)**

![imageRL1-8-6](https://yaber1965.github.io/images/imageRL1-8-6.jpg)

D先生 ： “なるほど・・・。今回は5000回で打ち切りましたが、50000回もやればブレークスルーしたのかもしれないですね。でも、我々が知りたいのは**「テクノメトリックスの工夫による学習加速」**です。”

QEU:FOUNDER ： “この場合、**Q値が評価のヒントになるかも**ね・・・。何はともあれ、まずはベースラインの設定が終わったわけです。”

D先生 ： “じゃ、次に行きましょう。次は、どんなメトリックスをやってみますか？”

QEU:FOUNDER ： “昔やったメトリックスを使って試してみましょう。ただし、あまり期待できないけど・・・(笑)。都合上5000回しか学習をしていませんが、もしPCがより強力になると50000回分の学習ができるようになります。つきましてはカンパください！！”

### [＞寄付のお願い(Donate me)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

D先生 ：“よろしくお願いします。”

## ～　まとめ　～

QEU:FOUNDER ： “なんか、最近、変な話を聞いたよ・・・。”

![imageRL1-8-7](https://yaber1965.github.io/images/imageRL1-8-7.jpg)

QEU:FOUNDER ： “海外の「とある場所」に進出して大儲けしたD社（仮）が、**本来の予想とは逆に従業員のコスト削減を進め、それが公になったため新聞沙汰**になり、かなり客足が遠くなったという話・・・。”

D先生 : “その経営者、「〇カ（馬〇）」じゃないでしょうか？**「労働力＝消費者」**なんでしょう・・・？”

QEU:FOUNDER ： “D社（仮）の母国では、そのような基本的な認識がないでしょう・・・。ちなみに、そこの従業員が受けたことの細かな内容はFACEBOOKのグループでシェアされています。そもそも、その母国には、この手の「シェア」がないですね。人間が分断されている・・・。”
\
![imageRL1-8-8](https://yaber1965.github.io/images/imageRL1-8-8.jpg)

D先生 : “分断されると弱くなる・・・。近年起こった種々の問題の根本なんでしょうね。”

C部長 : “そもそも**「クミアイ」**ってないんですか？”

![imageRL1-8-9](https://yaber1965.github.io/images/imageRL1-8-9.jpg)

QEU:FOUNDER ： “ちゃんと「仕事」をしていますよ。どっかの国とは違って・・・（笑）。”

C部長 : “気がめいってきた・・・。”
