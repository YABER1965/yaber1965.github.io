## QEUR21_TYNLP2:　自然言語解析(その3)

## ～　「ベースライン」というか、「ベンチマーク」・・・　～

### ・・・　前回のつづき　・・・

D先生 ： “エンベディングが終わったので、あとはディープラーニングで予測するだけですね。”

![image2-72-1](https://yaber1965.github.io/images/image2-72-1.jpg)

QEU:FOUNDER ： “今回はエンベッドのインプットを受けて、ディープラーニング(DL)による学習だね。DLにはあんまり興味なし・・・・で、プログラムをドン！！前回にダブる部分は省略します。”

```python
# -------
#
# 自然言語処理(NLP)をディープラーニングで解く
# その2(ディープラーニング)
# nlp_example_LSTM.py
#
# -------
import re, numpy as np, pandas as pd
from tensorflow import keras
from tensorflow.keras.optimizers import Adam # - Works
# -------
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers
from sklearn.model_selection import train_test_split
# -------
import matplotlib.pyplot as plt

・・・　中略　・・・

# -------
# 言語データのトークン化
tokenizer= Tokenizer(num_words=max_features,lower= True)
tokenizer.fit_on_texts(list(x_train))
tokenized_train =tokenizer.texts_to_sequences(x_train)
tokenized_test  =tokenizer.texts_to_sequences(x_test)
train_x =pad_sequences(tokenized_train,maxlen=max_len)
test_x  =pad_sequences(tokenized_test,maxlen=max_len)
# -------
num_sample = 50
print('---- NO:{} ----'.format(num_sample))
print('Sample cleaned: ', x_train[num_sample])
print('------')
print('Sample embedded: ', train_x[num_sample])

# -------
# モデルの設定(LSTM)
inp = Input(shape=(max_len,))
x = Embedding(max_features,embed_size)(inp)
x = Bidirectional(LSTM(64, return_sequences=True))(x)
x = GlobalMaxPool1D()(x)
x= Dropout(0.15)(x)
x= Dense(6,activation="sigmoid") (x)
model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy',optimizer= Adam(learning_rate=3e-4),metrics=['accuracy'])
print(model.summary())
Model: "model_1"
#=================================================================
# Layer (type)                Output Shape              Param #   
#=================================================================
# input_2 (InputLayer)        [(None, 100)]             0         
# embedding_1 (Embedding)     (None, 100, 100)          2000000                                                                   
# bidirectional_1 (Bidirectional)  (None, 100, 128)         84480                                                             
# global_max_pooling1d_1 (GlobalMaxPooling1D)  (None, 128)    0         
# dropout_1 (Dropout)         (None, 128)               0         
# dense_1 (Dense)             (None, 6)                 774       

# -------
# パラメータの設定(2)
batch_size = 128
epochs = 6

# -------
# 学習する
callbacks_list = [keras.callbacks.EarlyStopping(monitor='accuracy',patience=5,), keras.callbacks.ModelCheckpoint(filepath='my_model.h5',monitor='val_loss',save_best_only=True,)]
mod-el.fit(train_x,y_train,epochs=epochs,batch_size=batch_size,validation_split=0.25,callbacks=callbacks_list,verbose=1)
#Epoch 1/6
#935/935 [==============================] - 171s 181ms/step - loss: 0.1331 - accuracy: 0.8724 - val_loss: 0.0749 - val_accuracy: 0.9940
#Epoch 2/6
#935/935 [==============================] - 178s 190ms/step - loss: 0.0659 - accuracy: 0.9813 - val_loss: 0.0591 - val_accuracy: 0.9940
#Epoch 3/6
#935/935 [==============================] - 180s 193ms/step - loss: 0.0541 - accuracy: 0.9919 - val_loss: 0.0566 - val_accuracy: 0.9941
#Epoch 4/6
#935/935 [==============================] - 178s 190ms/step - loss: 0.0481 - accuracy: 0.9913 - val_loss: 0.0546 - val_accuracy: 0.9941
#Epoch 5/6
#935/935 [==============================] - 178s 190ms/step - loss: 0.0444 - accuracy: 0.9886 - val_loss: 0.0561 - val_accuracy: 0.9941
#Epoch 6/6
#935/935 [==============================] - 191s 205ms/step - loss: 0.0418 - accuracy: 0.9887 - val_loss: 0.0562 - val_accuracy: 0.9941

```

D先生 ： “学習状況がコメントになっています。それらのコメントによるとずいぶん学習の予測精度が高いですね。”

```python
# -------
# チェックをする
#　TRAIN：同データセットを使う
# -------
# チェック値
check_x = train_x[0:29999, :]
comp_y  = y_train[0:29999, :]
# チェック値
y_check = model.predict(check_x,batch_size=batch_size,verbose=1)
print("y_check : ",y_check.shape)
# 比較値
#print("comp_y : ",comp_y.shape)
len_data = comp_y.shape[0]
#print(len_data)

# -------
# 配列の初期化
y_toxic_A   = []
y_toxic_B   = []
y_sevtx_A   = []
y_sevtx_B   = []

# -------
# 'toxic', j=0
for i in range(len_data):
    if comp_y[i,0] > 0.01:
        y_toxic_A.append(y_check[i,0])
    else:
        y_toxic_B.append(y_check[i,0])
#print(len(y_toxic_A))
#print("A : ",np.mean(y_toxic_A))
#print("B : ",np.mean(y_toxic_B))

# -------
# 'severe_toxic', j=1
for i in range(len_data):
    if comp_y[i,1] > 0.01:
        y_sevtx_A.append(y_check[i,1])
    else:
        y_sevtx_B.append(y_check[i,1])
print(len(y_sevtx_A))
print("A : ",np.mean(y_sevtx_A))
print("B : ",np.mean(y_sevtx_B))

# -------
# Creating dataset
bp_data = [y_toxic_A, y_toxic_B, y_sevtx_A, y_sevtx_B, ]
 
fig = plt.figure(figsize =(14, 8))
ax1 = fig.add_subplot(111)
 
# Creating axes instance
bp1 = ax1.boxplot(bp_data, patch_artist = True,
                notch ='True', vert = 0)

# changing color and linewidth of
# whiskers
for whisker in bp1['whiskers']:
    whisker.set(color ='#8B008B', linewidth = 1.5, linestyle =":")
 
# changing color and linewidth of
# caps
for cap in bp1['caps']:
    cap.set(color ='#8B008B', linewidth = 2)
 
# changing color and linewidth of
# medians
for median in bp1['medians']:
    median.set(color ='red', linewidth = 3)
 
# changing style of fliers
for flier in bp1['fliers']:
    flier.set(marker ='D', color ='#e7298a', alpha = 0.5)
     
# x-axis labels
ax1.set_yticklabels(['y_toxic_A', 'y_toxic_B', 'y_sevtx_A', 'y_sevtx_B'])
 
# Adding title
plt.title("box plot of prediction value(toxic, severe_toxic)")
 
# Removing top axes and right axes
# ticks
ax1.get_xaxis().tick_bottom()
ax1.get_yaxis().tick_left()
     
# show plot
plt.show()

```

QEU:FOUNDER ： “まずは、学習データ(train)の一部を予測し、実績と比較しました。ボックスプロットでやってみましょう。”

![image2-72-2](https://yaber1965.github.io/images/image2-72-2.jpg)

D先生 ： “グラフでもある意味当たり前の結果が出ていますね。項目toxicの場合には、それぞれ0と1付近にきれいに分離しています。もう一つの項目はもうちょっと分かりにくい・・・。これはラベル値が１となるデータ数が少ないためでしょうね。”

```python
# -------
# ヒストグラムで作画する
import numpy as np
import matplotlib.pyplot as plt

fig3 = plt.figure(figsize =(14, 8))
ax3  = fig3.add_subplot(111)
ax3.hist([y_toxic_A, y_toxic_B], label=['y_toxic_A', 'y_toxic_B'])
plt.legend(loc='upper right')
plt.show()

```

QEU:FOUNDER ： “念のため、ヒストグラムで分布を描いてみましょう。”

![image2-72-3](https://yaber1965.github.io/images/image2-72-3.jpg)

D先生 ： “なるほど・・・。それでは、独立した比較用のデータ（test）でやってみてみましょう。”

```python
# -------
# テストをする
#　TEST：別データセットを使う
# -------
# 予測値
y_pred=model.predict(test_x,batch_size=batch_size,verbose=1)
#print("y_pred : ",y_pred.shape)
# 実際値
#print("y_test : ",y_test.shape)
len_data = y_test.shape[0]
#print(len_data)

```

QEU:FOUNDER ： “同じようにやってみましょう。これはボックスプロット・・・。”

![image2-72-4](https://yaber1965.github.io/images/image2-72-4.jpg)

QEU:FOUNDER ： “これはヒストグラムね・・・。”

![image2-72-5](https://yaber1965.github.io/images/image2-72-5.jpg)

D先生 ： “あ～あ・・・、結果はがっかりです。どうせkaggleのことだから、testには難しいモノを入れているんでしょうけど・・・。”

QEU:FOUNDER ： “まあ、これは「ベースライン」にはなるでしょ？じゃあ、これからQEUオリジナルのやり方でといてみましょうか・・・。今回はLSTMのスキームを使いました。”

![image2-72-6](https://yaber1965.github.io/images/image2-72-6.jpg)

D先生 ： “いままでのディープラーニングのやり方とはちがうんですか？

QEU:FOUNDER ： “自然言語処理用らしいです。この２つの文をみてください。意味は同じですか？”

__A:　He is a student__
__B:　Is he a student__

D先生 ： “もちろん違います。Bは疑問文ですから・・・。順番を考慮するということは、こういうことね・・・。”

QEU:FOUNDER ： “はっきりいうけど、我々のやり方はこれほどパフォーマンスが良くなるとは思わないんですよ。その意味で、今回の結果は「永遠のベンチマーク」かな・・・？（笑）”

## ～　まとめ　～

QEU:FOUNDER ： “おまちかねの、「おっさんコーナー」です。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/GOCGCgMSvGY" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 ： “あれ？このひとは・・・。 “

QEU:FOUNDER ： “何も言うな。ついでにコレ・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/u5mcx5qq-4s" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

C部長 : “いいオッサンがいて、安心ですね。”



