## QEUR21_RL2048T22:　Pattern Feature Engineering(その3) ～ パターンRT法でインプットする

## ～　テクノメトリックスが必要レベルに達した！！　～

D先生 ： “じゃあ、いよいよパターンRT法でゲーム2048のプレイを学習しましょう。これ（↓）が前回のブログに従ってパターンファイルを4種パターン分生成した結果です。”

![imageRL1-22-1](https://yaber1965.github.io/images/imageRL1-22-1.jpg)

QEU:FOUNDER ： “上記はターンが100番目の盤面情報をヒートマップで可視化しました。若干、パターンの区別があいまいな部分が見られます。でも、それなりにイケるんじゃない（笑）？これを使ってプログラムを・・・。”


```python
# ----------------
# 2048ゲームの強化学習システム
# step5 : パターンRTを動的メトリックスとして活用する
# step5 : DQN_patternRT_game2048_agent.py
# step5 : DQNとスライドRT法でデータを生成します
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import copy, random, time
import pandas as pd
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
#%matplotlib inline
# ---------------- 
# environment
import RT_auto_game_2048_2

# =================================================
# difinition of function(1) - ONEHOT関連
# =================================================
# [array]-action をonehot変換します。
def create_arronehot(action):
    # ----------------
    # action number / onehot
    # 0 / 1000
    # 1 / 0100
    # 2 / 0010
    # 3 / 0001
    # ----------------
    # ONEHOTに変換
    if action == 0:
        arr_onehot = [1.0, 0.0, 0.0, 0.0]
    elif action == 1:
        arr_onehot = [0.0, 1.0, 0.0, 0.0]
    elif action == 2:
        arr_onehot = [0.0, 0.0, 1.0, 0.0]
    elif action == 3:
        arr_onehot = [0.0, 0.0, 0.0, 1.0]

    return np.array(arr_onehot)

# ゲームパターンのCSVファイルを読み込み表示する
def read_gamepattern(file_readcsv): 
 
    # 画像異常検出ファイルの読み込み
    df = pd.read_csv(file_readcsv) 
    max_turn = len(df)
    #print("データ量 max_dfplay",max_dfplay)
    #print(df)
    # --------------------------------------------------
    # 選択項目の読み込み
    #　基本情報-["iplay","batch","name","score","usefile","maxturn","actmatch","perform","epsilon"]
    mx_pattern   = df.loc[:,"p0":"p15"].values
    #print("----- mx_pattern -----")
    #print(mx_pattern)
    
    return mx_pattern

# =================================================
# difinition of function(2)
# =================================================
# イレギュラー検出関数(タイル移動後の状態とスコア)
def is_invalid_action(state, a_order):
    # ----------------
    spare = copy.deepcopy(state)  # 状態
    #print("spare",type(spare))
    reward = 0  # 当該行動による報酬
    if a_order == 0:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y, x - z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, x - z_cnt] *= 2
                    reward += spare[y, x - z_cnt]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 1:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y - z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y - z_cnt, x] *= 2
                    reward += spare[y - z_cnt, x]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 2:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, 3 - x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, 3 - x] != prev:
                    tmp = spare[y, 3 - x]
                    spare[y, 3 - x] = 0
                    spare[y, 3 - x + z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, 3 - x + z_cnt] *= 2
                    reward += spare[y, 3 - x + z_cnt]
                    spare[y, 3 - x] = 0
                    prev = -1
    elif a_order == 3:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[3 - y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[3 - y, x] != prev:
                    tmp = state[3 - y, x]
                    spare[3 - y, x] = 0
                    spare[3 - y + z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[3 - y + z_cnt, x] *= 2
                    reward += spare[3 - y + z_cnt, x]
                    spare[3 - y, x] = 0
                    prev = -1

    if (state - spare).any() == False:
        return True, reward, spare
    else:
        return False, reward, spare


# ---------------------------
# newRTメトリックスを計算する(テンソル活用版)
def calc_newRT(tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    L1_loss = torch.nn.L1Loss()
    btY1_yarray, Y2_yarray = [], []

    # 繰り返し
    y = tsr_sig_matrix
    x = tsr_tani_array

    xx = torch.dot(x,x)
    xy = torch.dot(x,y)
    beta = xy/xx
    mDistance   = L1_loss(y, beta*x)

    return beta.item(), mDistance.item()

# ----------------
# 移動可能な方向をまとめ、RTメトリックスを生成する
def calc_mxlog2(arr_state):    

    tsr_state = torch.tensor(arr_state).float()
    #print(tsr_state)
    # ---------------------------
    # tsr_spareを対数化する
    for jCol in range(16):
        raw_value = tsr_state[jCol]
        # print("make_metrics - raw_value",raw_value)
        if raw_value > 0:
            tsr_state[jCol] = math.log2(raw_value)
        else:
            tsr_state[jCol] = 0

    return tsr_state

# ----------------
# 移動可能な方向をまとめ、RTメトリックスを生成する
def calc_RTmovables(state, tsr_tani):    

    # --------------------------------------------------
    # 配列の初期化
    spare = np.array(state)
    # --------------------------------------------------
    # オリジナル操作方法 -> 数字に変更します(ENVIRONMENT)
    # 数字を上へ -> 1
    # 数字を下へ -> 3
    # 数字を左へ -> 0
    # 数字を右へ -> 2
    # --------------------------------------------------
    a_map, movables, arr_rewards = [0, 1, 2, 3], [], []
    iCnt, arr_states = 0, []
    for i_map in a_map:
        flg_invalid, val_reward, spare_next = is_invalid_action(spare, i_map)
        if flg_invalid == False:
            arr_states.append(spare_next.flatten())
            arr_rewards.append(val_reward + 4)
            movables.append(i_map)
            iCnt = iCnt + 1
    num_avail = iCnt
    
    # --------------------------------------------------
    # newRT法の生成
    # --------------------------------------------------
    # 単位空間
    # 移動可能な方向をまとめ、RTメトリックスを生成する
    #tsr_tani    = calc_mxlog2(spare.flatten())
    #print("tsr_tani",tsr_tani)

    # 信号空間
    tsr_signal  = torch.zeros(16).float()
    tsr_signal  = calc_mxlog2(spare_next.flatten())

    # --------------------------------------------------
    # FEATURE-ENGINEERING(新RTメトリックスを計算する)
    btY1_yarray, Y2_yarray = np.zeros(4), np.zeros(4)
    for iCnt_tani in range(4):
        btY1_yarray[iCnt_tani], Y2_yarray[iCnt_tani] = calc_newRT(tsr_signal, tsr_tani[iCnt_tani])
    
    # ------------------
    # 信号空間用の畳み込みテンソルを生成する 
    tsr_features  = torch.zeros(state_size).float()     # state_size
    for iCnt in [0,1,2,3]:
        iZero = int(2 * iCnt)
        iPone = int(2 * iCnt + 1)
        tsr_features[iZero] = btY1_yarray[iCnt]
        tsr_features[iPone] = Y2_yarray[iCnt]
    # 結果の出力
    #print("----- tsr_features -----")
    #print(tsr_features)

    return num_avail, movables, arr_rewards, tsr_features

#=================================================
# agent_main class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 64)
        self.fc2 = torch.nn.Linear(64, 64)
        self.fc3 = torch.nn.Linear(64, dim_output)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Solving the maze in Deep Q-learning
class DQN_Solver:
    def __init__(self, state_size, action_size):
        self.state_size  = state_size
        self.action_size = action_size
        self.memory  = deque(maxlen=MEMORY_CAPACITY)
        self.gamma   = GAMMA
        self.epsilon = EPSILON
        self.e_decay = EPDECAY
        self.e_min   = EPMIN
        self.learning_rate = LRATE

        # --------------------------------------------------
        # crate instance for input
        self.model = Net()
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.model.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.model.eval()


    def remember_memory(self, state, action, reward, next_state, next_movables, done):
        self.memory.append((state, action, reward, next_state, next_movables, done))


    def choose_action(self, x_input_array, num_avail, movables, arr_rewards, iCnt_turn, iCnt_play):        # 定义动作选择函数 (x为状态)

        # --------------------------------------------------
        # オリジナル操作方法 -> 数字に変更します(ENVIRONMENT)
        # 数字を上へ -> 1
        # 数字を下へ -> 3
        # 数字を左へ -> 0
        # 数字を右へ -> 2
        # --------------------------------------------------
        # 命令の初期選択
        a_order, acType, rl_Qvalue, rl_done = -1, "NA", -1, False
        mx_input    = []  # 状態(state)と行動(action)マトリックス
        val_boltz   = 10
        if (num_avail == 0):
            rl_done = True
            acType  = "gameover"
            print("ゲームオーバー -- iCnt_play: {0}, iCnt_turn: {1}".format(iCnt_play, iCnt_turn))
        else:
            if self.epsilon <= random.random():
                # DQNによる選択
                acType  = "machine"
                iCnt = 0
                for a in movables:
                    # --- model ---
                    # [array]-action をonehot変換します。
                    arr_onehot = create_arronehot(a)
                    temp_s_a = np.append(x_input_array, arr_onehot)
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.append(mx_input, [temp_s_a], axis=0)  # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                mx_input = np.array(mx_input)
                # print("----- mx_input -----")
                # print(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.model(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                rl_Qvalue   = np.max(y_pred)
                temp_action = np.argmax(y_pred)
                a_order     = movables[temp_action]
            else:                                                                   # 随机选择动作
                # 乱数による選択
                acType  = "random"
                qs_prob = softmax(np.array(arr_rewards) / val_boltz)
                a_order = np.random.choice(movables, p=qs_prob)
                
        # --------------------------------------------------
        if iCnt_turn%10 == 0:
            print("行動タイプ:{0}, プレイ:{1}, ターン:{2}, 行動選択結果:{3}".format(acType, iCnt_play, iCnt_turn, a_order))

        return a_order, acType, rl_Qvalue, rl_done


    def replay_experience(self, batch_size):

        # set training parameters
        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate)
        criterion = torch.nn.MSELoss()
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = np.array([]), np.array([])
        for ibat in range(batch_size):
            state, action, reward, next_state, next_movables, done = minibatch[ibat]
            # [array]-action をonehot変換します。
            arr_onehot = create_arronehot(action)
            state_action_curr = np.append(state, arr_onehot)
            #print("state_action_curr",state_action_curr)
            if done:
                target_f = reward
            else:
                mx_input = []  # 状態(state)と行動(action)マトリックス
                iCnt = 0
                for imov in next_movables:
                    # [array]-action をonehot変換します。
                    arr_onehot = create_arronehot(imov)
                    temp_s_a = np.append(next_state, arr_onehot)
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.append(mx_input, [temp_s_a], axis=0)  # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                    # ----
                mx_input = np.array(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.model(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # -----
            if ibat == 0:
                X = [state_action_curr]  # 状態(S)行動(A)マトリックス
            else:
                X = np.append(X, [state_action_curr], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_next = torch.from_numpy(X).float()
        expected_q_value = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_next:",state_action_next)
        #print("expected_q_value:",expected_q_value)
        
        # --- building model ---
        q_value = self.model(state_action_next)

        # calculate loss
        loss = criterion(q_value, expected_q_value)

        # update weights
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Show progress
        print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return self.epsilon, loss.data.numpy()

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # 記録用パラメタ類(プレイベース)の初期化
        self.arr_iCnt_play  = []  # count game play    プレイ番号
        self.arr_maxturn    = []  # max_turn   ゲームのターン数
        self.arr_csv_play   = []  # name game play    プレイファイル名のリスト
        self.arr_maxscore   = []  # rl_score game play    最終プレイスコア
        # ----------
        self.arr_epsilon    = []  # イプシロン
        self.arr_upright    = []  # 角のスコア（右上）
        self.arr_upleft     = []  # 角のスコア（左上）
        self.arr_downright  = []  # 角のスコア（右下）
        self.arr_downleft   = []  # 角のスコア（左下）
        # ----------
        # 統計データの初期化(プレイベース)
        self.arr_loss       = []  # 学習損失
        # ----------
        # AV値の分析用(プレイベース)
        self.arr_num_AV     = []  # AV値のN数
        self.arr_max_AV     = []  # AV値の最大値
        self.arr_q25_AV     = []  # AV値の4分の1値
        self.arr_q75_AV     = []  # AV値の4分の3値

        # --------------------------------------------------
        # CSV-playlist用のリスト
        self.arr_usefile    = np.zeros(num_episodes)        # usefile    CSVファイルを出力するか use 1, un-use 0
        self.arr_perform    = np.zeros(num_episodes)        # performance   命令の一致率
        
        # --------------------------------------------------
        gamepanel       = RT_auto_game_2048_2.Board()
        self.game2048   = RT_auto_game_2048_2.Game(gamepanel)
        self.game2048.gamepanel.gridCell = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])

        # --------------------------------------------------
        # ゲームパターンの単位空間テンソル
        self.tsr_pattern = torch.zeros([num_maxturns,max_ptn_parts,16])
        # ゲームパターン用部品(4種類)を読み込む
        for iPtn in range(max_ptn_parts):    # max_ptn_parts
            # ゲームパターンのCSVファイルを読み込み表示する
            file_ptn_input  = foldername + code_ptn_input[iPtn] + ".csv"  # ファイルパス名の生成 
            mx_pattern      = read_gamepattern(file_ptn_input)
            for iTurn in range(num_maxturns):
                self.tsr_pattern[iTurn, iPtn] = torch.tensor(mx_pattern[iTurn])
        #print(self.tsr_pattern)

        # --------------------------------------------------
        # ゲーム2048をプレイする
        for iCnt_play in range(num_episodes):       # num_episodes
        
            num_maxturn, num_maxscore, arr_corner = self.get_episode(iCnt_play)

            # ----------
            # DQN-Experience Replay学習
            val_epsilon, loss = dql_solver.replay_experience(BATCH_SIZE)

            # ----------
            # ゲームデータの出力
            code_csvout = "NA"
            if iCnt_play%40 == 0:
                # CSV ファイル (file_csvout) として出力する
                code_csvout = "game2048_play{0}_score{1}.csv".format(iCnt_play, num_maxscore)       # file name  
                #print("メトリックス保管用CSVファイル名 ：{0}".format(code_csvout))
                #code_csvout = self.save_csvGAME(iCnt_play, code_csvout)        # CSVファイルに保存する

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iCnt_play.append(iCnt_play)        # count game play    プレイ番号
            self.arr_maxturn.append(num_maxturn)        # max_turn   ゲームのターン数
            self.arr_csv_play.append(code_csvout)       # name game play    プレイファイル名のリスト
            self.arr_maxscore.append(num_maxscore)      # rl_score game play    最終プレイスコア
            self.arr_loss.append(loss)                  # DQN-Experience Replay学習
            # ----------
            self.arr_epsilon.append(val_epsilon)        # イプシロン
            self.arr_upright.append(arr_corner[0])      # 角のスコア（右上）
            self.arr_upleft.append(arr_corner[1])       # 角のスコア（左上）
            self.arr_downright.append(arr_corner[2])    # 角のスコア（右下）
            self.arr_downleft.append(arr_corner[3])     # 角のスコア（左下）         

            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%20 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する(1)
        x = list(range(num_episodes))
        
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(x, self.arr_epsilon)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Corner score')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('number')
        ax2.scatter(x, self.arr_upright, label="upright", color="red")
        ax2.scatter(x, self.arr_upleft, label="upleft", color="blue")
        ax2.scatter(x, self.arr_downright, label="downright", color="orange")
        ax2.scatter(x, self.arr_downleft, label="downleft", color="green")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph(num_episodes)

        # --------------------------------------------------
        # ゲームプレイリストをCSVファイルに保存する
        #print("ゲームプレイリストをCSVファイルに保存する")
        #self.save_playlist()

        # --------------------------------------------------
        # PyTorchモデルの保存
        #torch.save(ActorCritic.state_dict(), file_output_model)


    # --------------------------------------------------
    def get_episode(self, iCnt_play):

        # --------------------------------------------------
        acType, val_reward, rl_done, a_order  = "random", 0, False, 0
        state    = np.array([[0]*4 for i in range(4)])
        a_order_prev, ttlscore_prev = 0, 0
        iCnt_turn , num_maxturn, num_maxscore = 0, 0, 0
        
        # --------------------------------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_icount = []        # カウンタリスト
        self.arr_order = []         # 指示リスト
        self.arr_avail = []         # 使用可能な命令数リスト
        self.arr_ttlscore = []      # ゲームスコアリスト(真数)  
        self.arr_reward = []        # 報酬リスト(真数)  
        self.arr_yzero = []         # ゼロ・スコアの数
        self.arr_acType = []        # 指示タイプ(random,machine)
        self.arr_predAV = []        # DL予測されたAV値

        # --------------------------------------------------
        # ゲーム・エピソードを行う
        state = self.reset()  # 環境をﾘｾｯﾄする
        num_avail, movables, arr_rewards, s = calc_RTmovables(state, self.tsr_pattern[0])
        #print("    s       ", s)
        #print("    state       ", state)
        iCnt_turn, ttlscore, ttlscore_prev= 0, 0, 0
        while True: 
        
            # --------------------------------------------------
            # 命令を選択する
            a_order, acType, rl_Qvalue, rl_done = dql_solver.choose_action(s, num_avail, movables, arr_rewards, iCnt_turn, iCnt_play) 
            # ------
            arr_corner = np.zeros(4)
            arr_corner[0] = state[0][3]   # arr_upright , 角のスコア（右上）
            arr_corner[1] = state[0][0]   # self.arr_upleft , 角のスコア（左上）
            arr_corner[2] = state[3][3]   # self.arr_downright , 角のスコア（右下）
            arr_corner[3] = state[3][0]   # self.arr_downleft , 角のスコア（左下）

            # --------------------------------------------------
            # 環境(ENVIRONMENT)を実行する
            state_next, ttlscore, done = self.game2048.link_keys(a_order)
            if iCnt_turn >= num_maxturns:
                iCnt_pattern = num_maxturns - 1
            else:
                iCnt_pattern = iCnt_turn
            num_avail, next_movables, arr_rewards, s_temp = calc_RTmovables(state_next, self.tsr_pattern[iCnt_pattern])
            #print("状態テンソル - s_temp:", s_temp)

            # 報酬を計算する
            val_reward = ttlscore - ttlscore_prev

            # --------------------------------------------------
            # 学習データの積み上げ
            # 変数の変換(machine, random)
            s       = s
            a       = a_order
            r       = val_reward 
            s_      = s_temp

            # Experience Replay配列に保管する
            if num_avail > 0:
                dql_solver.remember_memory(s, a, r, s_, next_movables, done)

            # --------------------------------------------------  
            self.arr_icount.append(iCnt_turn)       # カウンタリスト
            self.arr_order.append(a_order)        # 指示リスト
            self.arr_avail.append(num_avail)     # 使用可能な命令数リスト
            self.arr_ttlscore.append(ttlscore)      # ゲームスコアリスト(真数)  
            self.arr_reward.append(val_reward)       # 報酬リスト(真数)  
            self.arr_acType.append(acType)       # 指示タイプ(random,machine)     
            self.arr_predAV.append(rl_Qvalue)      # DL予測されたAV値
            # ----------   
            board_flatten = np.array(state).flatten()
            cc = Counter(board_flatten)
            self.arr_yzero.append(cc[0]) 

            # --------------------------------------------------
            # 次回への引き渡し
            s               = s_temp
            state           = state_next
            movables        = next_movables
            ttlscore_prev   = ttlscore
            iCnt_turn       = iCnt_turn + 1

            if done:       # 如果done为True
                print("# of episode :{}, turn : {}, ttlscore : {:.1f}".format(iCnt_play, iCnt_turn, ttlscore))
                break      

        # --------------------------------------------------  
        self.arr_num_AV.append(iCnt_turn)                               # AV値のN数
        self.arr_max_AV.append(np.max(self.arr_predAV))                 # AV値の最大値
        self.arr_q25_AV.append(np.percentile(self.arr_predAV, q=25))    # AV値の4分の1値
        self.arr_q75_AV.append(np.percentile(self.arr_predAV, q=75))    # AV値の4分の3値

        # -------------------------------------------------- 
        # プレイリストに引き渡す変数
        num_maxturn     = iCnt_turn
        num_maxscore    = ttlscore

        return num_maxturn, num_maxscore, arr_corner


    # ----------------
    # ゲームをﾘｾｯﾄする
    def reset(self):   
    
        state_init = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])
    
        # --------------------------------------------------
        # パラメタの初期化(Panel)
        self.game2048.gamepanel.gridCell=state_init
        self.game2048.gamepanel.compress=False
        self.game2048.gamepanel.merge=False
        self.game2048.gamepanel.moved=False
        self.game2048.gamepanel.score=0
        
        # --------------------------------------------------
        # パラメタの初期化(Game)
        self.game2048.end=False
        self.game2048.won=False

        # --------------------------------------------------
        # 記録用リストの初期化(ターンベース)
        self.arr_icount = []        # カウンタリスト
        self.arr_order = []         # 指示リスト
        self.arr_avail = []         # 使用可能な命令数リスト
        self.arr_ttlscore = []      # ゲームスコアリスト(真数)  
        self.arr_reward = []        # 報酬リスト(真数)  
        self.arr_yzero = []         # ゼロ・スコアの数
        self.arr_acType = []        # 指示タイプ(random,machine)
        self.arr_predAV = []        # DL予測されたAV値

        return state_init
        
        
    # ----------------
    # 学習結果のグラフ化
    def show_graph(self, num_episodes):

        x = list(range(num_episodes))
        # print(x)

        fig = plt.figure(figsize=(12, 10))
        # -----
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : loss')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('Learn-loss')
        ax1.grid(True)
        ax1.plot(x, self.arr_loss)
        # -----
        ax2 = fig.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : QValues')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('QValue')
        #ax2.set_ylim(bottom=0)
        ax2.grid(True)
        ax2.plot(x, self.arr_q25_AV, label="q25_AV", color="blue")
        ax2.plot(x, self.arr_q75_AV, label="q75_AV", color="red")
        ax2.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(x, self.arr_maxturn, label="original", color="blue")
        ax3.plot(x, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(x, self.arr_maxscore, label="original", color="blue")
        ax4.plot(x, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # DQN Parameter
    state_size  = 8
    action_size = 4
    # -----
    dim_input   = state_size + action_size
    dim_output  = 1

    # ---------------------------
    # ハイパーパラメタ
    BATCH_SIZE = 128                         # サンプルサイズ
    LRATE = 0.005                            # 学習率
    EPSILON = 0.99                           # greedy policy
    EPDECAY = 0.999
    EPMIN = 0.01
    GAMMA = 0.95                             # reward discount
    MEMORY_CAPACITY = 2000                   # メモリ容量
    # ---------------------------
    SLEEP_TIME      = 0.01
    print_interval  = 50                     # 出力頻度
    num_episodes    = 5000                   # 繰り返し回数

    # ---------------------------
    # パラメタの設定
    max_ptn_parts   = 4                     # 畳み込み部品数
    num_maxturns    = 100                   # 学習の繰り返し数

    # ---------------------------
    # フォルダ名の指定
    foldername = "./ARRAY_RTCNN/"  # My project folder

    # ---------------------------
    # solver
    dql_solver  = DQN_Solver(state_size, action_size)

    # ---------------------------
    # 入力用：パターン部品のCSVファイルを定義する
    nam_ptn_input = "畳み込み部品用CSVデータ" 
    code_ptn_input = ["NA"] * 4 # CSVコードの指定
    code_ptn_input[0] = "train_pattern_order0-1" # CSVコードの指定
    code_ptn_input[1] = "train_pattern_order0-3" # CSVコードの指定
    code_ptn_input[2] = "train_pattern_order2-1" # CSVコードの指定
    code_ptn_input[3] = "train_pattern_order2-3" # CSVコードの指定
    print("---- 入力用パターンCSVファイル名 ----")
    print(code_ptn_input)
    print("----------------------")

    # ---------------------------
    # 出力用:ゲームプレイリスト用のCSVファイルを定義する
    nam_csv_playlist    = "ゲームプレイリストのCSVデータ" 
    code_csv_playlist   = "train_playlist_DQN.csv" # CSVコードの指定
    file_csv_playlist   = foldername + code_csv_playlist  # ファイルパス名の生成
    print("------------ 計測データ出力用のCSV名 -------------")   
    print("ゲームプレイリストのCSVファイル名 ：{0}".format(file_csv_playlist))

    # ---------------------------
    # 出力用:Pytorchモデルのファイル名
    comment_output_model = "game2048_DQN"
    code_output_model = "model_game2048_DQN.pt"
    file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # DQN-ERで学習する
    Agent()

```

QEU:FOUNDER ： “・・・プログラムをドン（笑）！まずは、この学習結果(↓)をみてみましょう。前回（スライドRT）、前々回（畳み込みRT）のプロジェクトと同様に5000回の学習です・・・。”

**(イプシロン値とゲーム盤角部の数値の推移)**

![imageRL1-22-2](https://yaber1965.github.io/images/imageRL1-22-2.jpg)

**(その他のパフォーマンスの推移)**

![imageRL1-22-3](https://yaber1965.github.io/images/imageRL1-22-3.jpg)

D先生 ： “あれ？いままでのテクノメトリックスとは全然違う・・・。参考までに、前々回の「畳み込みRT法」の学習結果を見せてください。「スライドRT法」の結果とは、そんなにパフォーマンスが違わないから・・・！！”

**(イプシロン値とゲーム盤角部の数値の推移)**

![imageRL1-22-4](https://yaber1965.github.io/images/imageRL1-22-4.jpg)

**(その他のパフォーマンスの推移)**

![imageRL1-22-5](https://yaber1965.github.io/images/imageRL1-22-5.jpg)

QEU:FOUNDER ： “う～ん・・・、比較すると、か～なり微妙（笑）！・・・というか、スコアの上げトレンドはパターンRT法を使った場合の方がより明確です。パターンRTの勝ち！”

D先生 ： “Q値に注目すれば、差異が明確にわかりますね・・・。**畳み込みRT学習ではQ値はゆっくり上昇しますが、パターンRT学習ではQ値は急上昇します**。それにしても、パターンRT学習のQ値の変動は大きいですね。”

QEU:FOUNDER ： “我々が準備した、4種類のパターンファイルのうち1つの中身を確認してみましょう。グラフの横軸がターン数で、縦軸が対数値です。パネル（p0,p1…）ごとにプロットしています。バラツキがひどいので、移動平均した散布図の方が見やすいと思います。”

![imageRL1-22-6](https://yaber1965.github.io/images/imageRL1-22-6.jpg)

D先生 ： “なんか、変な動きをしますねぇ・・・。このパターンファイルに問題があるんじゃないですか？”

![imageRL1-22-7](https://yaber1965.github.io/images/imageRL1-22-7.jpg)

QEU:FOUNDER ： “ゲームオーバーとなるターン数が200以下のものがたくさんはいっているんです。”

D先生 ： “だから、ヒートマップで可視化したパターンが不明確になっているんですね。これ・・・、改善したほうがいいんじゃない？”

QEU:FOUNDER ： “つぎにすべてのゲームが200ターン以上の好成績データだけで作成したパターンでやってみましょうか・・・。”

D先生 ： “FOUNDER・・・。コレ・・・、わざとやったんでしょう？”

QEU:FOUNDER ： “バレたか・・・（笑）。この程度のいいかげんなパターンでも明確な差が出てくるから、パターンRTはすごいんだと思いますよ。次回のパターンの差異に伴うパフォーマンスの変化で本質が見やすくなります。あと2回、完結編が追加になりました。つきましてはカンパください！！”

### [＞寄付のお願い(Donate me)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

D先生 ：“よろしくお願いします。”

## ～　まとめ　～

C部長 : “さあて、QEU名物のイケメン・バトルだ・・・！！”

![imageRL1-22-8](https://yaber1965.github.io/images/imageRL1-22-8.jpg)

QEU:FOUNDER ： “私のイケメン（右）の話だが、実はそうでもない。お笑いの話です。それでは動画をドン、お笑いがいきなり始まります・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/rjOd3dV_zvU?start=5981" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 : “ちょっとまった・・・。これは私が担当しているイケメン（中）の関連です。・・・それにしても、この芸はおもしろいなァ、**「Standup Comedy」**か・・・。いままでのJ国にはなかったな・・・。しいて言えば、談志の漫談か・・・。”

C部長 : “海外では流行しているんですか？”

<iframe width="560" height="315" src="https://www.youtube.com/embed/_MhxEoP-35k?start=61" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 : “**流行というか、生活の一部**です。すでに引退したが、A国のレイトショー、David Lettermanあたりは、オープニングで必ずコレをやります。格好いいんだ、コレが・・・。”

QEU:FOUNDER ： “日常生活の一部・・・。欧米のバーには必ずこんな感じの芸人がいるんです。だから、AGTやBGTであれだけの人材を輩出するわけで・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/XjzxtCYBSXg" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ： “AGTやBGTの番組構成を見ればわかるが、Standup Comedyの取り扱いは、歌やダンスとは別格です。準決勝までは必ず1人は入賞されるように配慮されています。”

D先生 : “でも、文化が違うんで、全然笑えないんだよねぇ（笑）。”

QEU:FOUNDER : “海外のお客さんに落語で笑えといってもね・・・（笑）。”

