## QEUR21_TYNLP1:　自然言語解析 (その2)

## ～　埋め込み(Embedding)は便利だねぇ・・・　～

D先生 ： “FOUNDER・・・、最初になにやるの？fast.ai?”

QEU:FOUNDER ： “白猫(fast.ai)、黒猫(Keras)なんでもいいです。で、今回はKerasで・・・。”

![image2-71-1](https://yaber1965.github.io/images/image2-71-1.jpg)

QEU:FOUNDER ： “今回の説明は、埋め込み(embedding)層の構築までです。ちょっと、学習データの構造に触れておきましょうか・・・。”

![image2-71-2](https://yaber1965.github.io/images/image2-71-2.jpg)

QEU:FOUNDER ： “プログラムにちょっと入るが、今回はデータをシンプル化するためにstopwordsを設けています。この言葉は、今回の判別に従来な作用をしていないと考えて消しているんです。そうすると、計算処理が軽くなってきます。”


```python
# ストップワードの設定
list_stopwords = ["i","im","my","your","they","their","a","an","the","is","be","it","its","and","or","any","do","that","this","of","to",]

```
 (image2-71-3)

QEU:FOUNDER ： “stopwordsって、本当はもっとたくさんあるんだろうけどね。何はともあれ、あとはプログラムをドン・・・。”


```python
# -------
#
# 自然言語処理(NLP)をディープラーニングで解く
# その1(準備段階、エンベッドまで)
#
# -------
import re, numpy as np, pandas as pd
from tensorflow import keras
from tensorflow.keras.optimizers import Adam # - Works
# -------
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers
from sklearn.model_selection import train_test_split
# -------
#import numpy as np, pandas as pd
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')
test_label = pd.read_csv('data/test_labels.csv')
train.head()

# -------
train.isnull().sum()
#id               0
#comment_text     0
#toxic            0
#severe_toxic     0
#obscene          0
#threat           0
#insult           0
#identity_hate    0
#dtype: int64

# -------
test.isnull().sum()
#id              0
#comment_text    0
#dtype: int64

# -------
labels  = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y_train = train[labels].values
y_test  = test_label[labels].values
comments_train = train['comment_text']
comments_test = test['comment_text']
comments_train = list(comments_train)

# -------
import matplotlib.pyplot as plt

# ---------------------------
# Code to draw bar graph for visualising distribution of classes within each label.
fig = plt.figure(figsize=(16, 10))
ax = fig.add_subplot(1, 1, 1)
barWidth = 0.4

bars1 = [sum(train['toxic'] == 1), sum(train['obscene'] == 1), sum(train['insult'] == 1), sum(train['severe_toxic'] == 1),
         sum(train['identity_hate'] == 1), sum(train['threat'] == 1)]
bars2 = [sum(train['toxic'] == 0), sum(train['obscene'] == 0), sum(train['insult'] == 0), sum(train['severe_toxic'] == 0),
         sum(train['identity_hate'] == 0), sum(train['threat'] == 0)]

r1 = np.arange(len(bars1))
r2 = [x + barWidth for x in r1]

ax.bar(r1, bars1, color='steelblue', width=barWidth, label='labeled = 1')
ax.bar(r2, bars2, color='lightsteelblue', width=barWidth, label='labeled = 0')

ax.set_title("distribution of classes within each label")
ax.set_xlabel('group', fontweight='bold')
plt.xticks([r + barWidth for r in range(len(bars1))], ['Toxic', 'Obscene', 'Insult', 'Severe Toxic', 'Identity Hate',
                                                       'Threat'])
plt.legend()
plt.show()


# ストップワードの設定
list_stopwords = ["i","im","my","your","they","their","a","an","the","is","be","it","its","and","or","any","do","that","this","of","to",]

# コメントの不要部分を掃除する関数
def clean_text(text):
    cnt_words = 0 
    output = ""
    text = str(text).replace("\n", "")
    text = re.sub(r'[^\w\s]','',text).lower()
    text = text.split(" ")
    for word in text:
        if word not in list_stopwords:
            output = output + " " + word
            cnt_words = cnt_words + 1
    return cnt_words, output.strip()

# コメントリストを生成する(100まで)
x_train = [] 
num_train = []
for line in comments_train: 
    cnt_words, output_x = clean_text(line)
    num_train.append(cnt_words)
    x_train.append(output_x)
#x_test = [] 
#for line in comments_test: 
#    x_test.append(clean_text(line))
print("x_train:",len(x_train))
print('Sample data:', x_train[3], y_train[3])


# ---------------------------
import seaborn as sns

# histgram of comment lengths
fig2 = plt.figure(figsize=(16, 10))
ax = fig2.add_subplot(1, 1, 1)

sns.set(color_codes=True)
sns.distplot(num_train, kde=False, bins=20, color="steelblue")

#plt.legend()
plt.show()


# -------
# パラメータの設定(1)
embed_size=100
max_features=20000
max_len=100

# -------
# 言語データのトークン化
tokenizer= Tokenizer(num_words=max_features,lower= True)
tokenizer.fit_on_texts(list(x_train))
tokenized_train=tokenizer.texts_to_sequences(x_train)
train_x=pad_sequences(tokenized_train,maxlen=max_len)
# -------
num_sample = 2
print('---- NO:{} ----'.format(num_sample))
print('Sample cleaned: ', x_train[num_sample])
print('------')
print('Sample embedded: ', train_x[num_sample])

# ---------------------------
# 埋め込み計算結果をCSVファイルに保存する
train['simple_text'] = x_train
train['count'] = num_train
del train['comment_text']
#train['embeded'] = train_x
train.head()

# CSV ファイル (pattern.csv) として出力する
file_csvout = "simple_train.csv" # ファイルパス名の生成
train.to_csv(file_csvout)

```

D先生 ： “なるほど、こういう感じにエンベットされるんですね。”

![image2-71-4](https://yaber1965.github.io/images/image2-71-4.jpg)

QEU:FOUNDER ： “ちなみに、stopwordsを使っていないときはこうなっていました。”

![image2-71-5](https://yaber1965.github.io/images/image2-71-5.jpg)

D先生 ： “埋め込みベクトルの数を100次元にしているんですね。今回はコメント（文字列）が長かったので、ベクトルの要素のほとんどに数字が埋まっているんだけど、文字列が短いと・・・。”

![image2-71-6](https://yaber1965.github.io/images/image2-71-6.jpg)

D先生 ： “短い文字列ではベクトルは数個だけを使っているようですね。”

QEU:FOUNDER ： “あとは、CSVファイルで一旦出力して、その結果をみてみましょう。文字列が長いモノで並べました。”

![image2-71-7](https://yaber1965.github.io/images/image2-71-7.jpg)

D先生 ： “あ～ぁ・・・、やっちゃった（笑）。”

QEU:FOUNDER ： “まあ、これも重要なデータではありますね。”

## ～　まとめ　～

### ・・・　つづきですが、急ブレーキがかかります　・・・

QEU:FOUNDER ： “検査を省力するのが王道ですよね。”

C部長 ： “はやく、例のプロジェクトを始めてください！“

![image2-71-8](https://yaber1965.github.io/images/image2-71-8.jpg)

QEU:FOUNDER ： “あっ、しまった！！検査の話を早くはじめ過ぎた。話題を変えましょう。C部長のところにはたくさんの美人が集まって、うらやましい・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/LxbJJ74Nz7Y" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 ： “Cさんはいいなぁ・・・。“

