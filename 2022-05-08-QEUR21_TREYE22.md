## QEUR21_TREYE22:　本実験～両目法（その１）: メトリックスの画像化

## ～　両目（Double-eye）RTメトリックスとは・・・　～

D先生 ： “それでは、最初のステップとして両目法の**RTメトリックス（η=ηL-ηR）**を計算してみましょう。例によって、メトリックスの分布はヒートマップで表現するんですか？”

![image3-42-1](https://yaber1965.github.io/images/image3-42-1.jpg)

QEU:FOUNDER ： “今回は行列データ量が大きくなるので、いっそのこと**画像に変換**しましょう。それではプログラムをドン！！”

```python
# ------------------
# 畳み込みRT法の本実験
# testDE1_deyeRT_CSVout.py
# ------------------
# モジュールのインポート
import cv2
from fastai.vision.all import *
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ------------------
# 読み込み先の指定
foldername = "./ARRAY_RTCNN/"
newsize = (900, 300)
#pic_nameL = "camera0_0_0_88_005_00_0_OK.png"   # 正常(普通色)
#pic_nameC = "camera1_0_0_88_005_00_0_OK.png"   # PINNO3-1端子抜け(普通色)
#pic_nameR = "camera2_0_0_88_005_00_0_OK.png"   # PINNO4-1端子抜け(普通色)
pic_nameL = "camera0_0_0_88_m003_007_0_COLOR.png"   # 正常(色変更)
pic_nameC = "camera1_0_0_88_m003_007_0_COLOR.png"   # PINNO3-1端子抜け(色変更)
pic_nameR = "camera2_0_0_88_m003_007_0_COLOR.png"   # PINNO4-1端子抜け(色変更)

# ------------------
# 左(L)画像
filename = foldername + pic_nameL
imgL = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
img_cropL = imgL[212:856,47:1867]

# リサイズ
img_resizeL = cv2.resize(img_cropL , newsize)

# ------------------
# 中央(C)画像
filename = foldername + pic_nameC
imgC = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
img_cropC = imgC[212:856,47:1867]

# リサイズ
img_resizeC = cv2.resize(img_cropC , newsize)

# ------------------
# 右(R)画像
filename = foldername + pic_nameR
imgR = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
img_cropR = imgR[212:856,47:1867]

# リサイズ
img_resizeR = cv2.resize(img_cropR , newsize)
# 画像出力
#cv2.imshow("image(Right)", img_resizeR)
#cv2.waitKey(0)
#cv2.destroyAllWindows()

# ------------------
# 画像データのテンソル化(L,C,R)
img_tensorL = tensor(img_resizeL/255)  # 信号空間
img_tensorC = tensor(img_resizeC/255)  # 単位空間（標準ベクトル）
img_tensorR = tensor(img_resizeR/255)  # 信号空間
#print(img_tensorC)

# ---------------------------
# RTメトリックスを計算する(テンソル活用版)
def calc_deyeRTmet(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    lnr_yarray, btY1_yarray = [], []
    st_yarray, sb_yarray, se_yarray, ve_yarray = [], [], [], []
    yita_yarray, Y2_yarray = [], []

    # 有効除数の計算
    yuko = torch.dot(tsr_tani_array, tsr_tani_array)
    #print(yuko)

    # 線形式の計算
    for i in range(len_temp):
        lnr_yitem  = torch.dot(tsr_sig_matrix[i], tsr_tani_array)
        btY1_yitem = lnr_yitem / yuko
        lnr_yarray.append(lnr_yitem.item())
        btY1_yarray.append(btY1_yitem.item())
    #print(lnr_yarray)
    #print(btY1_yarray)

    # 全変動ST及び各種中間指標SB,SE,VE,η
    for i in range(len_temp):
        sum_item  = torch.dot(tsr_sig_matrix[i], tsr_sig_matrix[i])
        st_yarray.append(sum_item.item())
        sb_yarray.append(lnr_yarray[i] ** 2 / yuko.item())
        se_yarray.append(st_yarray[i] - sb_yarray[i])
    #print(st_yarray)
    #print(sb_yarray)
    #print(se_yarray)

        # 異常処理
        temp_ve = se_yarray[i] / float(max_jy_index - 1.0)
        if temp_ve < 0.0001:
            #comment = 'ゼロ異常発生！！'
            #print(comment)
            ve_yarray.append(0.0)   
            yita_yarray.append(0.0)  
            Y2_yarray.append(0.0) 
        else:
            temp_Y2 = math.sqrt(temp_ve)
            ve_yarray.append(temp_ve)   
            yita_yarray.append(1.0 / temp_ve)  
            Y2_yarray.append(temp_Y2) 
    #print(btY1_yarray)
    #print(Y2_yarray)

    return round(Y2_yarray[0] - Y2_yarray[1], 5)

# ---------------------------
# テンソルイメージ処理のパラメタ
max_tsr_row   = newsize[1]  #(900, 300)
max_tsr_col   = newsize[0]  #(900, 300)
# ストライド量
stride_tsr_row  = 5
stride_tsr_col  = 5
# RT法処理のサイズ
rtm_tsr_row   = 5
rtm_tsr_col   = 5
# RTベクトル長と信号空間長
max_jy_index  = 5*5
len_temp      = 2  # 右と左

# ---------------------------
# テンソル行列の処理(for)開始点のベクトルを生成
row_range = []
sum_row   = 0
while sum_row <= max_tsr_row-rtm_tsr_row:
    row_range.append(sum_row)
    sum_row  += stride_tsr_row
#print(row_range)    # len 60
# -----
col_range = []
sum_col   = 0
while sum_col <= max_tsr_col-rtm_tsr_col:
    col_range.append(sum_col)
    sum_col  += stride_tsr_col
#print(col_range)    # len 180

# ---------------------------
# 両目RT法によるマトリックスの生成
num_rows = len(row_range)
num_cols = len(col_range)
# ------
arr_row = []
arr_col = []
deyeY2_pool = []
# ------
cnt_row = 0
for row in row_range:
    cnt_col = 0
    for col in col_range:
    
        # ------
        # 単位空間の空間ベクトルを生成する
        tsr_tani_array = img_tensorC[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
        #print(tsr_tani_array)

        # ------
        # 信号空間の空間マトリックスを生成する
        tsr_sigL = img_tensorL[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
        tsr_sigR = img_tensorR[row:row+rtm_tsr_row,col:col+rtm_tsr_col].flatten()
        # -----
        tsr_sig_matrix = torch.stack([tsr_sigL, tsr_sigR])
        #print("----- tsr_sig_matrix -----")
        #print(tsr_sig_matrix)

        # ------
        # 両目法RTメトリックスを計算する
        deyeY2 = calc_deyeRTmet(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array)
        #print(deyeY2)

        # ------
        # 信号空間の空間マトリックスを生成する
        arr_row.append(row)
        arr_col.append(col)
        deyeY2_pool.append(deyeY2)
        # ------
        # COUNT UP
        #print(cnt_row, cnt_col)
        cnt_col = cnt_col + 1
    # ------
    # COUNT UP.append
    cnt_row = cnt_row + 1

# ---------------------------
# 結果を画像化する
temp_deyeY2 = []
for i in range(len(deyeY2_pool)):
    val_img = int(deyeY2_pool[i]*255 + 120)
    temp_deyeY2.append(val_img)
mx2_deyeY2 = np.array(temp_deyeY2).reshape([num_rows, num_cols])
#print(mx2_deyeY2)   # (60, 180)
# ------
from PIL import Image
im_deyeY2 = Image.fromarray(mx2_deyeY2)
img_resize = im_deyeY2.resize((180*3, 60*3))
img_resize.show()

```

QEU:FOUNDER ： “ピンの色が普通である場合の3枚の画像（左、中央、右）をインプットしたときの解析結果の画像をみてみましょう。**3枚（右-中央-左）の画像から作ったメトリックス**ですから、画像が立体化されているでしょう？”

![image3-42-2](https://yaber1965.github.io/images/image3-42-2.jpg)

D先生 ： “中央のピン（PINO5-1あたり）の様子を見ればわかります。このメトリックスは端子の品質評価にはよさげですね。”

QEU:FOUNDER ： “このRTメトリックスにはもう一つの利点があります。ただし、逆に欠点ともいえるかもね・・・（笑）。ピンに色を塗ってみました。そのとき、両目RTメトリックスはどうなるか？”

![image3-42-3](https://yaber1965.github.io/images/image3-42-3.jpg)

D先生 ： “ずいぶん「えげつなく」色を変えますね（笑）。それでも・・・。”

![image3-42-4](https://yaber1965.github.io/images/image3-42-4.jpg)

QEU:FOUNDER ： “両目RTメトリックスの分布はほとんど変化しません。いやぁ困った・・・、端子さびなどの色の変化を検出できない・・・。”

D先生 ： “これは逆に言うと、**この両目RTメトリックスを使えばVRによる学習データでも実用十分な正解度で不良判別できる**可能性を示しています。”

QEU:FOUNDER ： “どうしても色の異常も判別したければ、白黒画像ではなくRGB画像を使う必要があるよね。かなり計算処理が大変だけど・・・。”

## ～　まとめ　～

### ・・・　前回のつづきです　・・・

QEU:FOUNDER ： “（リーナス君の）オタクの遊びが世界を変えたんだよ。今思えば、LINUXがもたらした革命はGAFAどころじゃなかった。”

![image3-42-5](https://yaber1965.github.io/images/image3-42-5.jpg)

D先生 ： “皆が見えないだけ・・・。”

C部長 ： “LinuxはOSなので地味ですよね。”

D先生 : “いや・・・違うって・・・。**最大の革命は「（成果物を）共有」すること**・・・。ちなみに、ひと昔のQEUシステムが目指した品質改善スキーム（DMAICS）のSは共有（shear）です。これは、もともとLinuxの発想からきています。”

![image3-42-6](https://yaber1965.github.io/images/image3-42-6.jpg)

C部長： “なつかしー。”

QEU:FOUNDER ： “GNUに代表されるフリーソフトウェアの考え方ですよね。**フリーとは「タダ（無料）」という意味ではなく、「自由」という意味です。**”

![image3-42-7](https://yaber1965.github.io/images/image3-42-7.jpg)

D先生 ： “フリーソフトって、（タダなので）ありがたいけど変な考え方だと思っていました。しかし、**実際に創造する人たち（プログラマetc）からみると自然なこと**です。”

QEU:FOUNDER ： “リーナスもインターネットで「X86で動くUNIXができたよ」と、世界中に知らせたときに多くの人が連絡してくれたそうです。そして、これまた「好き者」が一緒になって開発が進み、今のWindowやMacをしのぐOSシステムに成長していきました。**そのようにしてできた成果物には「所有の概念」はありうるの？**”

D先生 ： “「Richard Stallman」のいううとことの、**「フリー（自由）」**しかなくなりますね”

QEU:FOUNDER ： “**「共有」というのは、ある意味で創造物の必然**なんです。逆に言うと、一人で完結している成果物は大したことがない（笑）。**すごい成果物ほどお金を儲けることができないという矛盾**・・・。しかし、共有という概念はプログラムだけじゃない。コンテンツの共有化はデジタルアート関連ではもっと普及していいます。HDRI_HAVENの例をドン！！”

![image3-42-8](https://yaber1965.github.io/images/image3-42-8.jpg)

QEU:FOUNDER ： “HDRIは3DCGを作るときに必要不可欠なHDRI（360度立体）画像のことであり、このシステムはHDRIをシェアしています。もちろん有料コンテンツもあるからね。”

D先生 ： “こういう共有物がたくさん手に入らないと、CGクリエイターもコストと手間がかかって仕方ないですからね。**HDRIをHAVENに共有しないと、困るのは自分なんです。**”

QEU:FOUNDER ： “**自動検査技術の普及を決めるのは「コンテンツの共有」**でしょうね。もし、コネクタの3DCGデータが共有されていれば、あっという間に学習データが作れるんです。”

