## Reinforcement learning using "Pattern RT method" as techno-metrics

### [Technology field]

This invention uses RT metrics which is multivariate analysis method of the Taguchi Method, and it applies to reinforcement learning its applied method named “Pattern RT Method”.

### [Technology background]

Reinforcement learning began in the 1960s when the Bellman equation was proposed as control engineering; AI won the Go world championship in 2016, deep learning embedded in reinforcement learning played a major role (Figure 1). And reinforcement learning methods are advancing rapidly after deep learning has got popularity (Figure 2). In recent years, reinforcement learning has been introduced to robots to achieve complex behaviors such as bipedal walking and automatic driving.

**(Fig. 1: Historic Victory of ALPHA GO)**

![imageRL1-27-1](https://yaber1965.github.io/images/imageRL1-27-1.jpg)

**(Fig. 2: Development of Reinforcement Learning Technology)**

![imageRL1-27-2](https://yaber1965.github.io/images/imageRL1-27-2.jpg)

However, still challenges remain for reinforcement learning to be popular. Still only a few examples of robots using reinforcement learning are available (Figure 3) because reinforcement learning is too costly for training (learning) up to now.

**(Fig. 3: Current Status of Reinforcement Learning)**

![imageRL1-27-3](https://yaber1965.github.io/images/imageRL1-27-3.jpg)

Reinforcement learning means that the learning subject (Agent) communicates with environment while the Agent learns how to issue commands (actions) (Figure 4). Whereas VR (simulation, etc.) technology can be more helpful to reduce learning cost than real environment (Fig. 5). If simulation can make its learning like real environment, development can start at earlier stage of product realization process (R&D phase), resulting in faster product release (Fig. 6). 

**(Fig. 4: Reinforcement Learning Ecosystem)**

![imageRL1-27-4](https://yaber1965.github.io/images/imageRL1-27-4.jpg)

**(Fig. 5: Substituting a real environment with a simulation (VR))**

![imageRL1-27-5](https://yaber1965.github.io/images/imageRL1-27-5.jpg)

**(Fig. 6: Product Realization Process)**

![imageRL1-27-6](https://yaber1965.github.io/images/imageRL1-27-6.jpg)

As an example of complex reinforcement learning, convolutional neural network (CNN) is used for automatic driving and Atari. CNN(Fig.7) compresses 2 dimensional images into 1 dimensional vector to input to deep learning. Although this method is extremely powerful, it requires powerful computing facility due to multi-layer convolution functions.

**(Fig. 7: How CNN works)**

![imageRL1-27-7](https://yaber1965.github.io/images/imageRL1-27-7.jpg)

### [Problems to be solved by this invention]

Here is an example of computational load when the famous Atari_Breakout is trained by CNN reinforcement learning (see movie). The PC used for this project not only has Intel_i7 CPU which is high-spec CPU even in 2020, but also the computer in question probably might have high-performance GPU although it is not mentioned in the text. Even though, 90 hours calculation time are required for learning (Figure 8).

**(Movie: Examples of Atari's Reinforcement Learning)**

<iframe width="560" height="315" src="https://www.youtube.com/embed/V1eYniJ0Rnk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

**(Fig. 8: Computational load for CNN reinforcement learning)**

![imageRL1-27-8](https://yaber1965.github.io/images/imageRL1-27-8.jpg)

Computer (PC) required for reinforcement learning are so high spec because functions defined in deep learning are complex. Of course, the complexity of the function depends on project (environment, game etc) to be solved, but the function used for training in this Atari_Breakout example is relatively simple one for CNN reinforcement learning.

**(Fig. 9: Comparison of DL Function Definitions)**

![imageRL1-27-9](https://yaber1965.github.io/images/imageRL1-27-9.jpg)

To make reinforcement learning easier-to-use for robotics, it is necessary to achieve similar performance with simpler architectures (functions). No matter the method is not generic, or the logic could be applied to only a few projects. Anyway, reinforcement learning is applicable to only 30 percent of themes... (see comments in Figure 3).

### [Means to solve problems]

This invention modifies DL architecture (function definition) of deep learning (DL) reinforcement learning as shown in Figure 10 and it uses "pattern RT metrics" for input to deep learning.

**(Fig. 10: DL Function Architecture)**

![imageRL1-27-10](https://yaber1965.github.io/images/imageRL1-27-10.jpg)

The concept of this invention is not new and it is called Feature_Engineering in machine learning. Its typical example is Principal Component Analysis (PCA) which is widely used in image recognition (Figure 11). Principal component analysis converts two-dimensional image into one-dimensional vectors called principal components. Using 150 of these vectors, the image can be reproduced fairly well.

**(Fig. 11: Feature-Engineering with PCA)**

![imageRL1-27-11](https://yaber1965.github.io/images/imageRL1-27-11.jpg)

However, PCA cannot simplify deep learning’s input data to the extreme. "Simplifying DL input data" mentioned above has two implications: one is "minimizing data dimension to be input" and the other is "making the fitted DL function close to linear”. For example, as the simplification of the function, the performance of automated visual inspection machine using the conventional RT method and one using new RT method (Figures 12 and 13) are compared.

**(Fig. 12: Performance of automated visual inspection machines using conventional RT methods)**

![imageRL1-27-12](https://yaber1965.github.io/images/imageRL1-27-12.jpg)

**(Fig. 13: Performance of automated visual inspection machines using new RT method)**

![imageRL1-27-13](https://yaber1965.github.io/images/imageRL1-27-13.jpg)

Comparing Figures 12 and 13, changing RT metrics resulted in 20% of impact in discrimination performance. Furthermore, the type of kernel of optimal performance among the four types applied to SVM (Support Vector Machine) has also changed. The new RT metrics not only improved performance, but also changed kernel from "Poly (conventional RT)" to "Linear (new RT)" (Figure 14).


**(Fig. 14: Kernel Types in SVM)**

![imageRL1-27-14](https://yaber1965.github.io/images/imageRL1-27-14.jpg)

Thus, we should optimize DL function properly using (techno-)metrics of this invention (Figure 15). Now, one of tips of how to obtain "good metrics" is to match order pattern (action) with state pattern. As for reinforcement learning’s characteristics, number of dimensions of state is generally no less than the number of actions.

**(Fig. 15: Function Simplification by metrics)**

![imageRL1-27-15](https://yaber1965.github.io/images/imageRL1-27-15.jpg)

The Taguchi method's RT method is very useful to generate these metrics: it compares the multivariate data (vector) under standard conditions with one under measurement to convert their differences into two-dimensional values: sensitivity and SN(signal-to-noise) ratio (Figure 16).

**(Fig. 16: Principles of RT metrics)**

![imageRL1-27-16](https://yaber1965.github.io/images/imageRL1-27-16.jpg)

The "Pattern RT Method" is a methodology to generate multiple representative patterns, which are used for unit space (standard vector). On the other hand, signal space (measurement vector) is only one to compare to generate metrics. The bundle of these metrics has become "state input" of reinforcement learning.

**(Fig. 17: Pattern RT Metrics)**

![imageRL1-27-17](https://yaber1965.github.io/images/imageRL1-27-17.jpg)

### [Example_1]

The game 2048, shown in Figure 18, will be used for environment of reinforcement learning. Its board consist of 4x4 numbered panels, which can be slide up, down, left, right to increase these number for higher score. 

**(Fig. 18: Games adopted as environment)**

![imageRL1-27-18](https://yaber1965.github.io/images/imageRL1-27-18.jpg)

An experiment was done using "convolutional RT method" as comparison with targets to evaluate this invention. The convolutional RT method applies convolution to convert patterns into metrics　like CNN (convolutional neural network). Training results are shown in Figure 19 and Figure 20. 

**(Fig. 19: Epsilon values and game board corner values)**

![imageRL1-27-19](https://yaber1965.github.io/images/imageRL1-27-19.jpg)

**(Fig. 20:  Other Performance Trends)**

![imageRL1-27-20](https://yaber1965.github.io/images/imageRL1-27-20.jpg)

This experiment uses DQN experience replay as reinforcement learning method, which is repeated 5000 times. 

### [Example_2]

The next experiment is applied "sliding RT method" as comparison case. The sliding RT method has only one member of unit space as the basis of evaluation, and it has multiple members of signal space members for comparison (Figure 21). Whereas the pattern of the nth turn is used for unit space and signal space are ones after pressing up, down, left, right keys (4 levels) actions. 

**(Fig. 21: Slide RT’s concept)**

![imageRL1-27-21](https://yaber1965.github.io/images/imageRL1-27-21.jpg)

This trial also raise performance with learning, but almost the same level as one of Example (1). 

**(Fig. 22: Epsilon values and game board corner values)**

![imageRL1-27-22](https://yaber1965.github.io/images/imageRL1-27-22.jpg)

**(Fig. 23: Other Performance Trends)**

![imageRL1-27-23](https://yaber1965.github.io/images/imageRL1-27-23.jpg)

This experiment also uses DQN experience replay as reinforcement learning method as well as Example (1), 5000 times are repeated. 

### [Example_3]

This experiment is a case of reinforcement learning applied "Pattern RT method" of this invention. This method requires to build patterns first. The pattern of game instructions (ACTION) when score increases during learning is shown in Figure 24. Thus, it is known that score can be increased under certain instruction patterns in game 2048. 

**(Fig. 24: Instructional Patterns at Better Scores)**

![imageRL1-27-24](https://yaber1965.github.io/images/imageRL1-27-24.jpg)

There are four types of **“standard operation"** with high scores in game 2048, the breakdown is shown in Figure 25. Patterns are generated for unit space automatically according to above knowledge. An example of standard patterns using random numbers is shown in Figure 26. 

**(Fig. 25: Capture multiple images at the same time)**

- **命令0（左）と命令1（上）　→　パターンが左上に偏る**
- **命令0（左）と命令3（下）　→　パターンが左下に偏る**
- **命令2（右）と命令1（上）　→　パターンが右上に偏る**
- **命令2（右）と命令3（下）　→　パターンが右下に偏る**


**(Fig. 26: Slight back and tilt of terminals)**

![imageRL1-27-25](https://yaber1965.github.io/images/imageRL1-27-25.jpg)

Experimental results after 5000 iterations of reinforcement learning are shown in Figure 27 and Figure 28. Learning performance is slightly higher than examples (1) and (2). 

**(Fig. 27: Epsilon values and game board’s corner values)**

![imageRL1-27-26](https://yaber1965.github.io/images/imageRL1-27-26.jpg)

**(Fig. 28: Other Performance Trends)**

![imageRL1-27-27](https://yaber1965.github.io/images/imageRL1-27-27.jpg)

Further experimental results are shown in Figures 29 and 30, which increased the number of learning iteration to 20000. Some plays have value of 1024 at corners and play with score over 12000 appears in the second half of the learning process. 

**(Fig. 29: Epsilon values and game board corner values)**

![imageRL1-27-28](https://yaber1965.github.io/images/imageRL1-27-28.jpg)

**(Fig. 30: Other Performance Trends)**

![imageRL1-27-29](https://yaber1965.github.io/images/imageRL1-27-29.jpg)

For a reference, this is a report of a college student's graduate research about reinforcement learning of 2048 game using DQN. The function used in the experiment in the graduation thesis has 300 nodes and 5 layers which function is quite complex. On the other hand, the function used in this invention has only 128 nodes and 2 layers (Fig. 31). (Note: The basis for determining the architecture of the function in the report is unknown. The optimal architecture was not verified in this example, and there is room for further study to improve the score.) 

**(Fig. 31: DL Function Architecture)**

![imageRL1-27-30](https://yaber1965.github.io/images/imageRL1-27-30.jpg)

Figures 32 and 33 show the results of reinforcement learning experiments in the above report. It is not possible to simply compare between the result of this invention and the report since the various conditions are different. However, it is reasonable to consider that this invention with simple function (NODE:128 x LAYER:2) has performance of "Experiment 2" in the report. Thus, reinforcement learning with techno-metrics could get higher score than conventional methods even though simpler function. 

**(Fig. 32: Experimental results -1)**

![imageRL1-27-31](https://yaber1965.github.io/images/imageRL1-27-31.jpg)

**(Fig. 33: Experimental results -2)**

![imageRL1-27-32](https://yaber1965.github.io/images/imageRL1-27-32.jpg)

### [Industrial applicability]

This invention could be used only for small portion of cases where reinforcement learning can be used (Figure 3). However, it is still said that it is useful. It is not easy to use pattern RT method without making some modifications to "environment" of reinforcement learning. However, some artificial environments such as machining process are easy to improve environment, even if it is originally difficult to create patterns.

**(Fig. 3(Revisited): Current Status of Reinforcement Learning)**

![imageRL1-27-33](https://yaber1965.github.io/images/imageRL1-27-33.jpg)

If we could find "patterns" to facilitate reinforcement learning, it will not be difficult to apply them into the process. Reinforcement learning using the pattern RT method can works under lower cost than conventional reinforcement learning because it is easier to learn in VR (virtual space). Moreover, its learned results would be robust to changes of environment.

The invention cannot be applied in cases which environment cannot be converted by virtual reality (VR). It is more reasonable that "not worthy" rather than "not possible”. Currently, various VR software has been released (Figure 34 as an example), and many technical problems can be solved according to skills of the developer (VR user).

**(Fig. 34: Unity-VR)**

![imageRL1-27-34](https://yaber1965.github.io/images/imageRL1-27-34.jpg)
