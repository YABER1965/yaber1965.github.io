## QEUR21_FATOY15:　協調フィルタリング (その1)

### ～　枯れ木も山のなんとか　～

D先生 ： “おや？またやるの？このシリーズがこれほど伸びるとは思いませんでした。”

QEU:FOUNDER ： “いやぁ・・・、今回もfastaiで言及されている内容なので、やはり極力このシリーズに載せておきたい。ほっておくと忘れられてしまい、埋没しかねないとおもって・・・。”

![image1-60-1](https://yaber1965.github.io/images/image1-60-1.jpg)

D先生 ： “J国の場合だと大いにあり得ます。いまだに、この手のモノはCOOLであると思っているようですし・・・。・・・つまり、自分には関係はないと・・・。”

QEU:FOUNDER ： “だから「猫の国」は成長しないんだよ・・・。まあ、話したいことは結局のところ**「エンベッディング（embedding）」**のことだけなんですよね、前回のCAT2VECで述べたアレ・・・。”

D先生 ： “前回はエンベッディングをKerasで計算しました。エンベッディングって、なんでしたっけ？”

![image1-60-2](https://yaber1965.github.io/images/image1-60-2.jpg)

QEU:FOUNDER ： “この説明（↑）は分かりやすかった・・・。”

D先生 ： “まだわかんない・・・。”

![image1-60-3](https://yaber1965.github.io/images/image1-60-3.jpg)

QEU:FOUNDER ： “図が入っている分だけ、この説明がわかりやすいかな？ただし、整合が採れているかどうかあやしい・・・。（1）すべてのアイテム表現がワンホットに変換される。(2)さらに、そのワンホット表現がベクトルに変換される。(3)ベクトルの次元数はPyTorchでカスタム設定できる。これだけわかればいいかな・・・。”

D先生 ： “もちろん、Kerasでもできるわけですよね・・・。”

QEU:FOUNDER ： “じゃあ、これから事例研究をやるよ。しかも、かなり面白い奴(↓)・・・。”

![image1-60-4](https://yaber1965.github.io/images/image1-60-4.jpg)

D先生 ： “YouTubeでも多分、同じしくみを使っているはずです。いわゆる**「レコメンデーション（推薦）・システム」**でしょ？”

QEU:FOUNDER ： “ユーザの閲覧履歴を記録し、それを分析してユーザーにとって「有益な」動画を推薦するわけ・・・。”

D先生 ： “で？なに？「有益(な動画)」って・・・。”

QEU:FOUNDER ： “知らない・・・（笑）。なにはともあれ、いきなりプログラムをドン・・・。”

```python
# -----
# 協調フィルタリングのためのエンベッドベクトル作成プログラム
# colab_filtering_example.py
# 詳細はオリジナルを参照してください
# 参考：https://github.com/devforfu/pytorch_playground/blob/master/movielens.ipynb
# 参考：https://www.youtube.com/watch?v=LxbJJ74Nz7Y&t=396s
# -----
import math
import copy
import pickle
import zipfile
from textwrap import wrap
from itertools import zip_longest
from collections import defaultdict
# -----
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
# -----
import torch
from torch import nn
from torch import optim
from torch.nn import functional as F 
from torch.optim.lr_scheduler import _LRScheduler
plt.style.use('ggplot')
# -----
def set_random_seed(state=1):
    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)
    for set_state in gens:
        set_state(state)
RANDOM_STATE = 1
set_random_seed(RANDOM_STATE)

# -----
# pick any other dataset instead
# 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'
ratings = pd.read_csv('./ratings.csv') 
movies = pd.read_csv('./movies.csv') 
ratings.head()
movies.head()

# -----
def tabular_preview(ratings, n=15):
    """Creates a cross-tabular view of users vs movies."""
    
    user_groups = ratings.groupby('userId')['rating'].count()
    top_users = user_groups.sort_values(ascending=False)[:15]

    movie_groups = ratings.groupby('movieId')['rating'].count()
    top_movies = movie_groups.sort_values(ascending=False)[:15]

    top = (
        ratings.
        join(top_users, rsuffix='_r', how='inner', on='userId').
        join(top_movies, rsuffix='_r', how='inner', on='movieId'))

    return pd.crosstab(top.userId, top.movieId, top.rating, aggfunc=np.sum)
tabular_preview(ratings, movies)

# -----
def create_dataset(ratings, top=None):
    if top is not None:
        ratings.groupby('userId')['rating'].count()
    
    unique_users = ratings.userId.unique()
    user_to_index = {old: new for new, old in enumerate(unique_users)}
    new_users = ratings.userId.map(user_to_index)
    
    unique_movies = ratings.movieId.unique()
    movie_to_index = {old: new for new, old in enumerate(unique_movies)}
    new_movies = ratings.movieId.map(movie_to_index)
    
    n_users = unique_users.shape[0]
    n_movies = unique_movies.shape[0]
    
    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})
    y = ratings['rating'].astype(np.float32)
    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)
(n, m), (X, y), _ = create_dataset(ratings)
print(f'Embeddings: {n} users, {m} movies')
print(f'Dataset shape: {X.shape}')
print(f'Target shape: {y.shape}')

中略

# -----
# Training Loop
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, ran-dom_state=RANDOM_STATE)
datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid)}
dataset_sizes = {'train': len(X_train), 'val': len(X_valid)}

# -----
minmax = ratings.rating.min(), ratings.rating.max()
minmax
#(0.5, 5.0)

# -----
net = EmbeddingNet(n_users=n, n_movies=m, 
    n_factors=150, hidden=[500, 500, 500], 
    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])

lr = 1e-3
wd = 1e-5
bs = 2000
n_epochs = 100
patience = 10
no_improvements = 0
best_loss = np.inf
best_weights = None
history = []
lr_history = []

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

net.to(device)
criterion = nn.MSELoss(reduction='sum')
optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)
iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))
scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))

for epoch in range(n_epochs):
    stats = {'epoch': epoch + 1, 'total': n_epochs}
    
    for phase in ('train', 'val'):
        if phase == 'train':
          training = True
        else:
          training = False

        running_loss = 0
        n_batches = 0
        
        for batch in batches(*datasets[phase], shuffle=training, bs=bs):
            x_batch, y_batch = [b.to(device) for b in batch]
            optimizer.zero_grad()
            #print(x_batch[:,0])
        
            # compute gradients only during 'train' phase
            with torch.set_grad_enabled(training):
                outputs = net(x_batch[:,0], x_batch[:,1], minmax)
                loss = criterion(outputs, y_batch)
                
                # don't update weights and rates when in 'val' phase
                if training:
                    loss.backward()
                    optimizer.step()
                    scheduler.step()
                    lr_history.extend(scheduler.get_lr())
                    
            running_loss += loss.item()
            
        epoch_loss = running_loss / dataset_sizes[phase]
        stats[phase] = epoch_loss
        
        # early stopping: save weights of the best model so far
        if phase == 'val':
            if epoch_loss < best_loss:
                print('loss improvement on epoch: %d' % (epoch + 1))
                best_loss = epoch_loss
                best_weights = copy.deepcopy(net.state_dict())
                no_improvements = 0
            else:
                no_improvements += 1
                
    history.append(stats)
    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))
    if no_improvements >= patience:
        print('early stopping after epoch {epoch:03d}'.format(**stats))
        break
    #loss improvement on epoch: 1
    #[001/100] train: 0.9892 - val: 0.8498
    #loss improvement on epoch: 2
    #[002/100] train: 0.7702 - val: 0.8044
    #loss improvement on epoch: 3

中略

    #[013/100] train: 0.4902 - val: 0.8221
    #[014/100] train: 0.4329 - val: 0.8491
    #[015/100] train: 0.4265 - val: 0.8525
    #early stopping after epoch 015

# -----
ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')
# -----
_ = plt.plot(lr_history[:2*iterations_per_epoch])

# -----
net.load_state_dict(best_weights)

groud_truth, predictions = [], []

with torch.no_grad():
    for batch in batches(*datasets['val'], shuffle=False, bs=bs):
        x_batch, y_batch = [b.to(device) for b in batch]
        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)
        groud_truth.extend(y_batch.tolist())
        predictions.extend(outputs.tolist())

groud_truth = np.asarray(groud_truth).ravel()
predictions = np.asarray(predictions).ravel()

final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))
print(f'Final RMSE: {final_loss:.4f}')
# Final RMSE: 0.8706

with open('./best.weights', 'wb') as file:
    pickle.dump(best_weights, file)

# -----
from sklearn.decomposition import PCA

with open('./best.weights', 'rb') as file:
    best_weights = pickle.load(file)
net.load_state_dict(best_weights)

def to_numpy(tensor):
    return tensor.cpu().numpy()

_, _, (user_id_map, movie_id_map) = create_dataset(ratings)
embed_to_original = {v: k for k, v in movie_id_map.items()}
popular_movies = rat-ings.groupby('movieId').movieId.count().sort_values(ascending=False).values[:1000]

embed = to_numpy(net.m.weight.data)
pca = PCA(n_components=5)
components = pca.fit(embed[popular_movies].T).components_
components.shape
# (5, 1000)

# -----
components_df = pd.DataFrame(components.T, columns=[f'fc{i}' for i in range(pca.n_components_)])
movie_ids = [embed_to_original[idx] for idx in components_df.index]
meta = movies.set_index('movieId')
components_df['movie_id'] = movie_ids
components_df['title'] = meta.loc[movie_ids].title.values
components_df['genres'] = meta.loc[movie_ids].genres.values
components_df.sample(4)

# ---------------------------
# リストを生成する（全体）
temp_data_name=components_df['title'].values
temp_x=components_df['fc0'].values
temp_y=components_df['fc1'].values
# ---------------------------
# 見えなくなるので、一部だけを抽出する
data_name=[]
x=[]
y=[]
for i in range(100):
    data_name.append(temp_data_name[i])
    x.append(temp_x[i])
    y.append(temp_y[i])
# ---------------------------
# グラフにプロットする
fig = plt.figure(figsize=(20, 15))
ax = fig.add_subplot(1, 1, 1)
for (i,j,k) in zip(x,y,data_name):
        ax.plot(i,j,marker='o', markersize=15)
        ax.annotate(k, xy=(i, j))
# 汎用要素を表示
ax.grid(True)  # grid表示ON
ax.set_title("Movie's Characteristics")
ax.set_xlabel('fc0')  # x軸ラベル
ax.set_ylabel('fc1')  # y軸ラベル
plt.show()

```

**（参考ブログ）**

![image1-60-5](https://yaber1965.github.io/images/image1-60-5.jpg)

QEU:FOUNDER ： “プログラムの途中での「中略」が多いけど、これは**「元のブログからとって（コピペ）してください」**という意味です。あと、ひょっとしたらコピペでエラーがでるかもしれないから、その時には我々のコードを参考にしてください。一応、エラーがないことを確認しているんで・・・。”

![image1-60-6](https://yaber1965.github.io/images/image1-60-6.jpg)

QEU:FOUNDER ： “CSVファイルを使ったほうが処理がやりやすいです。まずは、この中間結果を見てみましょう。行がユーザで列が映画です。この表がもっとも大事です。”

![image1-60-7](https://yaber1965.github.io/images/image1-60-7.jpg)

D先生 ： “えっ？大事なデータのはずなのにNaNがたくさんありますよ。データがないです・・・。”

QEU:FOUNDER ： “この**「NaNが大事」**なの・・・。レコメンドシステムは、NaNの（まだ見ていない）映画が見て面白い映画かどうかを予測するんです。”

D先生 ： “じゃあ、どうやって予測するんですか？”

QEU:FOUNDER ： “高評価したものと似たような特性の映画であれば推薦できるし、似たような特性のユーザが高評価した映画であれば推薦できるでしょ？これは最後はロジック次第・・・。何はともあれ、「エンベッド(embedding)」ができていないと分析できません。”

![image1-60-8](https://yaber1965.github.io/images/image1-60-8.jpg)

D先生 ： “エンベッドの出力ベクトル数を5件にしたんですか？”

QEU:FOUNDER ： “いや、このプログラムではエンベッドの出力を150件にしています。その後で、主成分分析（PCA）を使って5次元にまで落としているんです。”

D先生 ： “なるほど、PCAを使うのも合理的ですね。”

![image1-60-9](https://yaber1965.github.io/images/image1-60-9.jpg)

QEU:FOUNDER ： “特性をみるために散布図を最後に作成しました。ドン・・・。映画のバラツキからなにか傾向が見える？”

D先生 ： “見たときのない映画が多いので、私としてはなんとも・・・。スケールは大作の度合いを示すのかな？それともマジメさを示すのかな？”

QEU:FOUNDER ： “まあ、なんかあるような気がするでしょ・・・？”

D先生 ： “**前回のCAT2VECの背後にはこんな仕組みが動いていた**んですね、よくわかりました。このようにカテゴリ（アイテム）のベクトル化ができるのであれば、いろいろな応用ができますよね。ところで、前回と同様に、「タグチさん家の方法(Taguchi Methods)」って使えないの？”

QEU:FOUNDER ： “それは考え中です・・・。”


## ～　まとめ　～

QEU:FOUNDER ： “またオッサンをやります。消費税と経済を語るおっさん・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/EnaQCoO0SfM" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 ： “おっ・・・。“

C部長 ： “おっ・・・。“

QEU:FOUNDER ： “必見です。”

