## QEUR21_VINSP10: オリエンテーション ～ 旧プロジェクト技術明細書(Convolution RT, English ver.)

## ～　Preface　～

## [Technology background]

Various computer-based image recognition schemes have been developed through AI contests, etc., and deep learning has recently been highly appreciated its high level of discrimination performance (Fig. 1).

**(Fig. 1: Transition of image recognition technology)**

![image2-10A-2](https://yaber1965.github.io/images/image2-10A-2.jpg)

These high-performance image recognition technologies are designed to identify subtle features of natural objects (e.g., dogs, cats). However, sometimes it is not necessary to use such complex methods when iden-tifying signs that are freely designed by the user.

**(Fig. 2: Examples of signs)**

![image2-10A-3](https://yaber1965.github.io/images/image2-10A-3.jpg)

For signs, the most important feature is the color design. Different colors of signs often have different meanings. On the other hand, natural objects can be identified only by black and white image processing because only the shape and pattern are more important.

### [Means to solve problems.]

Present invention performs image recognition by using the convolution process together with RT (Recog-nition Taguchi) method, which is one of the Taguchi method techniques used for quality improvement. The image recognition so called by present invention is the detection of a specified sign (in this case, a red arrow on a white background) from a captured image, as shown in Fig. 3.

**(Fig. 3: concept of Five Eyes method)**

![image2-10A-4](https://yaber1965.github.io/images/image2-10A-4.jpg)

A similar technology as present invention is HAACASCADE technology, which is widely used for face and eye recognition (Fig. 4).

**(Fig. 4: HAACASCADE technology)**

![image2-10A-5](https://yaber1965.github.io/images/image2-10A-5.jpg)

The features of the present invention and HAACASCADE technology are compared as shown in Fig. 5. The advantage of the present invention is that users can easily identify their own designed signs.

**(Fig. 5: Comparison between double-eye method and the new double-eye method)**

![image2-10A-6](https://yaber1965.github.io/images/image2-10A-6.jpg)

The analysis flow of automatic visual inspection machine of present invention is shown below (Fig. 10). The first half of processing flow is FE method, and the second half is new MT method.

**(Fig. 6: Analysis flow of the present invention [convolution + RT metrics].))**

![image2-10A-7](https://yaber1965.github.io/images/image2-10A-7.jpg)

The six types of components that the invention uses in the convolution process are shown in Figures 7 and 8. Whereas, the parts for convolution are classified into three types: DATUM, LINE, and BEND. "DA-TUM" is designed as a zero-order function (cross of vertical and horizontal parts), "LINE" is designed as a linear function, and "BEND" is designed to detect a quadratic function.

**(Fig 7: Parts for folding, part 1)**

![image2-10A-8](https://yaber1965.github.io/images/image2-10A-8.jpg)

**(Fig 8: Parts for folding, part 2)**

![image2-10A-9](https://yaber1965.github.io/images/image2-10A-9.jpg)

Figures 9 and 10 show the results of the heatmap visualization of the circular image after convolution with the above components.

**(Fig 9: Convolution results for a circular image: Part 1)**

![image2-10A-10](https://yaber1965.github.io/images/image2-10A-10.jpg)

**(Fig 10: Convolution results for a circular image: Part 2)**

![image2-10A-11](https://yaber1965.github.io/images/image2-10A-11.jpg)

As another example, the results of similar processing on a cross-shaped image are shown in Figure 11 and Figure 12. Thus, the results processed with each component are completely different depending on the im-age to be inspected. In the case of a circular shape, the quadratic component is strongly detected, and in the case of a cross shape, the linear component is strongly detected.

**(Fig 11: Convolution results for crossed images: Part 1)**

![image2-10A-12](https://yaber1965.github.io/images/image2-10A-12.jpg)

**(Fig 12: Convolution results for crossed images: Part 2)**

![image2-10A-13](https://yaber1965.github.io/images/image2-10A-13.jpg)

RT method is one of the Taguchi methods proposed by Genichi Taguchi, and it is often used for image recognition. Its processing flow is shown in Fig. 13.

**(Fig. 13: Calculation of main effect)**

![image2-10A-14](https://yaber1965.github.io/images/image2-10A-14.jpg)

A conceptual model diagram of RT method is shown in Fig. 14, where RT method collapses a large amount of information (e.g., millions of pixels for an image) into two metrics, Y1 (sensitivity; linear component) and Y2 (signal-to-noise ratio; quadratic component).

**(Fig. 14: Analysis model of RT method)**

![image2-10A-15](https://yaber1965.github.io/images/image2-10A-15.jpg)

The difference between the present invention and ordinary RT methods is the definition of the unit space. While the ordinary RT method adopts "representative data of a homogeneous group of data to be detected" as the unit space, the present invention adopts "the convolution result of a zero-order function". This is be-cause RT method measures the distance between the unit space (representative data) and the measurement data, while the present invention extracts the primary and secondary components of the variation based on the zero-order component. Therefore, present invention can be considered as a kind of principal compo-nent analysis rather than discriminant analysis by distance. 

An example of the discrimination logic implemented in the image discrimination application is shown in Figure 15. Since there are only two metrics in the present invention, the judgment logic is extremely sim-ple. You can analyze a sample image to determine the range of variation of the metrics, and then use IF statements to make the decision. Of course, more advanced methods such as k-nearest neighbor, SVM, and deep learning can also be used. In that case, advanced knowledge is required for the user.

**(Fig. 15: Model of RT method)**

![image2-10A-16](https://yaber1965.github.io/images/image2-10A-16.jpg)

## [Example-1]

In order to understand the characteristics of this image recognition method, we experimented with various changes in the image to be measured and how the RT metrics changed (Figures 16, 17, 18, and 19).  

**(Fig. 16: Test image #1)**

![image2-10A-17](https://yaber1965.github.io/images/image2-10A-17.jpg)

**(Fig. 17: Test image #2)**

![image2-10A-18](https://yaber1965.github.io/images/image2-10A-18.jpg)

**(Fig. 18: Test image #3)**

![image2-10A-19](https://yaber1965.github.io/images/image2-10A-19.jpg)

**(Fig. 19: Test image #4)**

![image2-10A-20](https://yaber1965.github.io/images/image2-10A-20.jpg)

From this group of images, we created a scatter plot with RT metrics Y1 and Y2 (Fig. 20). We found that our metrics are strong for image movement and rotation, but weak for deformation and scaling.

**(Fig. 20: Y1-Y2 scatter diagram)**

![image2-10A-21](https://yaber1965.github.io/images/image2-10A-21.jpg)

The following image recognition logic has been designed based on these characteristics of RT-metrics. The data extraction and shaping unit in the first half of the logic ensures that the extracted images are about the same size in the contour detection and resizing functions. 

**(Figure 6 again: Analysis flow of the present invention [convolution + RT metrics])**

![image2-10A-22](https://yaber1965.github.io/images/image2-10A-22.jpg)

## [Example-2]

We have created an image recognition device (PC application) that applies the present invention. Since signs can be designed freely, here we created signs as follows (Figs. 20, 21). 

**(Figure 20: Test image #1)**

![image2-10A-23](https://yaber1965.github.io/images/image2-10A-23.jpg)

**(Figure 21: Test image #2)**

![image2-10A-24](https://yaber1965.github.io/images/image2-10A-24.jpg)

Figure 21 shows the image extraction result for a circular only image (Figure 20). In this case, the extract-ed image is not stable due to the rotation of the sign. In this case, the image recognition may also be unsta-ble as shown in Figure 22. 

**(Fig. 21: Image extraction results for "Test Image #1")**

![image2-10A-25](https://yaber1965.github.io/images/image2-10A-25.jpg)

**(Fig. 22: Image extraction results for "Test Image #1")**

![image2-10A-26](https://yaber1965.github.io/images/image2-10A-26.jpg)

Image extraction is more stable when "Test Image #2" is used, which is based on the design of the sign. Furthermore, by using the Euclidean distance of RGB values, the invention can extract only the necessary color information (Fig 24). 

**(Fig. 23: Image extraction results for "Test Image #2")**

![image2-10A-27](https://yaber1965.github.io/images/image2-10A-27.jpg)

**(Fig. 24: Image extraction results for "Test Image #2")**

![image2-10A-28](https://yaber1965.github.io/images/image2-10A-28.jpg)

## [Industrial applicability]

With present invention, user-designed signs and simple figures can be identified and automated actions such as visual inspection or driving by machines can be performed.

So far, its discrimination accuracy of our invention is not discussed. Although the accuracy of the image discrimination of the present invention is obviously low (Fig. 5), it is not a problem. Since users can de-sign their own signs arbitrarily, they only need to use signs that have 100% discrimination accuracy.

**(Fig. 5 again: Comparison between the invention and the HAACASCADE method)**

![image2-10A-29](https://yaber1965.github.io/images/image2-10A-29.jpg)

The most important advantage of present invention is the use of color for image recognition. By combin-ing images and shapes, an infinite number of unique signs can be generated. Therefore, it is easy to obtain 100% discrimination accuracy. 

**(Fig. 2 again: Data preprocessing)**

![image2-10A-30](https://yaber1965.github.io/images/image2-10A-30.jpg)

Image recognition technology is very simple. A single-board computer (Raspberry Pi, etc.) is sufficient for the required computer specifications, and the time required for learning will be minimal. 
