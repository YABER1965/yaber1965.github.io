## Automated visual inspection machine for inspecting connector terminals of wire harnesses

## [Technology field]

This invention relates to an automatic computerized visual inspection machine for inspecting connector terminals of wire harnesses.

## [Technology background]

According to Kaggle which conducts a machine learning technology competition, one of the trends in machine learning in 2022 is **"TinyML"**, which refers to small-scale machine learning applications that works on edge computers for IoT. Thus, machine learning technology has already entered the popularization phase (Fig. 1 and Fig. 2).

**(Fig. 1: Kaggle Articles)**

![image3-54-1](https://yaber1965.github.io/images/image3-54-1.jpg)

**(Fig. 2: About TinyML)**

![image3-54-2](https://yaber1965.github.io/images/image3-54-2.jpg)

The performance of inspection by human visual inspection is shown in Figure 3. Generally, the defect detection ratio by visual inspectors is about 80% at maximum. And the performance varies greatly depending on the physical and mental condition of the inspector (Fig. 4).

**(Fig. 3: Visual inspection performance)**

![image3-54-3](https://yaber1965.github.io/images/image3-54-3.jpg)

**(Fig. 4: Inspectors condition and flow-out)**

![image3-54-4](https://yaber1965.github.io/images/image3-54-4.jpg)

This invention applies TinyML to connector terminal inspection of **wiring harnesses (hereinafter referred to as "WH")**. WH is a collection of wires and connectors to facilitate wiring (assembly) of electrical systems shown in Figure 5. As a result, the defect rate of WH products is higher than that of other products. Among product defects, connector and terminal defects are regarded as serious defects because they are directly related to the basic performance of the product (conduction stability). Incidentally, there are two types of connectors, male and female, and it is the male connectors that are more prone to (terminal) defects.

**(Fig. 5: What is a wiring harness?)**

![image3-54-5](https://yaber1965.github.io/images/image3-54-5.jpg)

**(Fig. 6: Connector’s terminal inspection)**

![image3-54-6](https://yaber1965.github.io/images/image3-54-6.jpg)

For automatic inspection, it is necessary to collect learning data during the prototype mass production stage of the product realization process shown in Figure 7. However, in general, production number of prototypes is about 200pcs at most, which is not enough for learning data necessary for complex machine learning.

**(Fig. 7: Product Realization Process)**

![image3-54-7](https://yaber1965.github.io/images/image3-54-7.jpg)

Therefore, it is necessary to collect data in the design and development (R&D) phase prior to prototype mass production state. For this purpose, it is necessary to prepare learning data by VR (Virtual Reality). In the case of male connectors, learning data can be easily collected with 3DCG production tools (Fig. 8).

**(Fig. 8: Production of images of connectors by VR)**

![image3-54-8](https://yaber1965.github.io/images/image3-54-8.jpg)

Although it looks very easy for a human to inspect connector terminals, it is much more difficult for machines to perform this task. For example, let us consider a typical defective item called "Tanshi  Nuke”. "Tanshi Nuke” occurs when a terminal is inserted into a connector and the connector fails to lock the terminal as designed to make the terminal retract or tilt.

**(Fig. 9: Terminals retracted)**

![image3-54-9](https://yaber1965.github.io/images/image3-54-9.jpg)

**(Fig. 10: Terminals are tilted)**

![image3-54-10](https://yaber1965.github.io/images/image3-54-10.jpg)

The tilt of terminal can be detected with only one image. However, a vertically retracted terminal might not be detected correctly in a single image (Fig. 11). In other words, detecting anomalies in three-dimensional objects contains a different kind of difficulty than one when deep learning is applied to classify the type of object in an image.

**(Fig. 11: Examples of Image Recognition)**

![image3-54-11](https://yaber1965.github.io/images/image3-54-11.jpg)

### [Problems to be solved by this invention]

In order to run automated visual inspection machine at the launch of mass production, learning task must be completed prior to the design development phase. VR technology is used for this purpose. In addition, multiple cameras are installed in the VR as shown in Figures 12 and 13 to detect three-dimensional anomalies.

**(Fig. 12: Benefits of Multiple Cameras)**

![image3-54-12](https://yaber1965.github.io/images/image3-54-12.jpg)

**(Fig. 13: Multiple cameras installed in VR [Five-eyes])**

![image3-54-13](https://yaber1965.github.io/images/image3-54-13.jpg)

The invention recommends installing five cameras (Five-eyes: Fig. 14). Whereas the center camera is used to generate "standard vector (data)" for Double-eye RT method. The other cameras are used for "measurement vectors (data)" in the left-right and up-down directions.

**(Fig. 14: Concept of the Double-eye RT Method in Five-eyes)**

![image3-54-14](https://yaber1965.github.io/images/image3-54-14.jpg)

To detect anomalies, the invention uses the Mahalanobis distance (Fig. 15), which is set up as a "homogeneous space (unit space)" for good products.

**(Fig. 15: Mahalanobis distance)**

![image3-54-15](https://yaber1965.github.io/images/image3-54-15.jpg)

Instead of setting threshold values for each terminal (pin) to determine abnormalities, this invention uses a supervised learning method (SVM in this case).

## [Means to solve problems]

The system flow diagram of the automated visual inspection machine is shown in Figure 16. Whereas "Feature Engineering" in the figure refers to the calculation of Mahalanobis distance. However, the entire procedure of Five-eyes - Convolutional RT method - Mahalanobis distance should be referred to Feature Engineering originally.

**(Fig. 16: Flow diagram of automatic terminal inspection machine)**

![image3-54-16](https://yaber1965.github.io/images/image3-54-16.jpg)

The Double-eye RT method is the same technology as Five-eyes. The Double-eyes RT method requires three cameras installed to measure the behavior of an object in one axis (X or Y axis). The Five-eyes method, on the other hand, uses five cameras installed to measure the object's behavior in two axes (X and Y axes). The Two-Eyes RT method is a method to generate techno-metrics called "standard SN ratio" in the Taguchi method. The "standard vector" shown in Figure 17 is the image data collected by the central camera.

**(Fig. 17: standard SN ratio)**

![image3-54-17](https://yaber1965.github.io/images/image3-54-17.jpg)

Using the Double-eye RT method, one image can be generated from three images (right - center - left or top - center - bottom) of image data. That image contains a "three-dimensional" image. An example of image created with Double-eye RT metric is shown in Figure 18.

**(Fig. 18: Imaging of Double-eyes RT metrics [pins with normal color])**

![image3-54-18](https://yaber1965.github.io/images/image3-54-18.jpg)

Next, these pins are painted in different color to generate Double-eye RT images. Comparing the aforementioned images with normal pin color, it can be found that they have almost no difference. From this result, machine learning in VR will produce almost the same results as data taken in the actual field with this RT metric.

**(Fig. 19: Imaging of Double-eyes RT metrics [pin color changed])**

![image3-54-19](https://yaber1965.github.io/images/image3-54-19.jpg)

After generating the Double-eye RT metrics, they are converted to convolutional RT metrics. Convolutional RT metrics use the same RT logic as Double-eye method described above to generate metrics by inputting two different images, one for standard and another for measurement (Fig. 20).

**(Fig. 20: Principle of convolutional RT method)**

![image3-54-20](https://yaber1965.github.io/images/image3-54-20.jpg)

Convolutional RT-metrics uses the following eight types of convolutional components (fig. 21,22,23). Convolution is applied to the same image, and the standard image (standard vector) after convolution of standard components is compared with the measurement image (measurement vector) after convolution of various measurement components. And the difference is quantified into metrics (Y1,Y2), whereas Y1 represents the degree of rotation of the figure and Y2 represents the degree of deformation of the figure.

**(Fig. 21: bend group [parts for measurement vector])**

![image3-54-21](https://yaber1965.github.io/images/image3-54-21.jpg)

**(Fig. 22: line group [parts for measurement vector])**

![image3-54-22](https://yaber1965.github.io/images/image3-54-22.jpg)

**(Fig. 23: datum group [parts for standard vector])**

![image3-54-23](https://yaber1965.github.io/images/image3-54-23.jpg)

As a final step of Feature Engineering, **two types of Mahalanobis distances (DY1, DY2) are generated from sensitivity (Y1) and SN ratio (Y2) number with convolutional RT method**. These metrics becomes inputs of variance-covariance matrix for generating Mahalanobis distances. The training data (unit space) should be data of passed products.

**(Fig. 24: Feature Engineering)**

![image3-54-24](https://yaber1965.github.io/images/image3-54-24.jpg)

Feature-engineering is computed per pin (terminal), so the following feature maps of sensitivity (Y1) and SN ratio (Y2) are generated respectively. The new metrics are the value of the multiplication of Y1 and Y2, which is just an index for reference. The new metrics are not needed when determining with SVM. In Figure 23, the value of Y2 is higher in cells where terminal loss has occurred.

**(Fig. 25: Example of feature map [with defects])**

![image3-54-25](https://yaber1965.github.io/images/image3-54-25.jpg)

In this invention, the pass/fail of the inspected item is judged by SVM (Support Vector Machine). If all of the thresholds for judging normal and abnormal are the same within pins, then pass/fail judgments can be made based on Mahalanobis distance only. However, since each pin has different judgment criteria in reality, it is easier to use SVM for data learning.

## [Example_1]

This case study is the terminal inspection of a male connector consisting of 3x9 pins. This 3DCG software allows multiple cameras to be mounted in a virtual space to capture multiple images simultaneously (Fig. 24). 

**(Fig. 26: Capture multiple images at the same time)**

![image3-54-26](https://yaber1965.github.io/images/image3-54-26.jpg)

In this case study, the following two failure modes were generated for Tanshi Nuke: "(1) simple terminal retraction" and "(2) subtle terminal retraction and tilt". The feature maps for the former are shown in Figure 25 and the latter in Figure 26. In addition, two levels of locations where defects occur were defined as "(1) PINNO3-1" and "(2) PINNO4-1. 

**(Fig. 27: Simple terminal back)**

![image3-54-27](https://yaber1965.github.io/images/image3-54-27.jpg)

**(Fig. 28: Slight back and tilt of terminals)**

![image3-54-28](https://yaber1965.github.io/images/image3-54-28.jpg)

The Mahalanobis distance DY1 value is clearly higher at the location of the defect. However, it is unclear in the DY2 distribution. This may be because there is no terminal deformation in this failure mode, and therefore the Y2 values are not responsive. Presumably, if a terminal deformation failure mode were added, DY2 would respond significantly. 

The results of defect determination using SVM (Support Vector Machine) are shown in Figure 27 in the form of a confusion matrix. Whereas the labels are defined as "label=0 for no defects”, "label=1 for PINNO3-1” and "label=2 for PINNO4-1”. In this case study, we tried different types of kernels for SVM, and "kernel=poly" has the highest performance with a judgment accuracy of about 80%. 

**(Fig. 29: Confusion Matrix)**

![image3-54-29](https://yaber1965.github.io/images/image3-54-29.jpg)

The flow-out rate when applied this invention (automatic visual inspection machine) for workshop was calculated as shown in Figure 28. The calculated flow-out rate is 17%, which can be considered that almost same level as one of double inspection by human inspectors. 

**(Fig. 30: Calculation of Flow-out Rate)**

![image3-54-30](https://yaber1965.github.io/images/image3-54-30.jpg)

The amount of data used for this training is shown below, and since SVM is used, the results are relatively good, at least for the number of data. If more data is used for training, the accuracy may be even higher. However, this time, 5 images are generated per measurement. One of the problems with this invention is the large amount of data to be stored.

- **良品（端子抜けなし）　→　120件(LabelNO=0)**
- **不良品（垂直に端子抜け、位置はPINNO3-1）　→　60件(LabelNO=1)**
- **不良品（垂直に端子抜け、位置はPINNO4-1）　→　60件(LabelNO=2)**
- **不良品（斜めに端子抜け、位置はPINNO3-1）　→　60件(LabelNO=1)**
- **不良品（斜めに端子抜け、位置はPINNO4-1）　→　60件(LabelNO=2)**
- **SVMにおけるtrain_test_split率 = 0.3**

However, since all training process are done by computer using VR, the time required to prepare the training data is small and no loss of production sites has occurred. 

## [Industrial applicability]

This invention is originally applicable not only to connector terminal inspection, but also to visual inspection of three-dimensional objects; we only chose connector inspection as an example that can be experimented with very easily using VR.

The challenge is that five cameras are required. This is a cost issue rather than a technical one. In this case, "Five-Eyes (method)," which covers two axes of X and Y axes, was selected as an example. However, it might be dependent on the inspected object, "Double-eyes method," which covers only one axis, may could be sufficiently accurate. In the terminal inspection of male connectors in this case, even three cameras would produce a considerable degree of accuracy.

**(Fig. 31: When using 3 cameras [Double-eye method])**

![image3-54-31](https://yaber1965.github.io/images/image3-54-31.jpg)

Since the purpose of this invention was anomaly detection, Mahalanobis distance was used in the Feature Engineering (extraction of representative metrics) stage. If convolutional RT metrics were input to other machine learning methods instead of Mahalanobis distance, they could also be used for object discrimination. Multiple unit spaces would then need to be prepared. 


