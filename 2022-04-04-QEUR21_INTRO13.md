## QEUR21_INTRO13:　NN’erのためのTM（その13）～超能力AIの開発（その5）

## ～　やっと「超能力AIの完結」デス！！　～

### ・・・　前回の続き　・・・

D先生 ： “超能力AIって、一応はCS-T法を使いますが・・・。”

（CS-T法の入力）

![image1-13-1](https://yaber1965.github.io/images/image1-13-1.jpg)

（CS-T法の出力）

![image1-13-2](https://yaber1965.github.io/images/image1-13-2.jpg)

QEU:FOUNDER ： “このデータ(↑)が、少しだけ変換されて並び替えられます。これが超能力AIのインプットになります。”

![image1-13-3](https://yaber1965.github.io/images/image1-13-3.jpg)

D先生 ： “・・・で、肝心の右側コラム名「beta」の意味は？”

![image1-13-4](https://yaber1965.github.io/images/image1-13-4.jpg)

QEU:FOUNDER ： “以前に紹介したプログラムの中身を見ればわかります。これで説明を終わっていい（笑）？・・・で、超能力AIではタグチメソッドのように「誤差因子を束ねて総合SN比を計算」はせず、かわりに各誤差因子で比例係数βを計算したんです。”

![image1-13-5](https://yaber1965.github.io/images/image1-13-5.jpg)

D先生 ： “実際の0点比例グラフの挙動をみるに、データにはかなりの非線形があるような・・・(笑)。しかし、コレはちょっと「アバウトすぎるん」じゃありませんか？”

![image1-13-6](https://yaber1965.github.io/images/image1-13-6.jpg)

QEU:FOUNDER ： “もしこれで「アバウトだ～！」と思うのなら、前に紹介したように全部の因子をディープラーニングで計算すればいいです。そのとき、2つの方法のどちらが精度が良いのかが分かるから・・・。さて、これで「NNのためのTMは完結」デス。超能力AIのモデル式はこの図(↑)のようになります・・・。”

D先生 ： “これならば簡単な回帰分析で計算できますね。”

QEU:FOUNDER ： “・・・というわけで、プログラムをドン！おまけに、コード内に解析結果付き！！”

```python
# ---------------
# NN'erのためのタグチメソッド実験
# regression_super_ai.py
# CS-Tと統合AN比を使った「超能力AI」のデモ版の第三段階です
# 超能力AIのモデル式を計算します。
# ---------------
# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
%matplotlib inline

# ---------------------------
# フォルダ指定
folder_input    = "./"  # My project folder

# ---------------------------
# 機械学習用のCSVファイルを定義する
nam_csv_input    = "機械学習のCSVデータ" 
nam_code_input   = "super_ai_input.csv" # CSVコードの指定
file_csv_input = folder_input + nam_code_input  # ファイルパス名の生成 
print("入力用CSVファイル名 ：{0}".format(file_csv_input))

# loading the data
Dataframe = pd.read_csv(file_csv_input)
df = Dataframe.loc[:,"A":"beta"]

# show rows and columns
df.head()

# getting info.
df.info()
# ---
df.describe()

#	A	B	noise	beta
#count	34.000000	34.000000	34.000000	34.000000
#mean	0.529412	1.058824	0.500000	3.683577
#std	0.506640	0.814311	0.507519	1.689441
#min	0.000000	0.000000	0.000000	0.958100
#25%	0.000000	0.000000	0.000000	2.163853
#50%	1.000000	1.000000	0.500000	3.703704
#75%	1.000000	2.000000	1.000000	4.981882
#max	1.000000	2.000000	1.000000	6.851852

#---------------------
# まとめ(主効果のみ)
model = smf.ols(formula='beta ~ A+B+noise', data=df).fit()
summary = model.summary()
print(summary)

#                 coef    std err          t      P>|t|      [0.025      0.975]
#------------------------------------------------------------------------------
#Intercept      2.8792      0.519      5.552      0.000       1.820       3.938
#A             -1.1521      0.461     -2.498      0.018      -2.094      -0.210
#B              1.0264      0.287      3.576      0.001       0.440       1.613
#noise          0.6550      0.459      1.427      0.164      -0.283       1.593

#---------------------
# まとめ(主効果と交互作用)
model = smf.ols(formula='beta ~ A+B+noise+A:noise+B:noise', data=df).fit()
summary = model.summary()
print(summary)

#                 coef    std err          t      P>|t|      [0.025      0.975]
#------------------------------------------------------------------------------
#Intercept      2.8894      0.681      4.245      0.000       1.495       4.284
#A             -1.1852      0.675     -1.755      0.090      -2.568       0.198
#B              1.0334      0.420      2.460      0.020       0.173       1.894
#noise          0.6347      0.963      0.659      0.515      -1.337       2.606
#A:noise        0.0664      0.955      0.069      0.945      -1.890       2.022
#B:noise       -0.0140      0.594     -0.024      0.981      -1.231       1.203

```

D先生 ： “重回帰分析したところ、ノイズ(noise)項のp値が少し大きいですね。ひょっとしたら関数に非線形成分があるのかもしれません。”

```python
#                 coef    std err          t      P>|t|      [0.025      0.975]
#------------------------------------------------------------------------------
#Intercept      2.8792      0.519      5.552      0.000       1.820       3.938
#A             -1.1521      0.461     -2.498      0.018      -2.094      -0.210
#B              1.0264      0.287      3.576      0.001       0.440       1.613
#noise          0.6550      0.459      1.427      0.164      -0.283       1.593

```

QEU:FOUNDER ： “じゃあ高次の回帰分析を使ってもいいし、ディープラーニングでも使えば・・・。このようにすれば、「想定範囲を超えた場面で」より高いパフォーマンスを出せるような調整が可能でしょ？。”

![image1-13-7](https://yaber1965.github.io/images/image1-13-7.jpg)

## ～　完結！・・・追記です　～

D先生 ： “なるほど、こういうものか・・・。皆さん、この説明を聞いたら「なぁんだ、超能力AIは単純なモノだ」と思っているんじゃないかな？”

![image1-13-8](https://yaber1965.github.io/images/image1-13-8.jpg)

QEU:FOUNDER ： “この事例はあくまで「机上（の議論）」だからね・・・。なぜ小生がタグチメソッドを機械学習の中に入れたのかというと、機械学習の方法論(methodology)としてハードを含めて最適化することで機械学習をより単純化させ、より予測精度を上げる可能性は大いにあるから・・・。確かに、ディープラーニングはなんにでも近似するので便利だが、それに頼ってばかりはダメじゃない？ここで例を考えてみましょう。今回の事例の場合、項目AとBが超能力AIで使用できる因子ということになりました。モデル式は以下の通り・・・。”

__OUTPUT = [Ca*A[ハード因子]+Cb*B[ソフト因子]+Ce*noise]*予測式__
ここで、Ca,Cb,Ceは回帰分析で得られた係数


D先生 ： “なに・・・？ここでいう「ハード因子」と「ソフト因子」って・・・。”

QEU:FOUNDER ： “少しの間、その素朴な疑問を忘れて小生の説明を聞いてくんない（笑）？項目Aの水準の値が０（ゼロ）の場合には、この図（↓）のように予測値に大きな非線形要素があります。”

![image1-13-9](https://yaber1965.github.io/images/image1-13-9.jpg)

QEU:FOUNDER ： “しかし、このハード因子Aの水準を1にしたとき、ソフト因子を調整して信号値を変化させても、この図（↓）のように綺麗な0点比例式が得られました。このとき、D先生は超能力AIのコントロールのため、ハード因子Aを固定させたい？それともソフト因子Bと一緒に動かしたい？”

![image1-13-10](https://yaber1965.github.io/images/image1-13-10.jpg)

D先生 ： “そりゃあもう・・・、多少は制御ゲインが落ちても、絶対にハード因子AをA=1に固定させますよ。そうか、・・・FOUNDER、私はいままで超能力AIを「キワモノ」だと思ってました・・・。実際には、全然違いますねぇ。これは機械学習にはなくてはならない概念です。”

QEU:FOUNDER ： “そうだよ・・・。この超能力AIの概念は「今現在テスラ（仮名）が直面している問題を解決する」可能性のあるスキームかもしれないです、皆さまカンパください。”

[＞寄付のお願い(別ブログに移ります)＜](https://jpnqeur21intro.blogspot.com/2022/02/qeur21intro13nnertm13ai5.html)

D先生 ： “YOROSHIKU(Thank you very much).”


## ～　まとめ　～

### ・・・　QEU:ROUND1-95の内容の全体の1/3位の再編成が終わって　・・・

D先生 : “解析技術編が、我々QEU(Quality Engineering Unity)プロジェクトが10年（+α）で得た成果ですか・・・。”

QEU:FOUNDER ： “いやいや・・・、分野で分けると1/3程度だけ・・・。”

- 解析技術(NN’erのための品質工学など)
- 自動検査技術
- ロボティックス


D先生 ： “つぎは自動検査技術(QOCV: QEU Open-CV)ね・・・。”

QEU:FOUNDER ： “いやいや・・・。QEU解析技術には、まだ「CA：Computational Approach」が残っています・・・。”

D先生 ： “あっ・・・、そうか・・・。”

QEU:FOUNDER ： “それよりも、おっさんを見てください。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/aJKWrqnVm2I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 ： “A元総理などのすごい話を、ゆるく話していますね。”

