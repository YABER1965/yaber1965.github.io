## QEUR21_TYNLP3:　自然言語解析(その4) ～ RTメトリックスの活用(その1)

## ～　RTMの準備段階です　～

### ・・・　このプロジェクトは「お遊び」です。ちょっとだけ本気が混じってるけどね　・・・

D先生 ： “FOUNDER・・・、やっぱり今度もRTメトリックスを使うんですね。「たぐちさん家の方法」の応用として・・・。”

![image2-73-1](https://yaber1965.github.io/images/image2-73-1.jpg)

QEU:FOUNDER ： “そうですよ。今回はLSTMを使わないようにしたいから・・・。”

D先生 ： “LSTMとRT法の間に、どういう関係が？”

![image2-73-2](https://yaber1965.github.io/images/image2-73-2.jpg)

QEU:FOUNDER ： “RT法は標準値の考え方を取り入れています。**この標準値の特性によって普通のディープラーニングを使ってもLSTMと同様な作用が得られる**と思います。”

D先生 ： “例によって、RTメトリックスを前処理として使うという戦略っスね。”

QEU:FOUNDER ： “これ以上の話はおいおいと・・・。まずは基本データ抽出用のプログラムをドン・・・。TOXICのあるベクトルデータのみを抽出し、それをCSVファイルに保存しました。”


```python
# -------
#
# 自然言語処理(NLP)をRT法とディープラーニングで解く
# RTMその1　データの把握（step1）
# nlp_rtm_preprocess_step1.py
#
# -------
import re, numpy as np, pandas as pd
from tensorflow import keras
from tensorflow.keras.optimizers import Adam # - Works
# -------
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers
from sklearn.model_selection import train_test_split
# -------
import matplotlib.pyplot as plt

# ---------------------------
#import numpy as np, pandas as pd
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')
test_label = pd.read_csv('data/test_labels.csv')
#train.head()

# ---------------------------
# コメント（文字列）のリストを生成する
labels  = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y_train = train[labels].values
y_test  = test_label[labels].values
comments_train = train['comment_text']
comments_test = test['comment_text']
comments_train = list(comments_train)
#159571 #len(train) 
#153164 #len(test)

・・・　中略　・・・

# ---------------------------
# パラメータの設定(1)
embed_size=100
max_features=20000
max_len=90

# -------
# 言語データのトークン化、数値化（embedding）
tokenizer= Tokenizer(num_words=max_features,lower= True)
tokenizer.fit_on_texts(list(x_train))
tokenized_train =tokenizer.texts_to_sequences(x_train)
tokenized_test  =tokenizer.texts_to_sequences(x_test)
train_x =pad_sequences(tokenized_train,maxlen=max_len)
test_x  =pad_sequences(tokenized_test,maxlen=max_len)
# -------
num_sample = 150
print('---- NO:{} ----'.format(num_sample))
print('Sample cleaned: ', x_train[num_sample])
print('------')
print('Sample embedded: ', train_x[num_sample])

# ---------------------------
# 埋め込みデータフレームを生成する
arr_columns = ["s{}".format(i) for i in range(max_len)]
#print(arr_columns)
df_embedding = pd.DataFrame(train_x, columns=arr_columns)
#df_embedding.head(10)

# -------
# ２つのデータフレームを連結する
df_merge = pd.concat([df_train, df_embedding], axis=1)
#df_merge.head(10)
    
# -------
# mergeデータベースからtoxicのみを抽出する
df_train_toxic = df_merge[df_merge['tox_level']>0]
print(len(df_train_toxic))
df_train_toxic.head(10)

# ---------------------------
# CSV ファイル (pattern.csv) として出力する
foldername = "./"
file_csvout = foldername + "pattern_toxic.csv" # ファイルパス名の生成
df_train_toxic.to_csv(file_csvout)    

```

QEU:FOUNDER ： “結局、以下の結果がでてくるだけなんですけど・・・。”

![image2-73-3](https://yaber1965.github.io/images/image2-73-3.jpg)

D先生 ： “せっかくだから、抽出したデータを使って、「ちょっと突っ込んだ分析」をしてくれませんか？”

```python
# ---------------------------
#import numpy as np, pandas as pd
# CSV読み込み
raw_train = pd.read_csv('data/pattern_toxic.csv', encoding = 'ISO-8859-1')
raw_train.head()

# -----
# フレームの切り取り(short)
short_txt = raw_train[raw_train['len_txt']<200]
short_txt = short_txt.loc[:,"s0":"s89"]
short_txt.head()
#short_txt.describe()

# -----
# 統計処理(describe)
stat_short_txt = short_txt.describe()
mean_short_txt = stat_short_txt.loc['mean',:].values
p25_short_txt  = stat_short_txt.loc['25%',:].values
p75_short_txt  = stat_short_txt.loc['75%',:].values
print(p75_short_txt)

# -----
# 作画
x = list(range(90))
#print(x)

fig = plt.figure(figsize =(14, 8))
ax = fig.add_subplot(1, 1, 1)

ax.plot(x, mean_short_txt, label='mean')
ax.plot(x, p25_short_txt, label='25%')
ax.plot(x, p75_short_txt, label='75%')

ax.legend()
ax.set_title("short text vector (less than 200)")
ax.set_xlabel("vector")
ax.set_ylabel("value")
#ax.set_ylim(0, 3)

plt.show()

```

QEU:FOUNDER ： “じゃあ、簡単な分析用プログラムを作成したので、コメントのテキスト長によって埋め込みベクトルがどのように変化するかを見てみましょう。”

![image2-73-4](https://yaber1965.github.io/images/image2-73-4.jpg)

D先生 ： “なるほど、埋め込みベクトルはこのように変動するんですね。しかし、**この微妙な変動からtoxic判別をしている**んだからLSTMってすごいですね。”

QEU:FOUNDER ： “次は、もうちょっと分析を進めましょう。レーダーチャートを使って・・・。”

D先生 ： “はぁ？”

## ～　まとめ　～

### ・・・　MMTの話がつづきます　・・・

QEU:FOUNDER ： “おまちかねの「おっさんコーナー」です。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/Nip8V7pAghw" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 ： “この人はタフですねぇ・・・。日本中飛び回っている。 “

QEU:FOUNDER ： “引退したらグルメ番組をやるんじゃないか・・・（笑）。”




















