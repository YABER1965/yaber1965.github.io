## QEUR21_RL2048T11:　Feature Engineering(その2) ～ スライドRT法でインプットする

## ～　率直、考え方は大して良くはないが・・・　～

D先生 ： “それでは次のステップにつづきます。「スライドRT法」っていう、とってつけたような名前ですが要はコレ（↓）のことでしょう？”

**（2048ゲーム）**

![imageRL1-9-1](https://yaber1965.github.io/images/imageRL1-9-1.jpg)

**（スライドRT法の入力）**

- 単位空間　：　ｎ次のゲームパターン
- 信号空間　：　(n+1)次のゲームのパターン（とることができる数のみ）

QEU:FOUNDER ： “正解（笑）！（スライドRT法は）状態を「先読み」しているメリットがある一方で、パターンの表現力が悪いという懸念もあります。16個のパネルの値を2種類のメトリックス（感度、SN比）に圧縮しますからね・・・。”

![imageRL1-9-2](https://yaber1965.github.io/images/imageRL1-9-2.jpg)

D先生 ： “テクノ・メトリックスに対する理解が進んだ現在としては、大してコレには期待してないんですが、やってみましょう・・・！！他に言いたいことは？”

![imageRL1-9-3](https://yaber1965.github.io/images/imageRL1-9-3.jpg)

QEU:FOUNDER ： “スライドできなくなった方向のメトリックスについては、感度は1.0とSN比0.0を与えました。この部分が大事ね・・・。DL関数が収束するかどうかが決まるから・・・。それでは、プログラムをドン！！”


```python
# ----------------
# 2048ゲームの強化学習システム
# step2 : スライドRTを動的メトリックスとして活用する
# step2 : DQN_slideRT_game2048_agent.py
# step2 : DQNとスライドRT法でデータを生成します
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import copy, random, time
import pandas as pd
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
#%matplotlib inline
# ---------------- 
# environment
import RT_auto_game_2048_2

# =================================================
# difinition of function(1) - ONEHOT関連
# =================================================
# [array]-action をonehot変換します。
def create_arronehot(action):
    # ----------------
    # action number / onehot
    # 0 / 1000
    # 1 / 0100
    # 2 / 0010
    # 3 / 0001
    # ----------------
    # ONEHOTに変換
    if action == 0:
        arr_onehot = [1.0, 0.0, 0.0, 0.0]
    elif action == 1:
        arr_onehot = [0.0, 1.0, 0.0, 0.0]
    elif action == 2:
        arr_onehot = [0.0, 0.0, 1.0, 0.0]
    elif action == 3:
        arr_onehot = [0.0, 0.0, 0.0, 1.0]

    return np.array(arr_onehot)


# =================================================
# difinition of function(2)
# =================================================
# イレギュラー検出関数(タイル移動後の状態とスコア)
def is_invalid_action(state, a_order):
    # ----------------
    spare = copy.deepcopy(state)  # 状態
    #print("spare",type(spare))
    reward = 0  # 当該行動による報酬
    if a_order == 0:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y, x - z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, x - z_cnt] *= 2
                    reward += spare[y, x - z_cnt]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 1:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, x] != prev:
                    tmp = spare[y, x]
                    spare[y, x] = 0
                    spare[y - z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y - z_cnt, x] *= 2
                    reward += spare[y - z_cnt, x]
                    spare[y, x] = 0
                    prev = -1
    elif a_order == 2:
        for y in range(4):
            z_cnt = 0
            prev = -1
            for x in range(4):
                if spare[y, 3 - x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[y, 3 - x] != prev:
                    tmp = spare[y, 3 - x]
                    spare[y, 3 - x] = 0
                    spare[y, 3 - x + z_cnt] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[y, 3 - x + z_cnt] *= 2
                    reward += spare[y, 3 - x + z_cnt]
                    spare[y, 3 - x] = 0
                    prev = -1
    elif a_order == 3:
        for x in range(4):
            z_cnt = 0
            prev = -1
            for y in range(4):
                if spare[3 - y, x] == 0:
                    z_cnt = z_cnt + 1
                elif spare[3 - y, x] != prev:
                    tmp = state[3 - y, x]
                    spare[3 - y, x] = 0
                    spare[3 - y + z_cnt, x] = tmp
                    prev = tmp
                else:
                    z_cnt = z_cnt + 1
                    spare[3 - y + z_cnt, x] *= 2
                    reward += spare[3 - y + z_cnt, x]
                    spare[3 - y, x] = 0
                    prev = -1

    if (state - spare).any() == False:
        return True, reward, spare
    else:
        return False, reward, spare

# ---------------------------
# newRTメトリックスを計算する(テンソル活用版)
def calc_newRT(len_temp, max_jy_index, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    L1_loss = torch.nn.L1Loss()
    btY1_yarray, Y2_yarray = [], []

    # 繰り返し
    for i in range(len_temp):

        y = tsr_sig_matrix[i]
        x = tsr_tani_array

        xx = torch.dot(x,x)
        xy = torch.dot(x,y)
        beta = xy/xx
        mDistance   = L1_loss(y, beta*x)
        #print("i:{}, beta:{}".format(i,beta))
        #print("mDistance: ", mDistance.item())
        
        btY1_yarray.append(beta.item())
        Y2_yarray.append(mDistance.item())

    return torch.tensor(btY1_yarray).float(), torch.tensor(Y2_yarray).float()

# ----------------
# 移動可能な方向をまとめ、RTメトリックスを生成する
def calc_mxlog2(arr_state):    

    tsr_state = torch.tensor(arr_state).float()
    #print(tsr_state)
    # ---------------------------
    # tsr_spareを対数化する
    for jCol in range(16):
        raw_value = tsr_state[jCol]
        # print("make_metrics - raw_value",raw_value)
        if raw_value > 0:
            tsr_state[jCol] = math.log2(raw_value)
        else:
            tsr_state[jCol] = 0

    return tsr_state

# ----------------
# 移動可能な方向をまとめ、RTメトリックスを生成する
def calc_RTmovables(state):    

    # --------------------------------------------------
    # 配列の初期化
    spare = np.array(state)
    # --------------------------------------------------
    # オリジナル操作方法 -> 数字に変更します(ENVIRONMENT)
    # 数字を上へ -> 1
    # 数字を下へ -> 3
    # 数字を左へ -> 0
    # 数字を右へ -> 2
    # --------------------------------------------------
    a_map, movables, arr_rewards = [0, 1, 2, 3], [], []
    iCnt, arr_states = 0, []
    for i_map in a_map:
        flg_invalid, val_reward, spare_next = is_invalid_action(spare, i_map)
        if flg_invalid == False:
            arr_states.append(spare_next.flatten())
            arr_rewards.append(val_reward + 4)
            movables.append(i_map)
            iCnt = iCnt + 1
    num_avail = iCnt
    
    # --------------------------------------------------
    # newRT法の生成
    # --------------------------------------------------
    # 単位空間
    # 移動可能な方向をまとめ、RTメトリックスを生成する
    tsr_tani    = calc_mxlog2(spare.flatten())
    #print("tsr_tani",tsr_tani)

    # 信号空間
    tsr_signal  = torch.zeros([num_avail, 16]).float()
    for iCnt in range(num_avail):
        tsr_signal[iCnt]  = calc_mxlog2(arr_states[iCnt])
    #print("tsr_signal",tsr_signal)

    # --------------------------------------------------
    # FEATURE-ENGINEERING(新RTメトリックスを計算する)
    len_mx, max_jy_idx = num_avail, 16
    btY1_yarray, Y2_yarray = calc_newRT(len_mx, max_jy_idx, tsr_signal, tsr_tani)
    
    # ------------------
    # 信号空間用の畳み込みテンソルを生成する 
    tsr_features  = torch.zeros(state_size).float()     # state_size
    iAcc = 0
    for iCnt in [0,1,2,3]:
        #if iCnt == 0:
        iZero = int(2 * iCnt)
        iPone = int(2 * iCnt + 1)
        #print(iZero, iPone)
        if iCnt in movables:
            tsr_features[iZero] = btY1_yarray[iAcc]
            tsr_features[iPone] = Y2_yarray[iAcc]
            iAcc = iAcc + 1
        else:
            tsr_features[iZero] = 1.0
            tsr_features[iPone] = 0.0
    # 結果の出力
    #print("----- tsr_features -----")
    #print(tsr_features)

    return num_avail, movables, arr_rewards, tsr_features


#=================================================
# agent_main class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 64)
        self.fc2 = torch.nn.Linear(64, 64)
        self.fc3 = torch.nn.Linear(64, dim_output)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Solving the maze in Deep Q-learning
class DQN_Solver:
    def __init__(self, state_size, action_size):
        self.state_size  = state_size
        self.action_size = action_size
        self.memory  = deque(maxlen=MEMORY_CAPACITY)
        self.gamma   = GAMMA
        self.epsilon = EPSILON
        self.e_decay = EPDECAY
        self.e_min   = EPMIN
        self.learning_rate = LRATE

        # --------------------------------------------------
        # crate instance for input
        self.model = Net()
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.model.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.model.eval()


    def remember_memory(self, state, action, reward, next_state, next_movables, done):
        self.memory.append((state, action, reward, next_state, next_movables, done))


    def choose_action(self, x_input_array, num_avail, movables, arr_rewards, iCnt_turn, iCnt_play):        # 定义动作选择函数 (x为状态)

        # --------------------------------------------------
        # オリジナル操作方法 -> 数字に変更します(ENVIRONMENT)
        # 数字を上へ -> 1
        # 数字を下へ -> 3
        # 数字を左へ -> 0
        # 数字を右へ -> 2
        # --------------------------------------------------
        # 命令の初期選択
        a_order, acType, rl_Qvalue, rl_done = -1, "NA", -1, False
        mx_input    = []  # 状態(state)と行動(action)マトリックス
        val_boltz   = 10
        if (num_avail == 0):
            rl_done = True
            acType  = "gameover"
            print("ゲームオーバー -- iCnt_play: {0}, iCnt_turn: {1}".format(iCnt_play, iCnt_turn))
        else:
            if self.epsilon <= random.random():
                # DQNによる選択
                acType  = "machine"
                iCnt = 0
                for a in movables:
                    # --- model ---
                    # [array]-action をonehot変換します。
                    arr_onehot = create_arronehot(a)
                    temp_s_a = np.append(x_input_array, arr_onehot)
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.append(mx_input, [temp_s_a], axis=0)  # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                mx_input = np.array(mx_input)
                # print("----- mx_input -----")
                # print(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.model(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                rl_Qvalue   = np.max(y_pred)
                temp_action = np.argmax(y_pred)
                a_order     = movables[temp_action]
            else:                                                                   # 随机选择动作
                # 乱数による選択
                acType  = "random"
                qs_prob = softmax(np.array(arr_rewards) / val_boltz)
                a_order = np.random.choice(movables, p=qs_prob)
                
        # --------------------------------------------------
        if iCnt_turn%10 == 0:
            print("行動タイプ:{0}, プレイ:{1}, ターン:{2}, 行動選択結果:{3}".format(acType, iCnt_play, iCnt_turn, a_order))

        return a_order, acType, rl_Qvalue, rl_done


    def replay_experience(self, batch_size):

        # set training parameters
        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate)
        criterion = torch.nn.MSELoss()
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = np.array([]), np.array([])
        for ibat in range(batch_size):
            state, action, reward, next_state, next_movables, done = minibatch[ibat]
            # [array]-action をonehot変換します。
            arr_onehot = create_arronehot(action)
            state_action_curr = np.append(state, arr_onehot)
            #print("state_action_curr",state_action_curr)
            if done:
                target_f = reward
            else:
                mx_input = []  # 状態(state)と行動(action)マトリックス
                iCnt = 0
                for imov in next_movables:
                    # [array]-action をonehot変換します。
                    arr_onehot = create_arronehot(imov)
                    temp_s_a = np.append(next_state, arr_onehot)
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.append(mx_input, [temp_s_a], axis=0)  # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                    # ----
                mx_input = np.array(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.model(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # -----
            if ibat == 0:
                X = [state_action_curr]  # 状態(S)行動(A)マトリックス
            else:
                X = np.append(X, [state_action_curr], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_next = torch.from_numpy(X).float()
        expected_q_value = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_next:",state_action_next)
        #print("expected_q_value:",expected_q_value)
        
        # --- building model ---
        q_value = self.model(state_action_next)

        # calculate loss
        loss = criterion(q_value, expected_q_value)

        # update weights
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Show progress
        print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return self.epsilon, loss.data.numpy()


#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # 記録用パラメタ類(プレイベース)の初期化
        self.arr_iCnt_play  = []  # count game play    プレイ番号
        self.arr_maxturn    = []  # max_turn   ゲームのターン数
        self.arr_csv_play   = []  # name game play    プレイファイル名のリスト
        self.arr_maxscore   = []  # rl_score game play    最終プレイスコア
        # ----------
        self.arr_epsilon    = []  # イプシロン
        self.arr_upright    = []  # 角のスコア（右上）
        self.arr_upleft     = []  # 角のスコア（左上）
        self.arr_downright  = []  # 角のスコア（右下）
        self.arr_downleft   = []  # 角のスコア（左下）
        # ----------
        # 統計データの初期化(プレイベース)
        self.arr_loss       = []  # 学習損失
        # ----------
        # AV値の分析用(プレイベース)
        self.arr_num_AV     = []  # AV値のN数
        self.arr_max_AV     = []  # AV値の最大値
        self.arr_q25_AV     = []  # AV値の4分の1値
        self.arr_q75_AV     = []  # AV値の4分の3値

        # --------------------------------------------------
        # CSV-playlist用のリスト
        self.arr_usefile    = np.zeros(num_episodes)        # usefile    CSVファイルを出力するか use 1, un-use 0
        self.arr_perform    = np.zeros(num_episodes)        # performance   命令の一致率
        
        # --------------------------------------------------
        gamepanel       = RT_auto_game_2048_2.Board()
        self.game2048   = RT_auto_game_2048_2.Game(gamepanel)
        self.game2048.gamepanel.gridCell = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])

        # --------------------------------------------------
        # 畳み込み用部品(8種類)を読み込む
        for i_cnv in range(max_cnv_parts):    # max_cnv_parts
            # -----
            # 畳み込みファイル
            file_cnv_input = foldername + code_cnv_input[i_cnv] + ".csv"  # ファイルパス名の生成 
            mx_conv = read_csvfile(file_cnv_input, max_cnv_idx, max_cnv_col)
            if i_cnv == 0:    
                tsr_bend1 = torch.tensor(mx_conv).float()
            elif i_cnv == 1:
                tsr_bend2 = torch.tensor(mx_conv).float()
            elif i_cnv == 2:
                tsr_bend3 = torch.tensor(mx_conv).float()
            elif i_cnv == 3:
                tsr_bend4 = torch.tensor(mx_conv).float()
            elif i_cnv == 4:
                tsr_line1 = torch.tensor(mx_conv).float()
            elif i_cnv == 5:
                tsr_line2 = torch.tensor(mx_conv).float()
            elif i_cnv == 6:
                tsr_datum1 = torch.tensor(mx_conv).float()
            elif i_cnv == 7:
                tsr_datum2 = torch.tensor(mx_conv).float()

        # --------------------------------------------------
        # 畳み込みカーネルを生成する
        signal_kernels  = torch.stack([tsr_bend1, tsr_bend2, tsr_bend3, tsr_bend4, tsr_line1, tsr_line2])
        tani_kernel     = tsr_datum1 + tsr_datum2

        # --------------------------------------------------
        # ゲーム2048をプレイする
        for iCnt_play in range(num_episodes):       # num_episodes
        
            num_maxturn, num_maxscore, arr_corner = self.get_episode(tani_kernel, signal_kernels, iCnt_play)

            # ----------
            # DQN-Experience Replay学習
            val_epsilon, loss = dql_solver.replay_experience(BATCH_SIZE)

            # ----------
            code_csvout = "NA"
            if iCnt_play%40 == 0:
                # CSV ファイル (file_csvout) として出力する
                code_csvout = "game2048_play{0}_score{1}.csv".format(iCnt_play, num_maxscore)       # file name  
                print("メトリックス保管用CSVファイル名 ：{0}".format(code_csvout))
                # ----------
                # ゲームデータの出力
                #code_csvout = self.save_csvGAME(iCnt_play, code_csvout)        # CSVファイルに保存する

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iCnt_play.append(iCnt_play)        # count game play    プレイ番号
            self.arr_maxturn.append(num_maxturn)        # max_turn   ゲームのターン数
            self.arr_csv_play.append(code_csvout)       # name game play    プレイファイル名のリスト
            self.arr_maxscore.append(num_maxscore)      # rl_score game play    最終プレイスコア
            self.arr_loss.append(loss)                  # DQN-Experience Replay学習
            # ----------
            self.arr_epsilon.append(val_epsilon)        # イプシロン
            self.arr_upright.append(arr_corner[0])      # 角のスコア（右上）
            self.arr_upleft.append(arr_corner[1])       # 角のスコア（左上）
            self.arr_downright.append(arr_corner[2])    # 角のスコア（右下）
            self.arr_downleft.append(arr_corner[3])     # 角のスコア（左下）         

            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%20 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する(1)
        
        x = list(range(num_episodes))
        
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(x, self.arr_epsilon)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Corner score')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('number')
        ax2.scatter(x, self.arr_upright, label="upright", color="red")
        ax2.scatter(x, self.arr_upleft, label="upleft", color="blue")
        ax2.scatter(x, self.arr_downright, label="downright", color="orange")
        ax2.scatter(x, self.arr_downleft, label="downleft", color="green")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph(num_episodes)

        # --------------------------------------------------
        # ゲームプレイリストをCSVファイルに保存する
        #print("ゲームプレイリストをCSVファイルに保存する")
        #self.save_playlist()

        # --------------------------------------------------
        # PyTorchモデルの保存
        #torch.save(ActorCritic.state_dict(), file_output_model)


    # --------------------------------------------------
    def get_episode(self, tani_kernel, signal_kernels, iCnt_play):

        # --------------------------------------------------
        acType, val_reward, rl_done, a_order  = "random", 0, False, 0
        state    = np.array([[0]*4 for i in range(4)])
        a_order_prev, ttlscore_prev = 0, 0
        iCnt_turn , num_maxturn, num_maxscore = 0, 0, 0
        
        # --------------------------------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_icount = []        # カウンタリスト
        self.arr_order = []         # 指示リスト
        self.arr_avail = []         # 使用可能な命令数リスト
        self.arr_ttlscore = []      # ゲームスコアリスト(真数)  
        self.arr_reward = []        # 報酬リスト(真数)  
        self.arr_yzero = []         # ゼロ・スコアの数
        self.arr_acType = []        # 指示タイプ(random,machine)
        self.arr_predAV = []        # DL予測されたAV値

        # --------------------------------------------------
        # ゲーム・エピソードを行う
        state = self.reset()  # 環境をﾘｾｯﾄする
        num_avail, movables, arr_rewards, s = calc_RTmovables(state)
        #print("    s       ", s)
        #print("    state       ", state)
        iCnt_turn, ttlscore, ttlscore_prev= 0, 0, 0
        while True: 
        
            # --------------------------------------------------
            # 命令を選択する
            a_order, acType, rl_Qvalue, rl_done = dql_solver.choose_action(s, num_avail, movables, arr_rewards, iCnt_turn, iCnt_play) 
            # ------
            arr_corner = np.zeros(4)
            arr_corner[0] = state[0][3]   # arr_upright , 角のスコア（右上）
            arr_corner[1] = state[0][0]   # self.arr_upleft , 角のスコア（左上）
            arr_corner[2] = state[3][3]   # self.arr_downright , 角のスコア（右下）
            arr_corner[3] = state[3][0]   # self.arr_downleft , 角のスコア（左下）

            # --------------------------------------------------
            # 環境(ENVIRONMENT)を実行する
            state_next, ttlscore, done = self.game2048.link_keys(a_order)
            num_avail, next_movables, arr_rewards, s_temp = calc_RTmovables(state_next)
            #print("状態テンソル - s_temp:", s_temp)

            # 報酬を計算する
            val_reward = ttlscore - ttlscore_prev

            # --------------------------------------------------
            # 学習データの積み上げ
            # 変数の変換(machine, random)
            s       = s
            a       = a_order
            r       = val_reward 
            s_      = s_temp

            # Experience Replay配列に保管する
            if num_avail > 0:
                dql_solver.remember_memory(s, a, r, s_, next_movables, done)

            # --------------------------------------------------  
            self.arr_icount.append(iCnt_turn)       # カウンタリスト
            self.arr_order.append(a_order)        # 指示リスト
            self.arr_avail.append(num_avail)     # 使用可能な命令数リスト
            self.arr_ttlscore.append(ttlscore)      # ゲームスコアリスト(真数)  
            self.arr_reward.append(val_reward)       # 報酬リスト(真数)  
            self.arr_acType.append(acType)       # 指示タイプ(random,machine)     
            self.arr_predAV.append(rl_Qvalue)      # DL予測されたAV値
            # ----------   
            board_flatten = np.array(state).flatten()
            cc = Counter(board_flatten)
            self.arr_yzero.append(cc[0]) 

            # --------------------------------------------------
            # 次回への引き渡し
            s               = s_temp
            state           = state_next
            movables        = next_movables
            ttlscore_prev   = ttlscore
            iCnt_turn       = iCnt_turn + 1

            if done:       # 如果done为True
                print("# of episode :{}, turn : {}, ttlscore : {:.1f}".format(iCnt_play, iCnt_turn, ttlscore))
                break      

        # --------------------------------------------------  
        self.arr_num_AV.append(iCnt_turn)                               # AV値のN数
        self.arr_max_AV.append(np.max(self.arr_predAV))                 # AV値の最大値
        self.arr_q25_AV.append(np.percentile(self.arr_predAV, q=25))    # AV値の4分の1値
        self.arr_q75_AV.append(np.percentile(self.arr_predAV, q=75))    # AV値の4分の3値

        # -------------------------------------------------- 
        # プレイリストに引き渡す変数
        num_maxturn     = iCnt_turn
        num_maxscore    = ttlscore

        return num_maxturn, num_maxscore, arr_corner

    # ----------------
    # ゲームをﾘｾｯﾄする
    def reset(self):   
    
        state_init = np.array([[0, 2, 4, 0], [2, 0, 16, 0], [0, 0, 2, 0], [0, 8, 0, 4]])
    
        # --------------------------------------------------
        # パラメタの初期化(Panel)
        self.game2048.gamepanel.gridCell=state_init
        self.game2048.gamepanel.compress=False
        self.game2048.gamepanel.merge=False
        self.game2048.gamepanel.moved=False
        self.game2048.gamepanel.score=0
        
        # --------------------------------------------------
        # パラメタの初期化(Game)
        self.game2048.end=False
        self.game2048.won=False

        # --------------------------------------------------
        # 記録用リストの初期化(ターンベース)
        self.arr_icount = []        # カウンタリスト
        self.arr_order = []         # 指示リスト
        self.arr_avail = []         # 使用可能な命令数リスト
        self.arr_ttlscore = []      # ゲームスコアリスト(真数)  
        self.arr_reward = []        # 報酬リスト(真数)  
        self.arr_yzero = []         # ゼロ・スコアの数
        self.arr_acType = []        # 指示タイプ(random,machine)
        self.arr_predAV = []        # DL予測されたAV値

        return state_init
        
        
    # ----------------
    # 学習結果のグラフ化
    def show_graph(self, num_episodes):

        x = list(range(num_episodes))
        # print(x)

        fig = plt.figure(figsize=(12, 10))
        # -----
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : loss')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('Learn-loss')
        ax1.grid(True)
        ax1.plot(x, self.arr_loss)
        # -----
        ax2 = fig.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : QValues')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('QValue')
        #ax2.set_ylim(bottom=0)
        ax2.grid(True)
        ax2.plot(x, self.arr_q25_AV, label="q25_AV", color="blue")
        ax2.plot(x, self.arr_q75_AV, label="q75_AV", color="red")
        ax2.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()

        ax3 = fig.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(x, self.arr_maxturn, label="original", color="blue")
        ax3.plot(x, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        
        ax4 = fig.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(x, self.arr_maxscore, label="original", color="blue")
        ax4.plot(x, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # ---------------------------
    # DQN Parameter
    state_size  = 8
    action_size = 4
    # -----
    dim_input   = state_size + action_size
    dim_output  = 1

    # ---------------------------
    # ハイパーパラメタ
    BATCH_SIZE = 128                         # サンプルサイズ
    LRATE = 0.005                            # 学習率
    EPSILON = 0.99                           # greedy policy
    EPDECAY = 0.999
    EPMIN = 0.01
    GAMMA = 0.95                             # reward discount
    MEMORY_CAPACITY = 2000                   # メモリ容量
    # ---------------------------
    SLEEP_TIME      = 0.01
    print_interval  = 50                     # 出力頻度
    num_episodes    = 5000                   # 繰り返し回数

    # ---------------------------
    # パラメタの設定
    max_cnv_parts = 8       # 畳み込み部品数
    len_cmx       = max_cnv_parts - 2  # 信号空間の数
    cmax_jy_idx   = 4       # RT法のマトリックス
    # ---------------------------
    max_sp_idx  = 4          # 盤面のidx数
    max_cnv_idx = 3          # 畳み込み部品のidx数
    max_sp_col  = 4          # 盤面のidx数
    max_cnv_col = 3          # 畳み込み部品のidx数

    # ---------------------------
    # フォルダ名の指定
    foldername = "./ARRAY_RTCNN/"  # My project folder

    # ---------------------------
    # solver
    dql_solver  = DQN_Solver(state_size, action_size)

    # ---------------------------
    # ゲームプレイリスト用のCSVファイルを定義する
    nam_csv_playlist    = "ゲームプレイリストのCSVデータ" 
    code_csv_playlist   = "train_playlist_DQN.csv" # CSVコードの指定
    file_csv_playlist   = foldername + code_csv_playlist  # ファイルパス名の生成
    print("------------ 計測データ出力用のCSV名 -------------")   
    print("ゲームプレイリストのCSVファイル名 ：{0}".format(file_csv_playlist))

    # ---------------------------
    # 出力用Pytorchモデルのファイル名
    comment_output_model = "game2048_DQN"
    code_output_model = "model_game2048_DQN.pt"
    file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # DQN-ERで学習する
    Agent()

```

D先生 ： “畳み込みRT法を使わないだけで、ずいぶんプログラムが単純になりました。・・・そして、肝心のゲームのパフォーマンス（スコア）の方は・・・。”

**(イプシロン値とゲーム盤角部の数値の推移)**

![imageRL1-9-4](https://yaber1965.github.io/images/imageRL1-9-4.jpg)

**(その他のパフォーマンスの推移)**

![imageRL1-9-5](https://yaber1965.github.io/images/imageRL1-9-5.jpg)

D先生 ： “畳み込みRTを使わないだけで、ずいぶんプログラムが単純になりました。・・・そして、肝心のゲームのパフォーマンスの方は・・・。”

**(イプシロン値とゲーム盤角部の数値の推移)**

![imageRL1-9-6](https://yaber1965.github.io/images/imageRL1-9-6.jpg)

**(その他のパフォーマンスの推移)**

![imageRL1-9-7](https://yaber1965.github.io/images/imageRL1-9-7.jpg)

QEU:FOUNDER ： “えっ！！スライドRT法のパフォーマンスもかなりいいじゃないですか？でも、不思議・・・。新RT法ではゲームの進行に伴いQ値がどんどん大きくなるのに、スライドRT法では（Q値は）だんだん小さくなっていきます。”

D先生 ： “これも、5万回程度計算するとパフォーマンスの優劣がつくんでしょうね。”

QEU:FOUNDER ： “まあ、そこらへんの細かいことは、小生には興味ないから・・・。若干だけど、シンプルなメトリックスが提案できたので幸せです（笑）。”

D先生 ： “それでは、次にいきましょう。メトリックスを簡単にするために、さらにアグレッシブに・・・。”

QEU:FOUNDER ： “簡単ねぇ・・・。ある意味、面倒になるんですが・・・。実は、次回からが本番になります。つきましてはカンパください！！”

### [＞寄付のお願い(Donate me)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

D先生 ：“よろしくお願いします。”

## ～　まとめ　～

C部長 : “熱くなってきました、QEU名物のイケメン・バトルだ・・・！！”

![imageRL1-9-8](https://yaber1965.github.io/images/imageRL1-9-8.jpg)

D先生 : “何が暑くなった？お天気？”

movie

C部長 : “ワンちゃんの話です。”

![imageRL1-9-9](https://yaber1965.github.io/images/imageRL1-9-9.jpg)

D先生 : “くだらん。小生は猫派です・・・。”

C部長 : “咥えているのは・・・？”

D先生 : “魚だ。落ちはない・・・。”

